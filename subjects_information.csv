Subject,Information
Mahatma Gandhi,"Mahatma Gandhi, born Mohandas Karamchand Gandhi on October 2, 1869, in Porbandar, India, was the preeminent leader of the Indian independence movement against British colonial rule. He is renowned worldwide for his philosophy of nonviolent resistance, known as 'Satyagraha,' which sought to achieve social and political change without resorting to violence. Gandhi's principles of truth, nonviolence, and civil disobedience became powerful tools in the struggle for freedom and influenced many movements for civil rights and social justice across the globe.

Educated in law in London, Gandhi initially practiced as a lawyer in South Africa, where he first began developing his ideas on passive resistance in response to the racial discrimination he and others faced. In 1915, he returned to India and quickly became a leader in the Indian National Congress, advocating for self-rule (Swaraj) and the upliftment of the poor, particularly the rural peasants.

One of his most famous acts of civil disobedience was the Salt March in 1930, a 240-mile protest against the British monopoly on salt production. This march gained worldwide attention and symbolized India's refusal to be subjugated by unjust laws. Gandhi was also instrumental in campaigns against the oppressive British taxation system, the caste system, and the rights of untouchables, whom he called 'Harijans' or children of God.

Gandhi's efforts culminated in India's independence in 1947. However, the country was partitioned into Hindu-majority India and Muslim-majority Pakistan, a decision that deeply saddened Gandhi, as he had hoped for a united, pluralistic India. He spent his final years advocating for peace between Hindus and Muslims, but on January 30, 1948, he was assassinated by Nathuram Godse, a Hindu nationalist who opposed Gandhi's efforts to reconcile the two communities. Despite his death, Gandhi's legacy lives on, and he is remembered as the Father of the Nation in India and a symbol of peace and nonviolent resistance worldwide."
Mobile Phone,"A mobile phone, also known as a cellphone or smartphone, is a portable device that allows users to make calls, send text messages, and access a wide range of applications and services over a cellular network. The mobile phone has revolutionized communication since its inception and has become an integral part of daily life, allowing people to stay connected regardless of location.

The first mobile phones, introduced in the 1970s, were bulky, expensive, and limited to voice communication. Over the decades, advances in technology have led to significant reductions in size, cost, and weight while vastly improving functionality. Modern smartphones are essentially mini-computers, capable of not only voice communication but also internet browsing, social media access, gaming, photography, video recording, navigation, and much more.

Mobile phones operate on a network of cell towers, enabling calls to be routed from one tower to another as the user moves from place to place. Most modern phones also have access to 4G and 5G networks, which offer high-speed internet and support a wide variety of data-intensive applications.

The introduction of touchscreens, mobile apps, and advanced operating systems (like Android and iOS) has transformed mobile phones into powerful tools for both personal and professional use. Apps have turned phones into everything from cameras to GPS devices, fitness trackers, and even virtual assistants. Features such as cameras, high-resolution displays, and large storage capacities have also turned mobile phones into multimedia hubs, allowing users to create, store, and share content in real-time.

Today, mobile phones are ubiquitous, with billions of people around the world relying on them for communication, entertainment, education, and commerce. The advent of 5G technology promises to further revolutionize the mobile experience, enabling faster speeds and supporting the Internet of Things (IoT), autonomous vehicles, and smart cities."
Laptop,"A laptop, also known as a notebook computer, is a portable personal computer that integrates all the essential components of a desktop computer into a single, compact unit. The term 'laptop' was coined to describe a device that could be used on one's lap, unlike traditional desktop computers, which require a fixed workstation. Laptops are equipped with a screen, keyboard, touchpad or pointing device, and internal hardware such as a processor, memory, and storage, all housed in a lightweight, foldable design.

Laptops were initially developed in the 1980s as business tools for mobile professionals who needed computing power while traveling. Early models were limited in functionality and battery life, but technological advancements quickly made them more powerful, versatile, and affordable. Today, laptops are widely used for a variety of purposes, including work, education, entertainment, gaming, and creative tasks like video editing, graphic design, and programming.

Modern laptops come in various forms, ranging from ultra-thin and lightweight models designed for portability to high-performance gaming laptops with dedicated graphics cards and powerful processors. Most laptops feature rechargeable batteries, allowing them to operate without being plugged into a power source for several hours, making them ideal for users on the go.

Laptops run on operating systems such as Microsoft Windows, macOS, or Linux, and offer a full range of connectivity options, including Wi-Fi, Bluetooth, and USB ports. They are also equipped with webcams, microphones, and speakers, making them suitable for video conferencing and multimedia consumption.

In recent years, laptops have evolved to include hybrid models, known as 2-in-1 laptops, which can function as both a traditional laptop and a tablet. These devices feature touchscreen displays and can be used with a stylus for drawing or note-taking.

Laptops have become an essential tool for productivity and entertainment in the digital age, offering users the flexibility to work, learn, and create from virtually anywhere."
Television,"Television (TV) is a telecommunication medium for transmitting moving images and sound. Additionally, the term can refer to a physical television set rather than the medium of transmission. Television is a mass medium for advertising, entertainment, news, and sports. The medium is capable of more than ""radio broadcasting,"" which refers to an audio signal sent to radio receivers.
Television became available in crude experimental forms in the 1920s, but only after several years of further development was the new technology marketed to consumers. After World War II, an improved form of black-and-white television broadcasting became popular in the United Kingdom and the United States, and television sets became commonplace in homes, businesses, and institutions. During the 1950s, television was the primary medium for influencing public opinion. In the mid-1960s, color broadcasting was introduced in the U.S. and most other developed countries.
The availability of various types of archival storage media such as Betamax and VHS tapes, LaserDiscs, high-capacity hard disk drives, CDs, DVDs, flash drives, high-definition HD DVDs and Blu-ray Discs, and cloud digital video recorders has enabled viewers to watch pre-recorded material—such as movies—at home on their own time schedule. For many reasons, especially the convenience of remote retrieval, the storage of television and video programming now also occurs on the cloud (such as the video-on-demand service by Netflix). At the beginning of the 2010s, digital television transmissions greatly increased in popularity. Another development was the move from standard-definition television (SDTV) (576i, with 576 interlaced lines of resolution and 480i) to high-definition television (HDTV), which provides a resolution that is substantially higher. HDTV may be transmitted in different formats: 1080p, 1080i and 720p. Since 2010, with the invention of smart television, Internet television has increased the availability of television programs and movies via the Internet through streaming video services such as Netflix, Amazon Prime Video, iPlayer and Hulu.
In 2013, 79% of the world's households owned a television set. The replacement of earlier cathode-ray tube (CRT) screen displays with compact, energy-efficient, flat-panel alternative technologies such as LCDs (both fluorescent-backlit and LED), OLED displays, and plasma displays was a hardware revolution that began with computer monitors in the late 1990s. Most television sets sold in the 2000s were flat-panel, mainly LEDs. Major manufacturers announced the discontinuation of CRT, Digital Light Processing (DLP), plasma, and even fluorescent-backlit LCDs by the mid-2010s. LEDs are being gradually replaced by OLEDs. Also, major manufacturers have started increasingly producing smart TVs in the mid-2010s. Smart TVs with integrated Internet and Web 2.0 functions became the dominant form of television by the late 2010s.
Television signals were initially distributed only as terrestrial television using high-powered radio-frequency television transmitters to broadcast the signal to individual television receivers. Alternatively, television signals are distributed by coaxial cable or optical fiber, satellite systems, and, since the 2000s, via the Internet. Until the early 2000s, these were transmitted as analog signals, but a transition to digital television was expected to be completed worldwide by the late 2010s. A standard television set consists of multiple internal electronic circuits, including a tuner for receiving and decoding broadcast signals. A visual display device that lacks a tuner is correctly called a video monitor rather than a television.
The television broadcasts are mainly a simplex broadcast meaning that the transmitter cannot receive and the receiver cannot transmit.


== Etymology ==
The word television comes from Ancient Greek  τῆλε (tele) 'far' and Latin  visio 'sight'. The first documented usage of the term dates back to 1900, when the Russian scientist Constantin Perskyi used it in a paper that he presented in French at the first International Congress of Electricity, which ran from 18 to 25 August 1900 during the International World Fair in Paris.
The anglicized version of the term is first attested in 1907, when it was still ""...a theoretical system to transmit moving images over telegraph or telephone wires"". It was ""...formed in English or borrowed from French télévision."" In the 19th century and early 20th century, other ""...proposals for the name of a then-hypothetical technology for sending pictures over distance were telephote (1880) and televista (1904).""
The abbreviation TV is from 1948. The use of the term to mean ""a television set"" dates from 1941. The use of the term to mean ""television as a medium"" dates from 1927.
The term telly is more common in the UK. The slang term ""the tube"" or the ""boob tube"" derives from the bulky cathode-ray tube used on most TVs until the advent of flat-screen TVs. Another slang term for the TV is ""idiot box.""


== History ==


=== Mechanical ===

Facsimile transmission systems for still photographs pioneered methods of mechanical scanning of images in the early 19th century. Alexander Bain introduced the facsimile machine between 1843 and 1846. Frederick Bakewell demonstrated a working laboratory version in 1851. Willoughby Smith discovered the photoconductivity of the element selenium in 1873. As a 23-year-old German university student, Paul Julius Gottlieb Nipkow proposed and patented the Nipkow disk in 1884 in Berlin. This was a spinning disk with a spiral pattern of holes, so each hole scanned a line of the image. Although he never built a working model of the system, variations of Nipkow's spinning-disk ""image rasterizer"" became exceedingly common. Constantin Perskyi had coined the word television in a paper read to the International Electricity Congress at the International World Fair in Paris on 24 August 1900. Perskyi's paper reviewed the existing electromechanical technologies, mentioning the work of Nipkow and others. However, it was not until 1907 that developments in amplification tube technology by Lee de Forest and Arthur Korn, among others, made the design practical.
The first demonstration of the live transmission of images was by Georges Rignoux and A. Fournier in Paris in 1909. A matrix of 64 selenium cells, individually wired to a mechanical commutator, served as an electronic retina. In the receiver, a type of Kerr cell modulated the light, and a series of differently angled mirrors attached to the edge of a rotating disc scanned the modulated beam onto the display screen. A separate circuit regulated synchronization. The 8x8 pixel resolution in this proof-of-concept demonstration was just sufficient to clearly transmit individual letters of the alphabet. An updated image was transmitted ""several times"" each second.
In 1911, Boris Rosing and his student Vladimir Zworykin created a system that used a mechanical mirror-drum scanner to transmit, in Zworykin's words, ""very crude images"" over wires to the ""Braun tube"" (cathode-ray tube or ""CRT"") in the receiver. Moving images were not possible because, in the scanner: ""the sensitivity was not enough and the selenium cell was very laggy"".
In 1921, Édouard Belin sent the first image via radio waves with his belinograph.

By the 1920s, when amplification made television practical, Scottish inventor John Logie Baird employed the Nipkow disk in his prototype video systems. On 25 March 1925, Baird gave the first public demonstration of televised silhouette images in motion at Selfridges's department store in London. Since human faces had inadequate contrast to show up on his primitive system, he televised a ventriloquist's dummy named ""Stooky Bill,"" whose painted face had higher contrast, talking and moving. By 26 January 1926, he had demonstrated before members of the Royal Institution the transmission of an image of a face in motion by radio. This is widely regarded as the world's first true public television demonstration, exhibiting light, shade, and detail. Baird's system used the Nipkow disk for both scanning the image and displaying it. A brightly illuminated subject was placed in front of a spinning Nipkow disk set with lenses that swept images across a static photocell. The thallium sulfide (Thalofide) cell, developed by Theodore Case in the U.S., detected the light reflected from the subject and converted it into a proportional electrical signal. This was transmitted by AM radio waves to a receiver unit, where the video signal was applied to a neon light behind a second Nipkow disk rotating synchronized with the first. The brightness of the neon lamp was varied in proportion to the brightness of each spot on the image. As each hole in the disk passed by, one scan line of the image was reproduced. Baird's disk had 30 holes, producing an image with only 30 scan lines, just enough to recognize a human face. In 1927, Baird transmitted a signal over 438 miles (705 km) of telephone line between London and Glasgow. Baird's original 'televisor' now resides in the Science Museum, South Kensington.
In 1928, Baird's company (Baird Television Development Company/Cinema Television) broadcast the first transatlantic television signal between London and New York and the first shore-to-ship transmission. In 1929, he became involved in the first experimental mechanical television service in Germany. In November of the same year, Baird and Bernard Natan of Pathé established France's first television company, Télévision-Baird-Natan. In 1931, he made the first outdoor remote broadcast of The Derby. In 1932, he demonstrated ultra-short wave television. Baird's mechanical system reached a peak of 240 lines of resolution on BBC telecasts in 1936, though the mechanical system did not scan the televised scene directly. Instead, a 17.5 mm film was shot, rapidly developed, and then scanned while the film was still wet. 
A U.S. inventor, Charles Francis Jenkins, also pioneered the television. He published an article on ""Motion Pictures by Wireless"" in 1913, transmitted moving silhouette images for witnesses in December 1923, and on 13 June 1925, publicly demonstrated synchronized transmission of silhouette pictures. In 1925, Jenkins used the Nipkow disk and transmitted the silhouette image of a toy windmill in motion over a distance of 5 miles (8 km), from a naval radio station in Maryland to his laboratory in Washington, D.C., using a lensed disk scanner with a 48-line resolution. He was granted U.S. Patent No. 1,544,156 (Transmitting Pictures over Wireless) on 30 June 1925 (filed 13 March 1922).
Herbert E. Ives and Frank Gray of Bell Telephone Laboratories gave a dramatic demonstration of mechanical television on 7 April 1927. Their reflected-light television system included both small and large viewing screens. The small receiver had a 2-inch-wide by 2.5-inch-high screen (5 by 6 cm). The large receiver had a screen 24 inches wide by 30 inches high (60 by 75 cm). Both sets could reproduce reasonably accurate, monochromatic, moving images. Along with the pictures, the sets received synchronized sound. The system transmitted images over two paths: first, a copper wire link from Washington to New York City, then a radio link from Whippany, New Jersey. Comparing the two transmission methods, viewers noted no difference in quality. Subjects of the telecast included Secretary of Commerce Herbert Hoover. A flying-spot scanner beam illuminated these subjects. The scanner that produced the beam had a 50-aperture disk. The disc revolved at a rate of 18 frames per second, capturing one frame about every 56 milliseconds. (Today's systems typically transmit 30 or 60 frames per second, or one frame every 33.3 or 16.7 milliseconds, respectively.) Television historian Albert Abramson underscored the significance of the Bell Labs demonstration: ""It was, in fact, the best demonstration of a mechanical television system ever made to this time. It would be several years before any other system could even begin to compare with it in picture quality.""
In 1928, WRGB, then W2XB, was started as the world's first television station. It broadcast from the General Electric facility in Schenectady, NY. It was popularly known as ""WGY Television."" Meanwhile, in the Soviet Union, Leon Theremin had been developing a mirror drum-based television, starting with 16 lines resolution in 1925, then 32 lines, and eventually 64 using interlacing in 1926. As part of his thesis, on 7 May 1926, he electrically transmitted and then projected near-simultaneous moving images on a 5-square-foot (0.46 m2) screen.
By 1927 Theremin had achieved an image of 100 lines, a resolution that was not surpassed until May 1932 by RCA, with 120 lines.
On 25 December 1926, Kenjiro Takayanagi demonstrated a television system with a 40-line resolution that employed a Nipkow disk scanner and CRT display at Hamamatsu Industrial High School in Japan. This prototype is still on display at the Takayanagi Memorial Museum in Shizuoka University, Hamamatsu Campus. His research in creating a production model was halted by the SCAP after World War II.
Because only a limited number of holes could be made in the disks, and disks beyond a certain diameter became impractical, image resolution on mechanical television broadcasts was relatively low, ranging from about 30 lines up to 120 or so. Nevertheless, the image quality of 30-line transmissions steadily improved with technical advances, and by 1933 the UK broadcasts using the Baird system were remarkably clear. A few systems ranging into the 200-line region also went on the air. Two of these were the 180-line system that Compagnie des Compteurs (CDC) installed in Paris in 1935 and the 180-line system that Peck Television Corp. started in 1935 at station VE9AK in Montreal. The advancement of all-electronic television (including image dissectors and other camera tubes and cathode-ray tubes for the reproducer) marked the start of the end for mechanical systems as the dominant form of television. Mechanical television, despite its inferior image quality and generally smaller picture, would remain the primary television technology until the 1930s. The last mechanical telecasts ended in 1939 at stations run by a lot of public universities in the United States.


=== Electronic ===

In 1897, English physicist J. J. Thomson was able, in his three well-known experiments, to deflect cathode rays, a fundamental function of the modern cathode-ray tube (CRT). The earliest version of the CRT was invented by the German physicist Ferdinand Braun in 1897 and is also known as the ""Braun"" tube. It was a cold-cathode diode, a modification of the Crookes tube, with a phosphor-coated screen. Braun was the first to conceive the use of a CRT as a display device. The Braun tube became the foundation of 20th century television. In 1906 the Germans Max Dieckmann and Gustav Glage produced raster images for the first time in a CRT. In 1907, Russian scientist Boris Rosing used a CRT in the receiving end of an experimental video signal to form a picture. He managed to display simple geometric shapes onto the screen.
In 1908, Alan Archibald Campbell-Swinton, a fellow of the Royal Society (UK), published a letter in the scientific journal Nature in which he described how ""distant electric vision"" could be achieved by using a cathode-ray tube, or Braun tube, as both a transmitting and receiving device, he expanded on his vision in a speech given in London in 1911 and reported in The Times and the Journal of the Röntgen Society. In a letter to Nature published in October 1926, Campbell-Swinton also announced the results of some ""not very successful experiments"" he had conducted with G. M. Minchin and J. C. M. Stanton. They had attempted to generate an electrical signal by projecting an image onto a selenium-coated metal plate that was simultaneously scanned by a cathode ray beam. These experiments were conducted before March 1914, when Minchin died, but they were later repeated by two different teams in 1937, by H. Miller and J. W. Strange from EMI, and by H. Iams and A. Rose from RCA. Both teams successfully transmitted ""very faint"" images with the original Campbell-Swinton's selenium-coated plate. Although others had experimented with using a cathode-ray tube as a receiver, the concept of using one as a transmitter was novel. The first cathode-ray tube to use a hot cathode was developed by John B. Johnson (who gave his name to the term Johnson noise) and Harry Weiner Weinhart of Western Electric, and became a commercial product in 1922.
In 1926, Hungarian engineer Kálmán Tihanyi designed a television system using fully electronic scanning and display elements and employing the principle of ""charge storage"" within the scanning (or ""camera"") tube. The problem of low sensitivity to light resulting in low electrical output from transmitting or ""camera"" tubes would be solved with the introduction of charge-storage technology by Kálmán Tihanyi beginning in 1924. His solution was a camera tube that accumulated and stored electrical charges (""photoelectrons"") within the tube throughout each scanning cycle. The device was first described in a patent application he filed in Hungary in March 1926 for a television system he called ""Radioskop"". After further refinements included in a 1928 patent application, Tihanyi's patent was declared void in Great Britain in 1930, so he applied for patents in the United States. Although his breakthrough would be incorporated into the design of RCA's ""iconoscope"" in 1931, the U.S. patent for Tihanyi's transmitting tube would not be granted until May 1939. The patent for his receiving tube had been granted the previous October. Both patents had been purchased by RCA prior to their approval. Charge storage remains a basic principle in the design of imaging devices for television to the present day. On 25 December 1926, at Hamamatsu Industrial High School in Japan, Japanese inventor Kenjiro Takayanagi demonstrated a TV system with a 40-line resolution that employed a CRT display. This was the first working example of a fully electronic television receiver and Takayanagi's team later made improvements to this system parallel to other television developments. Takayanagi did not apply for a patent.
In the 1930s, Allen B. DuMont made the first CRTs to last 1,000 hours of use, one of the factors that led to the widespread adoption of television.
On 7 September 1927, U.S. inventor Philo Farnsworth's image dissector camera tube transmitted its first image, a simple straight line, at his laboratory at 202 Green Street in San Francisco. By 3 September 1928, Farnsworth had developed the system sufficiently to hold a demonstration for the press. This is widely regarded as the first electronic television demonstration. In 1929, the system was improved further by eliminating a motor generator so that his television system had no mechanical parts. That year, Farnsworth transmitted the first live human images with his system, including a three and a half-inch image of his wife Elma (""Pem"") with her eyes closed (possibly due to the bright lighting required).

Meanwhile, Vladimir Zworykin also experimented with the cathode-ray tube to create and show images. While working for Westinghouse Electric in 1923, he began to develop an electronic camera tube. However, in a 1925 demonstration, the image was dim, had low contrast and poor definition, and was stationary. Zworykin's imaging tube never got beyond the laboratory stage. However, RCA, which acquired the Westinghouse patent, asserted that the patent for Farnsworth's 1927 image dissector was written so broadly that it would exclude any other electronic imaging device. Thus, based on Zworykin's 1923 patent application, RCA filed a patent interference suit against Farnsworth. The U.S. Patent Office examiner disagreed in a 1935 decision, finding priority of invention for Farnsworth against Zworykin. Farnsworth claimed that Zworykin's 1923 system could not produce an electrical image of the type to challenge his patent. Zworykin received a patent in 1928 for a color transmission version of his 1923 patent application;, he also divided his original application in 1931. Zworykin was unable or unwilling to introduce evidence of a working model of his tube that was based on his 1923 patent application. In September 1939, after losing an appeal in the courts and being determined to go forward with the commercial manufacturing of television equipment, RCA agreed to pay Farnsworth US$1 million over ten years, in addition to license payments, to use his patents.
In 1933, RCA introduced an improved camera tube that relied on Tihanyi's charge storage principle. Called the ""Iconoscope"" by Zworykin, the new tube had a light sensitivity of about 75,000 lux, and thus was claimed to be much more sensitive than Farnsworth's image dissector. However, Farnsworth had overcome his power issues with his Image Dissector through the invention of a completely unique ""Multipactor"" device that he began work on in 1930, and demonstrated in 1931. This small tube could amplify a signal reportedly to the 60th power or better and showed great promise in all fields of electronics. Unfortunately, an issue with the multipactor was that it wore out at an unsatisfactory rate.

At the Berlin Radio Show in August 1931 in Berlin, Manfred von Ardenne gave a public demonstration of a television system using a CRT for both transmission and reception, the first completely electronic television transmission. However, Ardenne had not developed a camera tube, using the CRT instead as a flying-spot scanner to scan slides and film. Ardenne achieved his first transmission of television pictures on 24 December 1933, followed by test runs for a public television service in 1934. The world's first electronically scanned television service then started in Berlin in 1935, the Fernsehsender Paul Nipkow, culminating in the live broadcast of the 1936 Summer Olympic Games from Berlin to public places all over Germany.
Philo Farnsworth gave the world's first public demonstration of an all-electronic television system, using a live camera, at the Franklin Institute of Philadelphia on 25 August 1934 and for ten days afterward. Mexican inventor Guillermo González Camarena also played an important role in early television. His experiments with television (known as telectroescopía at first) began in 1931 and led to a patent for the ""trichromatic field sequential system"" color television in 1940. In Britain, the EMI engineering team led by Isaac Shoenberg applied in 1932 for a patent for a new device they called ""the Emitron"", which formed the heart of the cameras they designed for the BBC. On 2 November 1936, a 405-line broadcasting service employing the Emitron began at studios in Alexandra Palace and transmitted from a specially built mast atop one of the Victorian building's towers. It alternated briefly with Baird's mechanical system in adjoining studios but was more reliable and visibly superior. This was the world's first regular ""high-definition"" television service.
The original U.S. iconoscope was noisy, had a high ratio of interference to signal, and ultimately gave disappointing results, especially compared to the high-definition mechanical scanning systems that became available. The EMI team, under the supervision of Isaac Shoenberg, analyzed how the iconoscope (or Emitron) produced an electronic signal and concluded that its real efficiency was only about 5% of the theoretical maximum. They solved this problem by developing and patenting in 1934 two new camera tubes dubbed super-Emitron and CPS Emitron. The super-Emitron was between ten and fifteen times more sensitive than the original Emitron and iconoscope tubes, and, in some cases, this ratio was considerably greater. It was used for outside broadcasting by the BBC, for the first time, on Armistice Day 1937, when the general public could watch on a television set as the King laid a wreath at the Cenotaph. This was the first time that anyone had broadcast a live street scene from cameras installed on the roof of neighboring buildings because neither Farnsworth nor RCA would do the same until the 1939 New York World's Fair.

On the other hand, in 1934, Zworykin shared some patent rights with the German licensee company Telefunken. The ""image iconoscope"" (""Superikonoskop"" in Germany) was produced as a result of the collaboration. This tube is essentially identical to the super-Emitron. The production and commercialization of the super-Emitron and image iconoscope in Europe were not affected by the patent war between Zworykin and Farnsworth because Dieckmann and Hell had priority in Germany for the invention of the image dissector, having submitted a patent application for their Lichtelektrische Bildzerlegerröhre für Fernseher (Photoelectric Image Dissector Tube for Television) in Germany in 1925, two years before Farnsworth did the same in the United States. The image iconoscope (Superikonoskop) became the industrial standard for public broadcasting in Europe from 1936 until 1960, when it was replaced by the vidicon and plumbicon tubes. Indeed, it represented the European tradition in electronic tubes competing against the American tradition represented by the image orthicon. The German company Heimann produced the Superikonoskop for the 1936 Berlin Olympic Games, later Heimann also produced and commercialized it from 1940 to 1955; finally the Dutch company Philips produced and commercialized the image iconoscope and multicon from 1952 to 1958.
U.S. television broadcasting, at the time, consisted of a variety of markets in a wide range of sizes, each competing for programming and dominance with separate technology until deals were made and standards agreed upon in 1941. RCA, for example, used only Iconoscopes in the New York area, but Farnsworth Image Dissectors in Philadelphia and San Francisco. In September 1939, RCA agreed to pay the Farnsworth Television and Radio Corporation royalties over the next ten years for access to Farnsworth's patents. With this historic agreement in place, RCA integrated much of what was best about the Farnsworth Technology into their systems. In 1941, the United States implemented 525-line television. Electrical engineer Benjamin Adler played a prominent role in the development of television.
The world's first 625-line television standard was designed in the Soviet Union in 1944 and became a national standard in 1946. The first broadcast in 625-line standard occurred in Moscow in 1948. The concept of 625 lines per frame was subsequently implemented in the European CCIR standard. In 1936, Kálmán Tihanyi described the principle of plasma display, the first flat-panel display system.
Early electronic television sets were large and bulky, with analog circuits made of vacuum tubes. Following the invention of the first working transistor at Bell Labs, Sony founder Masaru Ibuka predicted in 1952 that the transition to electronic circuits made of transistors would lead to smaller and more portable television sets. The first fully transistorized, portable solid-state television set was the 8-inch Sony TV8-301, developed in 1959 and released in 1960. This began the transformation of television viewership from a communal viewing experience to a solitary viewing experience. By 1960, Sony had sold over 4 million portable television sets worldwide.


=== Color ===

The basic idea of using three monochrome images to produce a color image had been experimented with almost as soon as black-and-white televisions had first been built. Although he gave no practical details, among the earliest published proposals for television was one by Maurice Le Blanc in 1880 for a color system, including the first mentions in television literature of line and frame scanning. Polish inventor Jan Szczepanik patented a color television system in 1897, using a selenium photoelectric cell at the transmitter and an electromagnet controlling an oscillating mirror and a moving prism at the receiver. But his system contained no means of analyzing the spectrum of colors at the transmitting end and could not have worked as he described it. Another inventor, Hovannes Adamian, also experimented with color television as early as 1907. The first color television project is claimed by him, and was patented in Germany on 31 March 1908, patent No. 197183, then in Britain, on 1 April 1908, patent No. 7219, in France (patent No. 390326) and in Russia in 1910 (patent No. 17912).
Scottish inventor John Logie Baird demonstrated the world's first color transmission on 3 July 1928, using scanning discs at the transmitting and receiving ends with three spirals of apertures, each spiral with filters of a different primary color, and three light sources at the receiving end, with a commutator to alternate their illumination. Baird also made the world's first color broadcast on 4 February 1938, sending a mechanically scanned 120-line image from Baird's Crystal Palace studios to a projection screen at London's Dominion Theatre. Mechanically scanned color television was also demonstrated by Bell Laboratories in June 1929 using three complete systems of photoelectric cells, amplifiers, glow-tubes, and color filters, with a series of mirrors to superimpose the red, green, and blue images into one full-color image.
The first practical hybrid system was again pioneered by John Logie Baird. In 1940 he publicly demonstrated a color television combining a traditional black-and-white display with a rotating colored disk. This device was very ""deep"" but was later improved with a mirror folding the light path into an entirely practical device resembling a large conventional console. However, Baird was unhappy with the design, and, as early as 1944, had commented to a British government committee that a fully electronic device would be better.
In 1939, Hungarian engineer Peter Carl Goldmark introduced an electro-mechanical system while at CBS, which contained an Iconoscope sensor. The CBS field-sequential color system was partly mechanical, with a disc made of red, blue, and green filters spinning inside the television camera at 1,200 rpm and a similar disc spinning in synchronization in front of the cathode-ray tube inside the receiver set. The system was first demonstrated to the Federal Communications Commission (FCC) on 29 August 1940 and shown to the press on 4 September.
CBS began experimental color field tests using film as early as 28 August 1940 and live cameras by 12 November. NBC (owned by RCA) made its first field test of color television on 20 February 1941. CBS began daily color field tests on 1 June 1941. These color systems were not compatible with existing black-and-white television sets, and, as no color television sets were available to the public at this time, viewing of the color field tests was restricted to RCA and CBS engineers and the invited press. The War Production Board halted the manufacture of television and radio equipment for civilian use from 22 April 1942 to 20 August 1945, limiting any opportunity to introduce color television to the general public.
As early as 1940, Baird had started work on a fully electronic system he called Telechrome. Early Telechrome devices used two electron guns aimed at either side of a phosphor plate. The phosphor was patterned so the electrons from the guns only fell on one side of the patterning or the other. Using cyan and magenta phosphors, a reasonable limited-color image could be obtained. He also demonstrated the same system using monochrome signals to produce a 3D image (called ""stereoscopic"" at the time). A demonstration on 16 August 1944 was the first example of a practical color television system. Work on the Telechrome continued, and plans were made to introduce a three-gun version for full color. However, Baird's untimely death in 1946 ended the development of the Telechrome system.
Similar concepts were common through the 1940s and 1950s, differing primarily in the way they re-combined the colors generated by the three guns. The Geer tube was similar to Baird's concept but used small pyramids with the phosphors deposited on their outside faces instead of Baird's 3D patterning on a flat surface. The Penetron used three layers of phosphor on top of each other and increased the power of the beam to reach the upper layers when drawing those colors. The Chromatron used a set of focusing wires to select the colored phosphors arranged in vertical stripes on the tube.
One of the great technical challenges of introducing color broadcast television was the desire to conserve bandwidth, potentially three times that of the existing black-and-white standards, and not use an excessive amount of radio spectrum. In the United States, after considerable research, the National Television Systems Committee approved an all-electronic system developed by RCA, which encoded the color information separately from the brightness information and significantly reduced the resolution of the color information to conserve bandwidth. As black-and-white televisions could receive the same transmission and display it in black-and-white, the color system adopted is [backwards] ""compatible."" (""Compatible Color,"" featured in RCA advertisements of the period, is mentioned in the song ""America,"" of West Side Story, 1957.) The brightness image remained compatible with existing black-and-white television sets at slightly reduced resolution. In contrast, color televisions could decode the extra information in the signal and produce a limited-resolution color display. The higher-resolution black-and-white and lower-resolution color images combine in the brain to produce a seemingly high-resolution color image. The NTSC standard represented a significant technical achievement.

The first color broadcast (the first episode of the live program The Marriage) occurred on 8 July 1954. However, during the following ten years, most network broadcasts and nearly all local programming continued to be black-and-white. It was not until the mid-1960s that color sets started selling in large numbers, due in part to the color transition of 1965, in which it was announced that over half of all network prime-time programming would be broadcast in color that fall. The first all-color prime-time season came just one year later. In 1972, the last holdout among daytime network programs converted to color, resulting in the first completely all-color network season.
Early color sets were either floor-standing console models or tabletop versions nearly as bulky and heavy, so in practice they remained firmly anchored in one place. GE's relatively compact and lightweight Porta-Color set was introduced in the spring of 1966. It used a transistor-based UHF tuner. The first fully transistorized color television in the United States was the Quasar television introduced in 1967. These developments made watching color television a more flexible and convenient proposition.
In 1972, sales of color sets finally surpassed sales of black-and-white sets. Color broadcasting in Europe was not standardized on the PAL format until the 1960s, and broadcasts did not start until 1967. By this point, many of the technical issues in the early sets had been worked out, and the spread of color sets in Europe was fairly rapid. By the mid-1970s, the only stations broadcasting in black-and-white were a few high-numbered UHF stations in small markets and a handful of low-power repeater stations in even smaller markets such as vacation spots. By 1979, even the last of these had converted to color. By the early 1980s, B&W sets had been pushed into niche markets, notably low-power uses, small portable sets, or for use as video monitor screens in lower-cost consumer equipment. By the late 1980s, even these last holdout niche B&W environments had inevitably shifted to color sets.


=== Digital ===

Digital television (DTV) is the transmission of audio and video by digitally processed and multiplexed signals, in contrast to the analog and channel-separated signals used by analog television. Due to data compression, digital television can support more than one program in the same channel bandwidth. It is an innovative service that represents the most significant evolution in television broadcast technology since color television emerged in the 1950s. Digital television's roots have been tied very closely to the availability of inexpensive, high performance computers. It was not until the 1990s that digital television became possible. Digital television was previously not practically possible due to the impractically high bandwidth requirements of uncompressed digital video, requiring around 200 Mbit/s for a standard-definition television (SDTV) signal, and over 1 Gbit/s for high-definition television (HDTV).
A digital television service was proposed in 1986 by Nippon Telegraph and Telephone (NTT) and the Ministry of Posts and Telecommunication (MPT) in Japan, where there were plans to develop an ""Integrated Network System"" service. However, it was not possible to implement such a digital television service practically until the adoption of DCT video compression technology made it possible in the early 1990s.
In the mid-1980s, as Japanese consumer electronics firms forged ahead with the development of HDTV technology, the MUSE analog format proposed by NHK, a Japanese company, was seen as a pacesetter that threatened to eclipse U.S. electronics companies' technologies. Until June 1990, the Japanese MUSE standard, based on an analog system, was the front-runner among the more than 23 other technical concepts under consideration. Then, a U.S. company, General Instrument, demonstrated the possibility of a digital television signal. This breakthrough was of such significance that the FCC was persuaded to delay its decision on an ATV standard until a digitally-based standard could be developed.
In March 1990, when it became clear that a digital standard was possible, the FCC made several critical decisions. First, the Commission declared that the new ATV standard must be more than an enhanced analog signal but be able to provide a genuine HDTV signal with at least twice the resolution of existing television images. (7) Then, to ensure that viewers who did not wish to buy a new digital television set could continue to receive conventional television broadcasts, it dictated that the new ATV standard must be capable of being ""simulcast"" on different channels. (8) The new ATV standard also allowed the new DTV signal to be based on entirely new design principles. Although incompatible with the existing NTSC standard, the new DTV standard would be able to incorporate many improvements.
The last standards adopted by the FCC did not require a single standard for scanning formats, aspect ratios, or lines of resolution. This compromise resulted from a dispute between the consumer electronics industry (joined by some broadcasters) and the computer industry (joined by the film industry and some public interest groups) over which of the two scanning processes—interlaced or progressive—would be best suited for the newer digital HDTV compatible display devices. Interlaced scanning, which had been specifically designed for older analog CRT display technologies, scans even-numbered lines first, then odd-numbered ones. Interlaced scanning can be regarded as the first video compression model. It was partly developed in the 1940s to double the image resolution to exceed the limitations of television broadcast bandwidth. Another reason for its adoption was to limit the flickering on early CRT screens, whose phosphor-coated screens could only retain the image from the electron scanning gun for a relatively short duration. However, interlaced scanning does not work as efficiently on newer display devices such as Liquid-crystal (LCD), for example, which are better suited to a more frequent progressive refresh rate.
Progressive scanning, the format that the computer industry had long adopted for computer display monitors, scans every line in sequence, from top to bottom. Progressive scanning, in effect, doubles the amount of data generated for every full screen displayed in comparison to interlaced scanning by painting the screen in one pass in 1/60-second instead of two passes in 1/30-second. The computer industry argued that progressive scanning is superior because it does not ""flicker"" on the new standard of display devices in the manner of interlaced scanning. It also argued that progressive scanning enables easier connections with the Internet and is more cheaply converted to interlaced formats than vice versa. The film industry also supported progressive scanning because it offered a more efficient means of converting filmed programming into digital formats. For their part, the consumer electronics industry and broadcasters argued that interlaced scanning was the only technology that could transmit the highest quality pictures then (and currently) feasible, i.e., 1,080 lines per picture and 1,920 pixels per line. Broadcasters also favored interlaced scanning because their vast archive of interlaced programming is not readily compatible with a progressive format. William F. Schreiber, who was director of the Advanced Television Research Program at the Massachusetts Institute of Technology from 1983 until his retirement in 1990, thought that the continued advocacy of interlaced equipment originated from consumer electronics companies that were trying to get back the substantial investments they made in the interlaced technology.
Digital television transition started in late 2000s. All governments across the world set the deadline for analog shutdown by the 2010s. Initially, the adoption rate was low, as the first digital tuner-equipped television sets were costly. However, as the price of digital-capable television sets dropped, more and more households started converting to digital television sets. The transition is expected to be completed worldwide by the mid to late 2010s.


=== Smart television ===

The advent of digital television allowed innovations like smart television sets. A smart television sometimes referred to as a ""connected TV"" or ""hybrid TV,"" is a television set or set-top box with integrated Internet and Web 2.0 features and is an example of technological convergence between computers, television sets, and set-top boxes. Besides the traditional functions of television sets and set-top boxes provided through traditional Broadcasting media, these devices can also provide Internet TV, online interactive media, over-the-top content, as well as on-demand streaming media, and home networking access. These TVs come pre-loaded with an operating system.
Smart TV is not to be confused with Internet TV, Internet Protocol television (IPTV), or with Web TV. Internet television refers to receiving television content over the Internet instead of through traditional systems—terrestrial, cable, and satellite. IPTV is one of the emerging Internet television technology standards for television networks. Web television (WebTV) is a term used for programs created by a wide variety of companies and individuals for broadcast on Internet TV. A first patent was filed in 1994 (and extended the following year) for an ""intelligent"" television system, linked with data processing systems, using a digital or analog network. Apart from being linked to data networks, one key point is its ability to automatically download necessary software routines according to a user's demand and process their needs. Major TV manufacturers announced the production of smart TVs only for middle-end and high-end TVs in 2015. Smart TVs have gotten more affordable compared to when they were first introduced, with 46 million U.S. households having at least one as of 2019.


=== 3D ===

3D television conveys depth perception to the viewer by employing techniques such as stereoscopic display, multi-view display, 2D-plus-depth, or any other form of 3D display. Most modern 3D television sets use an active shutter 3D system or a polarized 3D system, and some are autostereoscopic without the need for glasses. Stereoscopic 3D television was demonstrated for the first time on 10 August 1928, by John Logie Baird in his company's premises at 133 Long Acre, London. Baird pioneered a variety of 3D television systems using electromechanical and cathode-ray tube techniques. The first 3D television was produced in 1935. The advent of digital television in the 2000s greatly improved 3D television sets. Although 3D television sets are quite popular for watching 3D home media, such as on Blu-ray discs, 3D programming has largely failed to make inroads with the public. As a result, many 3D television channels that started in the early 2010s were shut down by the mid-2010s. According to DisplaySearch 3D television shipments totaled 41.45 million units in 2012, compared with 24.14 in 2011 and 2.26 in 2010. As of late 2013, the number of 3D TV viewers started to decline.


== Broadcast systems ==


=== Terrestrial television ===

Programming is broadcast by television stations, sometimes called ""channels,"" as stations are licensed by their governments to broadcast only over assigned channels in the television band. At first, terrestrial broadcasting was the only way television could be widely distributed, and because bandwidth was limited, i.e., there were only a small number of channels available, government regulation was the norm. In the U.S., the Federal Communications Commission (FCC) allowed stations to broadcast advertisements beginning in July 1941 but required public service programming commitments as a requirement for a license. By contrast, the United Kingdom chose a different route, imposing a television license fee on owners of television reception equipment to fund the British Broadcasting Corporation (BBC), which had public service as part of its Royal Charter.
WRGB claims to be the world's oldest television station, tracing its roots to an experimental station founded on 13 January 1928, broadcasting from the General Electric factory in Schenectady, NY, under the call letters W2XB. It was popularly known as ""WGY Television"" after its sister radio station. Later, in 1928, General Electric started a second facility, this one in New York City, which had the call letters W2XBS and which today is known as WNBC. The two stations were experimental and had no regular programming, as receivers were operated by engineers within the company. The image of a Felix the Cat doll rotating on a turntable was broadcast for 2 hours every day for several years as engineers tested new technology. On 2 November 1936, the BBC began transmitting the world's first public regular high-definition service from the Victorian Alexandra Palace in north London. It therefore claims to be the birthplace of television broadcasting as we now know it.
With the widespread adoption of cable across the United States in the 1970s and 1980s, terrestrial television broadcasts have been in decline; in 2013 it was estimated that about 7% of US households used an antenna. A slight increase in use began around 2010 due to switchover to digital terrestrial television broadcasts, which offered pristine image quality over very large areas, and offered an alternative to cable television (CATV) for cord cutters. All other countries around the world are also in the process of either shutting down analog terrestrial television or switching over to digital terrestrial television.


=== Cable television ===
 

Cable television is a system of broadcasting television programming to paying subscribers via radio frequency (RF) signals transmitted through coaxial cables or light pulses through fiber-optic cables. This contrasts with traditional terrestrial television, in which the television signal is transmitted over the air by radio waves and received by a television antenna attached to the television. In the 2000s, FM radio programming, high-speed Internet, telephone service, and similar non-television services may also be provided through these cables. The abbreviation CATV is sometimes used for cable television in the United States. It originally stood for Community Access Television or Community Antenna Television, from cable television's origins in 1948: in areas where over-the-air reception was limited by distance from transmitters or mountainous terrain, large ""community antennas"" were constructed, and cable was run from them to individual homes.


=== Satellite television ===

Satellite television is a system of supplying television programming using broadcast signals relayed from communication satellites. The signals are received via an outdoor parabolic reflector antenna, usually referred to as a satellite dish and a low-noise block downconverter (LNB). A satellite receiver then decodes the desired television program for viewing on a television set. Receivers can be external set-top boxes, or a built-in television tuner. Satellite television provides a wide range of channels and services, especially to geographic areas without terrestrial television or cable television.
The most common method of reception is direct-broadcast satellite television (DBSTV), also known as ""direct to home"" (DTH). In DBSTV systems, signals are relayed from a direct broadcast satellite on the Ku wavelength and are completely digital. Satellite TV systems formerly used systems known as television receive-only. These systems received analog signals transmitted in the C-band spectrum from FSS type satellites and required the use of large dishes. Consequently, these systems were nicknamed ""big dish"" systems and were more expensive and less popular.
The direct-broadcast satellite television signals were earlier analog signals and later digital signals, both of which require a compatible receiver. Digital signals may include high-definition television (HDTV). Some transmissions and channels are free-to-air or free-to-view, while many other channels are pay television requiring a subscription.
In 1945, British science fiction writer Arthur C. Clarke proposed a worldwide communications system that would function by means of three satellites equally spaced apart in Earth orbit. This was published in the October 1945 issue of the Wireless World magazine and won him the Franklin Institute's Stuart Ballantine Medal in 1963.
The first satellite television signals from Europe to North America were relayed via the Telstar satellite over the Atlantic Ocean on 23 July 1962. The signals were received and broadcast in North American and European countries and watched by over 100 million. Launched in 1962, the Relay 1 satellite was the first satellite to transmit television signals from the US to Japan. The first geosynchronous communication satellite, Syncom 2, was launched on 26 July 1963.
The world's first commercial communications satellite, called Intelsat I and nicknamed ""Early Bird"", was launched into geosynchronous orbit on 6 April 1965. The first national network of television satellites, called Orbita, was created by the Soviet Union in October 1967, and was based on the principle of using the highly elliptical Molniya satellite for rebroadcasting and delivering of television signals to ground downlink stations. The first commercial North American satellite to carry television transmissions was Canada's geostationary Anik 1, which was launched on 9 November 1972. ATS-6, the world's first experimental educational and Direct Broadcast Satellite (DBS), was launched on 30 May 1974. It transmitted at 860 MHz using wideband FM modulation and had two sound channels. The transmissions were focused on the Indian subcontinent, but experimenters were able to receive the signal in Western Europe using home-constructed equipment that drew on UHF television design techniques already in use.
The first in a series of Soviet geostationary satellites to carry Direct-To-Home television, Ekran 1, was launched on 26 October 1976. It used a 714 MHz UHF downlink frequency so that the transmissions could be received with existing UHF television technology rather than microwave technology.


=== Internet television ===

Internet television (Internet TV) (or online television) is the digital distribution of television content via the Internet as opposed to traditional systems like terrestrial, cable, and satellite, although the Internet itself is received by terrestrial, cable, or satellite methods. Internet television is a general term that covers the delivery of television series and other video content over the Internet by video streaming technology, typically by major traditional television broadcasters. Internet television should not be confused with Smart TV, IPTV, or with Web TV. Smart television refers to the television set which has a built-in operating system. Internet Protocol television (IPTV) is one of the emerging Internet television technology standards for use by television networks. Web television is a term used for programs created by a wide variety of companies and individuals for broadcast on Internet television.
Traditional cable and satellite television providers began to offer services such as Sling TV, owned by Dish Network, which was unveiled in January 2015. DirecTV, another satellite television provider, launched their own streaming service, DirecTV Stream, in 2016. Sky launched a similar streaming service in the UK called Now. In 2013, Video on demand website Netflix earned the first Primetime Emmy Award nominations for original streaming television at the 65th Primetime Emmy Awards. Three of its series, House of Cards, Arrested Development, and Hemlock Grove, earned nominations that year. On July 13, 2015, cable company Comcast announced an HBO plus broadcast TV package at a price discounted from basic broadband plus basic cable.
In 2017, YouTube launched YouTube TV, a streaming service that allows users to watch live television programs from popular cable or network channels and record shows to stream anywhere, anytime. As of 2017, 28% of US adults cite streaming services as their main means for watching television, and 61% of those ages 18 to 29 cite it as their main method. As of 2018, Netflix is the world's largest streaming TV network and also the world's largest Internet media and entertainment company with 117 million paid subscribers, and by revenue and market cap. In 2020, the COVID-19 pandemic had a strong impact in the television streaming business with the lifestyle changes such as staying at home and lockdowns.


== Sets ==

A television set, also called a television receiver, television, TV set, TV, or ""telly,"" is a device that combines a tuner, display, amplifier, and speakers for the purpose of viewing television and hearing its audio components. Introduced in the late 1920s in mechanical form, television sets became a popular consumer product after World War II in electronic form, using cathode-ray tubes. The addition of color to broadcast television after 1953 further increased the popularity of television sets, and an outdoor antenna became a common feature of suburban homes. The ubiquitous television set became the display device for recorded media in the 1970s, such as Betamax and VHS, which enabled viewers to record TV shows and watch prerecorded movies. In the subsequent decades, Television sets were used to watch DVDs and Blu-ray Discs of movies and other content. Major TV manufacturers announced the discontinuation of CRT, DLP, plasma, and fluorescent-backlit LCDs by the mid-2010s. Televisions since 2010s mostly use LEDs. LEDs are expected to be gradually replaced by OLEDs in the near future.


=== Display technologies ===


==== Disk ====

The earliest systems employed a spinning disk to create and reproduce images. These usually had a low resolution and screen size and never became popular with the public.


==== CRT ====

The cathode-ray tube (CRT) is a vacuum tube containing one or more electron guns (a source of electrons or electron emitter) and a fluorescent screen used to view images. It has the means to accelerate and deflect the electron beam(s) onto the screen to create the images. The images may represent electrical waveforms (oscilloscope), pictures (television, computer monitor), radar targets or others. The CRT uses an evacuated glass envelope that is large, deep (i.e., long from front screen face to rear end), fairly heavy, and relatively fragile. As a matter of safety, the face is typically made of thick lead glass so as to be highly shatter-resistant and to block most X-ray emissions, particularly if the CRT is used in a consumer product.
In television sets and computer monitors, the entire front area of the tube is scanned repetitively and systematically in a fixed pattern called a raster. An image is produced by controlling the intensity of each of the three electron beams, one for each additive primary color (red, green, and blue) with a video signal as a reference. In all modern CRT monitors and televisions, the beams are bent by magnetic deflection, a varying magnetic field generated by coils and driven by electronic circuits around the neck of the tube, although electrostatic deflection is commonly used in oscilloscopes, a type of diagnostic instrument.


==== DLP ====

Digital Light Processing (DLP) is a type of video projector technology that uses a digital micromirror device. Some DLPs have a TV tuner, which makes them a type of TV display. It was originally developed in 1987 by Dr. Larry Hornbeck of Texas Instruments. While the DLP imaging device was invented by Texas Instruments, the first DLP-based projector was introduced by Digital Projection Ltd in 1997. Digital Projection and Texas Instruments were both awarded Emmy Awards in 1998 for the invention of the DLP projector technology. DLP is used in a variety of display applications, from traditional static displays to interactive displays and also non-traditional embedded applications, including medical, security, and industrial uses. DLP technology is used in DLP front projectors (standalone projection units for classrooms and businesses primarily) but also in private homes; in these cases, the image is projected onto a projection screen. DLP is also used in DLP rear projection television sets and digital signs. It is also used in about 85% of digital cinema projection.


==== Plasma ====

A plasma display panel (PDP) is a type of flat-panel display common to large television displays 30 inches (76 cm) or larger. They are called ""plasma"" displays because the technology uses small cells containing electrically charged ionized gases, or what are in essence chambers more commonly known as fluorescent lamps.


==== LCD ====

Liquid-crystal-display televisions (LCD TVs) are television sets that use liquid-crystal display technology to produce images. LCD televisions are much thinner and lighter than cathode-ray tube (CRTs) of similar display size and are available in much larger sizes (e.g., 90-inch diagonal). When manufacturing costs fell, this combination of features made LCDs practical for television receivers. LCDs come in two types: those using cold cathode fluorescent lamps, simply called LCDs, and those using LED as backlight called LEDs.
In 2007, LCD television sets surpassed sales of CRT-based television sets worldwide for the first time, and their sales figures relative to other technologies accelerated. LCD television sets have quickly displaced the only major competitors in the large-screen market, the Plasma display panel and rear-projection television. In mid 2010s LCDs especially LEDs became, by far, the most widely produced and sold television display type. LCDs also have disadvantages. Other technologies address these weaknesses, including OLEDs, FED and SED, but as of 2014 none of these have entered widespread production.


==== OLED ====

An OLED (organic light-emitting diode) is a light-emitting diode (LED) in which the emissive electroluminescent layer is a film of organic compound which emits light in response to an electric current. This layer of organic semiconductor is situated between two electrodes. Generally, at least one of these electrodes is transparent. OLEDs are used to create digital displays in devices such as television screens. It is also used for computer monitors and portable systems such as mobile phones, handheld game console, and PDAs.
There are two main groups of OLED: those based on small molecules and those employing polymers. Adding mobile ions to an OLED creates a light-emitting electrochemical cell or LEC, which has a slightly different mode of operation. OLED displays can use either passive-matrix (PMOLED) or active-matrix (AMOLED) addressing schemes. Active-matrix OLEDs require a thin-film transistor backplane to switch each individual pixel on or off but allow for higher resolution and larger display sizes.
An OLED display works without a backlight. Thus, it can display deep black levels and can be thinner and lighter than a liquid crystal display (LCD). In low ambient light conditions such as a dark room, an OLED screen can achieve a higher contrast ratio than an LCD, whether the LCD uses cold cathode fluorescent lamps or LED backlight. OLEDs are expected to replace other forms of display in the near future.


=== Display resolution ===


==== LD ====

Low-definition television or LDTV refers to television systems that have a lower screen resolution than standard-definition television systems such 240p (320*240). It is used in handheld television. The most common source of LDTV programming is the Internet, where mass distribution of higher-resolution video files could overwhelm computer servers and take too long to download. Many mobile phones and portable devices such as Apple's iPod Nano, or Sony's PlayStation Portable use LDTV video, as higher-resolution files would be excessive to the needs of their small screens (320×240 and 480×272 pixels respectively). The current generation of iPod Nanos has LDTV screens, as do the first three generations of iPod Touch and iPhone (480×320). For the first years of its existence, YouTube offered only one low-definition resolution of 320x240p at 30fps or less. A standard, consumer-grade videotape can be considered SDTV due to its resolution (approximately 360 × 480i/576i).


==== SD ====

Standard-definition television or SDTV refers to two different resolutions: 576i, with 576 interlaced lines of resolution, derived from the European-developed PAL and SECAM systems, and 480i based on the American National Television System Committee NTSC system. SDTV is a television system that uses a resolution that is not considered to be either high-definition television (720p, 1080i, 1080p, 1440p, 4K UHDTV, and 8K UHD) or enhanced-definition television (EDTV 480p). In North America, digital SDTV is broadcast in the same 4:3 aspect ratio as NTSC signals, with widescreen content being center cut. However, in other parts of the world that used the PAL or SECAM color systems, standard-definition television is now usually shown with a 16:9 aspect ratio, with the transition occurring between the mid-1990s and mid-2000s. Older programs with a 4:3 aspect ratio are shown in the United States as 4:3, with non-ATSC countries preferring to reduce the horizontal resolution by anamorphically scaling a pillarboxed image.


==== HD ====

High-definition television (HDTV) provides a resolution that is substantially higher than that of standard-definition television.
HDTV may be transmitted in various formats:

1080p: 1920×1080p: 2,073,600 pixels (~2.07 megapixels) per frame
1080i: 1920×1080i: 1,036,800 pixels (~1.04 MP) per field or 2,073,600 pixels (~2.07 MP) per frame
A non-standard CEA resolution exists in some countries such as 1440×1080i: 777,600 pixels (~0.78 MP) per field or 1,555,200 pixels (~1.56 MP) per frame
720p: 1280×720p: 921,600 pixels (~0.92 MP) per frame


==== UHD ====

Ultra-high-definition television (also known as Super Hi-Vision, Ultra HD television, UltraHD, UHDTV, or UHD) includes 4K UHD (2160p) and 8K UHD (4320p), which are two digital video formats proposed by NHK Science & Technology Research Laboratories and defined and approved by the International Telecommunication Union (ITU). The Consumer Electronics Association announced on 17 October 2012 that ""Ultra High Definition,"" or ""Ultra HD,"" would be used for displays that have an aspect ratio of at least 16:9 and at least one digital input capable of carrying and presenting natural video at a minimum resolution of 3840×2160 pixels.


=== Market share ===
North American consumers purchase a new television set on average every seven years, and the average household owns 2.8 televisions. As of 2011, 48 million are sold each year at an average price of $460 and size of 38 in (97 cm).


== Content ==


=== Programming ===

Getting TV programming shown to the public can happen in many other ways. After production, the next step is to market and deliver the product to whichever markets are open to using it. This typically happens on two levels:

Original run or First run: a producer creates a program of one or multiple episodes and shows it on a station or network that has either paid for the production itself or granted a license by the television producers to do the same.
Broadcast syndication: this is the terminology rather broadly used to describe secondary programming usages (beyond the original run). It includes secondary runs in the country of the first issue, but also international usage, which may not be managed by the originating producer. In many cases, other companies, television stations, or individuals are engaged to do the syndication work, in other words, to sell the product into the markets they are allowed to sell into by contract from the copyright holders; in most cases, the producers.
First-run programming is increasing on subscription services outside of the United States, but few domestically produced programs are syndicated on domestic free-to-air (FTA) elsewhere. This practice is increasing, however, generally on digital-only FTA channels or with subscriber-only, first-run material appearing on FTA. Unlike the United States, repeat FTA screenings of an FTA network program usually only occur on that network. Also, affiliates rarely buy or produce non-network programming that is not focused on local programming.


=== Genres ===

Television genres include a broad range of programming types that entertain, inform, and educate viewers. The most expensive entertainment genres to produce are usually dramas and dramatic miniseries. However, other genres, such as historical Western genres, may also have high production costs.
Pop culture entertainment genres include action-oriented shows such as police, crime, detective dramas, horror, or thriller shows. As well, there are also other variants of the drama genre, such as medical dramas and daytime soap operas. Sci-fi series can fall into either the drama or action category, depending on whether they emphasize philosophical questions or high adventure. Comedy is a popular genre that includes situation comedy (sitcom) and animated series for the adult demographic, such as Comedy Central's South Park.
The least expensive forms of entertainment programming genres are game shows, talk shows, variety shows, and reality television. Game shows feature contestants answering questions and solving puzzles to win prizes. Talk shows contain interviews with film, television, music, and sports celebrities and public figures. Variety shows feature a range of musical performers and other entertainers, such as comedians and magicians, introduced by a host or Master of Ceremonies. There is some crossover between some talk shows and variety shows because leading talk shows often feature performances by bands, singers, comedians, and other performers in between the interview segments. Reality television series ""regular"" people (i.e., not actors) facing unusual challenges or experiences ranging from arrest by police officers (COPS) to significant weight loss (The Biggest Loser). A derived version of reality shows depicts celebrities doing mundane activities such as going about their everyday life (The Osbournes, Snoop Dogg's Father Hood) or doing regular jobs (The Simple Life).
Fictional television programs that some television scholars and broadcasting advocacy groups argue are ""quality television"", include series such as Twin Peaks and The Sopranos. Kristin Thompson argues that some of these television series exhibit traits also found in art films, such as psychological realism, narrative complexity, and ambiguous plotlines. Nonfiction television programs that some television scholars and broadcasting advocacy groups argue are ""quality television"" include a range of serious, noncommercial programming aimed at a niche audience, such as documentaries and public affairs shows.


=== Funding ===

Around the world, broadcast television is financed by government, advertising, licensing (a form of tax), subscription, or any combination of these. To protect revenues, subscription television channels are usually encrypted to ensure that only subscribers receive the decryption codes to see the signal. Unencrypted channels are known as free-to-air or FTA. In 2009, the global TV market represented 1,217.2 million TV households with at least one TV and total revenues of 268.9 billion EUR (declining 1.2% compared to 2008). North America had the biggest TV revenue market share with 39% followed by Europe (31%), Asia-Pacific (21%), Latin America (8%), and Africa and the Middle East (2%). Globally, the different TV revenue sources are divided into 45–50% TV advertising revenues, 40–45% subscription fees, and 10% public funding.


==== Advertising ====

Television's broad reach makes it a powerful and attractive medium for advertisers. Many television networks and stations sell blocks of broadcast time to advertisers (""sponsors"") to fund their programming. Television advertisements (variously called a television commercial, commercial, or ad in American English, and known in British English as an advert) is a span of television programming produced and paid for by an organization, which conveys a message, typically to market a product or service. Advertising revenue provides a significant portion of the funding for most privately owned television networks. The vast majority of television advertisements today consist of brief advertising spots, ranging in length from a few seconds to several minutes (as well as program-length infomercials). Advertisements of this sort have been used to promote a wide variety of goods, services, and ideas since the beginning of television.

The effects of television advertising upon the viewing public (and the effects of mass media in general) have been the subject of discourse by philosophers, including Marshall McLuhan. The viewership of television programming, as measured by companies such as Nielsen Media Research, is often used as a metric for television advertisement placement and, consequently, for the rates charged to advertisers to air within a given network, television program, or time of day (called a ""daypart""). In many countries, including the United States, television campaign advertisements is considered indispensable for a political campaign. In other countries, such as France, political advertising on television is heavily restricted, while some countries, such as Norway, completely ban political advertisements.
The first official, paid television advertisement was broadcast in the United States on 1 July 1941, over New York station WNBT (now WNBC) before a baseball game between the Brooklyn Dodgers and Philadelphia Phillies. The announcement for Bulova watches, for which the company paid anywhere from $4.00 to $9.00 (reports vary), displayed a WNBT test pattern modified to look like a clock with the hands showing the time. The Bulova logo, with the phrase ""Bulova Watch Time,"" was shown in the lower right-hand quadrant of the test pattern while the second hand swept around the dial for one minute. The first TV ad broadcast in the U.K. was on ITV on 22 September 1955, advertising Gibbs SR toothpaste. The first TV ad broadcast in Asia was on Nippon Television in Tokyo on 28 August 1953, advertising Seikosha (now Seiko), which also displayed a clock with the current time.


==== United States ====
Since inception in the US in 1941, television commercials have become one of the most effective, persuasive, and popular methods of selling products of many sorts, especially consumer goods. During the 1940s and into the 1950s, programs were hosted by single advertisers. This, in turn, gave great creative control to the advertisers over the content of the show. Perhaps due to the quiz show scandals in the 1950s, networks shifted to the magazine concept, introducing advertising breaks with other advertisers.
U.S. advertising rates are determined primarily by Nielsen ratings. The time of the day and popularity of the channel determine how much a TV commercial can cost. For example, it can cost approximately $750,000 for a 30-second block of commercial time during the highly popular singing competition American Idol, while the same amount of time for the Super Bowl can cost several million dollars. Conversely, lesser-viewed time slots, such as early mornings and weekday afternoons, are often sold in bulk to producers of infomercials at far lower rates. In recent years, paid programs or infomercials have become common, usually in lengths of 30 minutes or one hour. Some drug companies and other businesses have even created ""news"" items for broadcast, known in the industry as video news releases, paying program directors to use them.
Some television programs also deliberately place products into their shows as advertisements, a practice started in feature films and known as product placement. For example, a character could be drinking a certain kind of soda, going to a particular chain restaurant, or driving a certain make of car. (This is sometimes very subtle, with shows having vehicles provided by manufacturers for low cost in exchange as a product placement). Sometimes, a specific brand or trade mark, or music from a certain artist or group, is used. (This excludes guest appearances by artists who perform on the show.)


==== United Kingdom ====
The TV regulator oversees TV advertising in the United Kingdom. Its restrictions have applied since the early days of commercially funded TV. Despite this, an early TV mogul, Roy Thomson, likened the broadcasting license as being a ""license to print money"". Restrictions mean that the big three national commercial TV channels: ITV, Channel 4, and Channel 5 can show an average of only seven minutes of advertising per hour (eight minutes in the peak period). Other broadcasters must average no more than nine minutes (twelve in the peak). This means that many imported TV shows from the U.S. have unnatural pauses where the British company does not use the narrative breaks intended for more frequent U.S. advertising. Advertisements must not be inserted in the course of certain specific proscribed types of programs that last less than half an hour in scheduled duration; this list includes any news or current affairs programs, documentaries, and programs for children; additionally, advertisements may not be carried in a program designed and broadcast for reception in schools or in any religious broadcasting service or other devotional program or during a formal Royal ceremony or occasion. There also must be clear demarcations in time between the programs and the advertisements. The BBC, being strictly non-commercial, is not allowed to show adverts on television in the U.K., though it has advertising-funded channels abroad. The majority of its budget comes from television license fees (see below) and broadcast syndication, the sale of content to other broadcasters.


==== Ireland ====
Broadcast advertising is regulated by the Broadcasting Authority of Ireland.


==== Subscription ====
Some TV channels are partly funded from subscriptions; therefore, the signals are encrypted during the broadcast to ensure that only the paying subscribers have access to the decryption codes to watch pay television or specialty channels. Most subscription services are also funded by advertising.


==== Taxation or license ====
Television services in some countries may be funded by a television licence or a form of taxation, which means that advertising plays a lesser role or no role at all. For example, some channels may carry no advertising at all and some very little, including:

Australia (ABC Television)
Belgium (VRT for Flanders and RTBF for Wallonia)
Denmark (DR)
Ireland (RTÉ)
Japan (NHK)
Norway (NRK)
Sweden (SVT)
Switzerland (SRG SSR)
Republic of China (Taiwan) (PTS)
United Kingdom (BBC Television)
United States (PBS)
The British Broadcasting Corporation's TV service carries no television advertising on its UK channels and is funded by an annual television license paid by the occupiers of premises receiving live telecasts. As of 2012 it was estimated that approximately 26.8 million UK private domestic households owned televisions, with approximately 25 million TV licences in all premises in force as of 2010. This television license fee is set by the government, but the BBC is not answerable to or controlled by the government. As of 2009 two main BBC TV channels were watched by almost 90% of the population each week and overall had 27% share of total viewing, despite the fact that 85% of homes were multi-channel, with 42% of these having access to 200 free-to-air channels via satellite and another 43% having access to 30 or more channels via Freeview. As of June 2021 the licence that funds the advertising-free BBC TV channels cost £159 for a colour TV Licence and £53.50 for a black and white TV Licence (free or reduced for some groups).
The Australian Broadcasting Corporation's television services in Australia carry no advertising by external sources; it is banned under the Australian Broadcasting Corporation Act 1983, which also ensures its editorial independence. The ABC receives most of its funding from the Australian Government (some revenue is received from its Commercial division), but it has suffered progressive funding cuts under Liberal governments since the 1996 Howard government, with particularly deep cuts in 2014 under the Turnbull government, and an ongoing indexation freeze as of 2021. The funds provide for the ABC's television, radio, online, and international outputs, although ABC Australia, which broadcasts throughout the Asia-Pacific region, receives additional funds through DFAT and some advertising on the channel.
In France, government-funded channels carry advertisements, yet those who own television sets have to pay an annual tax (""la redevance audiovisuelle"").
In Japan, NHK is paid for by license fees (known in Japanese as reception fee (受信料, Jushinryō)). The broadcast law that governs NHK's funding stipulates that any television equipped to receive NHK is required to pay. The fee is standardized, with discounts for office workers and students who commute, as well as a general discount for residents of Okinawa prefecture.


=== Broadcast programming ===

Broadcast programming, or TV listings in the United Kingdom, is the practice of organizing television programs in a schedule, with broadcast automation used to regularly change the scheduling of TV programs to build an audience for a new show, retain that audience, or compete with other broadcasters' programs.


== Social aspects ==

Television has played a pivotal role in the socialization of the 20th and 21st centuries. There are many aspects of television that can be addressed, including negative issues such as media violence. Current research is discovering that individuals suffering from social isolation can employ television to create what is termed a parasocial or faux relationship with characters from their favorite television shows and movies as a way of deflecting feelings of loneliness and social deprivation. Several studies have found that educational television has many advantages. The article ""The Good Things about Television"" argues that television can be a very powerful and effective learning tool for children if used wisely. With respect to faith, many Christian denominations use television for religious broadcasting.


=== Religious opposition ===
Methodist denominations in the conservative holiness movement, such as the Allegheny Wesleyan Methodist Connection and the Evangelical Wesleyan Church, eschew the use of the television. Some Baptists, such as those affiliated with Pensacola Christian College, also eschew television. Many Traditional Catholic congregations such as the Society of Saint Pius X (SSPX), as with Laestadian Lutherans, and Conservative Anabaptists such as the Dunkard Brethren Church, oppose the presence of television in the household, teaching that it is an occasion of sin.


== Negative impacts ==
Children, especially those aged five or younger, are at risk of injury from falling televisions. A CRT-style television that falls on a child will, because of its weight, hit with the equivalent force of falling multiple stories from a building. Newer flat-screen televisions are ""top-heavy and have narrow bases"", which means that a small child can easily pull one over. As of 2015, TV tip-overs were responsible for more than 10,000 injuries per year to children in the United States, at a cost of more than US$8 million per year (equivalent to US$10.28 million per year in 2023) in emergency care.
A 2017 study in The Journal of Human Resources found that exposure to cable television reduced cognitive ability and high school graduation rates for boys. This effect was stronger for boys from more educated families. The article suggests a mechanism where light television entertainment crowds out more cognitively stimulating activities.
With high lead content in CRTs and the rapid diffusion of new flat-panel display technologies, some of which (LCDs) use lamps which contain mercury, there is growing concern about electronic waste from discarded televisions. Related occupational health concerns exist, as well, for disassemblers removing copper wiring and other materials from CRTs. Further environmental concerns related to television design and use relate to the devices' increasing electrical energy requirements.


== See also ==


== References ==


== Further reading ==
Abramson, Albert (2003). The History of Television, 1942 to 2000. Jefferson, NC / London: McFarland. ISBN 978-0-7864-1220-4.
Pierre Bourdieu, On Television, The New Press, 2001.
Tim Brooks and Earle March, The Complete Guide to Prime Time Network and Cable TV Shows, 8th ed., Ballantine, 2002.
Jacques Derrida and Bernard Stiegler, Echographies of Television, Polity Press, 2002.
David E. Fisher and Marshall J. Fisher, Tube: the Invention of Television, Counterpoint, Washington, D.C., 1996, ISBN 1-887178-17-1.
Steven Johnson, Everything Bad is Good for You: How Today's Popular Culture Is Actually Making Us Smarter, New York, Riverhead (Penguin), 2005, 2006, ISBN 1-59448-194-6.
Leggett, Julian (April 1941). ""Television in Color"". Popular Mechanics. Chicago. Retrieved 7 December 2014.
Jerry Mander, Four Arguments for the Elimination of Television, Perennial, 1978.
Jerry Mander, In the Absence of the Sacred, Sierra Club Books, 1992, ISBN 0-87156-509-9.
Neil Postman, Amusing Ourselves to Death: Public Discourse in the Age of Show Business, New York, Penguin US, 1985, ISBN 0-670-80454-1.
Evan I. Schwartz, The Last Lone Inventor: A Tale of Genius, Deceit, and the Birth of Television, New York, Harper Paperbacks, 2003, ISBN 0-06-093559-6.
Beretta E. Smith-Shomade, Shaded Lives: African-American Women and Television, Rutgers University Press, 2002.
Alan Taylor, We, the Media: Pedagogic Intrusions into US Mainstream Film and Television News Broadcasting Rhetoric, Peter Lang, 2005, ISBN 3-631-51852-8.
Amanda D. Lotz, The Television Will Be Revolutionized, New York University Press, ISBN 978-0-8147-5220-3


== External links ==

Television at Curlie"
Taj Mahal,No Wikipedia page found for Taj Mahal
Chart,"A chart (sometimes known as a graph) is a graphical representation for data visualization, in which ""the data is represented by symbols, such as bars in a bar chart, lines in a line chart, or slices in a pie chart"". A chart can represent tabular numeric data, functions or some kinds of quality structure and provides different info.
The term ""chart"" as a graphical representation of data has multiple meanings:

A data chart is a type of diagram or graph, that organizes and represents a set of numerical or qualitative data.
Maps that are adorned with extra information (map surround) for a specific purpose are often known as charts, such as a nautical chart or aeronautical chart, typically spread over several map sheets.
Other domain-specific constructs are sometimes called charts, such as the chord chart in music notation or a record chart for album popularity.
Charts are often used to ease understanding of large quantities of data and the relationships between parts of the data. Charts can usually be read more quickly than the raw data. They are used in a wide variety of fields, and can be created by hand (often on graph paper) or by computer using a charting application. Certain types of charts are more useful for presenting a given data set than others. For example, data that presents percentages in different groups (such as ""satisfied, not satisfied, unsure"") are often displayed in a pie chart, but maybe more easily understood when presented in a horizontal bar chart. On the other hand, data that represents numbers that change over a period of time (such as ""annual revenue from 1990 to 2000"") might be best shown as a line chart.


== Features ==
A chart can take a large variety of forms. However, there are common features that provide the chart with its ability to extract meaning from data.
Typically the data in a chart is represented graphically since humans can infer meaning from pictures more quickly than from text. Thus, the text is generally used only to annotate the data.
One of the most important uses of text in a graph is the title. A graph's title usually appears above the main graphic and provides a succinct description of what the data in the graph refers to.
Dimensions in the data are often displayed on axes. If a horizontal and a vertical axis are used, they are usually referred to as the x-axis and y-axis. Each axis will have a scale, denoted by periodic graduations and usually accompanied by numerical or categorical indications. Each axis will typically also have a label displayed outside or beside it, briefly describing the dimension represented. If the scale is numerical, the label will often be suffixed with the unit of that scale in parentheses. For example, ""Distance traveled (m)"" is a typical x-axis label and would mean that the distance traveled, in units of meters, is related to the horizontal position of the data within the chart.
Within the graph, a grid of lines may appear to aid in the visual alignment of data. The grid can be enhanced by visually emphasizing the lines at regular or significant graduations. The emphasized lines are then called major gridlines, and the remainder is minor grid lines.
A chart's data can appear in all manner of formats and may include individual textual labels describing the datum associated with the indicated position in the chart. The data may appear as dots or shapes, connected or unconnected, and in any combination of colors and patterns. In addition, inferences or points of interest can be overlaid directly on the graph to further aid information extraction.
When the data appearing in a chart contains multiple variables, the chart may include a legend (also known as a key). A legend contains a list of the variables appearing in the chart and an example of their appearance. This information allows the data from each variable to be identified in the chart.


== Types ==


=== Common charts ===
Four of the most common charts are:

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		

This gallery shows:

A histogram consists of tabular frequencies, shown as adjacent rectangles, erected over discrete intervals (bins), with an area equal to the frequency of the observations in the interval; first introduced by Karl Pearson.
A bar chart is a chart with rectangular bars with lengths proportional to the values that they represent. The bars can be plotted vertically or horizontally. The first known bar charts are usually attributed to Nicole Oresme, Joseph Priestley, and William Playfair.
A pie chart shows percentage values as a slice of a pie; first introduced by William Playfair.
A line chart is a two-dimensional scatterplot of ordered observations where the observations are connected following their order. The first known line charts are usually credited to Francis Hauksbee, Nicolaus Samuel Cruquius, Johann Heinrich Lambert and William Playfair.
Other common charts are:

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		


=== Less-common charts ===
Examples of less common charts are:

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		

This gallery shows:

A bubble chart is a two-dimensional scatterplot where a third variable is represented by the size of the points.
A polar area diagram, sometimes called a Coxcomb chart, is an enhanced form of pie chart developed by Florence Nightingale.
A radar chart or ""spider chart"" or ""doi"" is a two-dimensional chart of three or more quantitative variables represented on axes starting from the same point.
A waterfall chart also known as a ""Walk"" chart, is a special type of floating-column chart.
A tree map where the areas of the rectangles correspond to values.  Other dimensions can be represented with color or hue. Smaller areas go to the bottom right corner.
A streamgraph, a stacked, curvilinear area graph displaced around a central axis
A GapChart, a time series chart showing evolving gaps and equalities between series. Other dimensions can be represented with color or hue.


=== Field-specific charts ===
Some types of charts have specific uses in a certain field

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		

This gallery shows:

Stock market prices are often depicted with an open-high-low-close chart with a traditional bar chart of volume at the bottom.
Candlestick charts are another type of bar chart used to describe price movements of an equity over time.
A Kagi chart is a time-independent stock tracking chart that attempts to minimise noise.
Alternatively, where less detail is required, and chart size is paramount, a Sparkline may be used.
Other examples:

Interest rates, temperatures, etc., at the close of the period are plotted with a line chart.
Project planners use a Gantt chart to show the timing of tasks as they occur over time.


=== Well-known named charts ===
Some of the better-known named charts are:

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		

Some specific charts have become well known by effectively explaining a phenomenon or idea.

An Allele chart is a chart originating from the study of genetics to show the interaction of two data points in a grid.
A Gantt chart helps in scheduling complex projects.
The Nolan chart and the Pournelle chart classify political philosophies according to two axes of variation.
A PERT chart is often used in project management.
The Smith chart serves in radio electronics.


=== Other charts ===
There are dozens of other types of charts. Here are some of them:

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		

One more example: Bernal chart


=== Common plots ===

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		


== Chart software ==
While charts can be drawn by hand, computer software is often used to automatically produce a chart based on entered data. For examples of commonly used software tools, see List of charting software.


== See also ==

Diagram
Table (information)
Drakon-chart
Exploratory data analysis
Graphic organizer
Information graphics
Mathematical diagram
Official statistics
Plot (graphics)
Edward Tufte
Misleading graph


== References ==


== Further reading ==

Brinton, Willard Cope. Graphic methods for presenting facts. The Engineering magazine company, 1914.
Karsten, Karl G. Charts and graphs: An introduction to graphic methods in the control and analysis of statistics. Prentice-Hall, 1923, 1925."
laptop,"A laptop computer or notebook computer, also known as a laptop or notebook, is a small, portable personal computer (PC). Laptops typically have a clamshell form factor with a flat-panel screen on the inside of the upper lid and an alphanumeric keyboard and pointing device on the inside of the lower lid. Most of the computer's internal hardware is fitted inside the lower lid enclosure under the keyboard, although many modern laptops have a built-in webcam at the top of the screen, and some even feature a touchscreen display. In most cases, unlike tablet computers which run on mobile operating systems, laptops tend to run on desktop operating systems, which were originally developed for desktop computers.
Laptops can run on both AC power and rechargable battery packs and can be folded shut for convenient storage and transportation, making them suitable for mobile use. Laptops are used in a variety of settings, such as at work (especially on business trips), in education, for playing games, web browsing, for personal multimedia, and for general home computer use.
The word laptop, modeled after the term desktop (as in desktop computer), refers to the fact that the computer can be practically placed on the user's lap; while the word notebook refers to most laptops sharing a form factor with paper notebooks. As of 2024, in American English, the terms laptop and notebook are used interchangeably; in other dialects of English, one or the other may be preferred. The term notebook originally referred to a type of portable computer that was smaller and lighter than mainstream laptops of the time, but has since come to mean the same thing and no longer refers to any specific size.
Laptops combine many of the input/output components and capabilities of a desktop computer into a single unit, including a display screen (usually 11–17 in or 280–430 mm in diagonal size), small speakers, a keyboard, and a pointing device (namely compact ones such as touchpads or pointing sticks). Most modern laptops include a built-in webcam and microphone, and many also have touchscreens. Hardware specifications may vary significantly between different types, models, and price points.
Design elements, form factors, and construction can also vary significantly between models depending on the intended use. Examples of specialized models of laptops include 2-in-1 laptops, with keyboards that either be detached or pivoted out of view from the display (often marketed having a ""laptop mode""); rugged laptops, for use in construction or military applications; and low-production-cost laptops such as those from the One Laptop per Child (OLPC) organization, which incorporate features like solar charging and semi-flexible components not found on most laptop computers. Portable computers, which later developed into modern laptops, were originally considered to be a small niche market, mostly for specialized field applications, such as in the military, for accountants, or traveling sales representatives. As portable computers evolved into modern laptops, they became widely used for a variety of purposes.


== History ==

The history of the laptop follows closely behind the development of the personal computer itself. A ""personal, portable information manipulator"" was imagined by Alan Kay at Xerox PARC in 1968, and described in his 1972 paper as the ""Dynabook"". The IBM Special Computer APL Machine Portable (SCAMP) was demonstrated in 1973. This prototype was based on the IBM PALM processor. The IBM 5100, the first commercially available portable computer, appeared in September 1975, and was based on the SCAMP prototype.

As 8-bit CPU machines became widely accepted, the number of portables increased rapidly. The first ""laptop-sized notebook computer"" was the Epson HX-20, invented (patented) by Suwa Seikosha's Yukio Yokozawa in July 1980, introduced at the COMDEX computer show in Las Vegas by Japanese company Seiko Epson in 1981, and released in July 1982. It had an LCD screen, a rechargeable battery, and a calculator-size printer, in a 1.6 kg (3.5 lb) chassis, the size of an A4 notebook. It was described as a ""laptop"" and ""notebook"" computer in its patent.

Both Tandy/RadioShack and Hewlett-Packard (HP) also produced portable computers of varying designs during this period. The first laptops using the flip form factor appeared in the early 1980s. The Dulmont Magnum was released in Australia in 1981–82, but was not marketed internationally until 1984–85. The US$8,150 (equivalent to $25,730 in 2023) GRiD Compass 1101, released in 1982, was used at NASA and by the military, among others. The Sharp PC-5000, the Ampere WS-1, and Gavilan SC were released between 1983 and 1985. The Toshiba T1100 won acceptance by PC experts and the mass market as a way to have PC portability.
From 1983 onward, several new input techniques were developed and included in laptops, including the touch pad (Gavilan SC, 1983), the pointing stick (IBM ThinkPad 700, 1992), and handwriting recognition (Linus Write-Top, 1987). Some CPUs, such as the 1990 Intel i386SL, were designed to use minimum power to increase battery life of portable computers and were supported by dynamic power management features such as Intel SpeedStep and AMD PowerNow! in some designs.
Some laptops in the 1980s using red plasma displays could only be used when connected to AC power, and had a built in power supply.
The development of memory cards was driven in the 1980s by the need for a floppy-disk-drive alternative, having lower power consumption, less weight, and reduced volume in laptops. The Personal Computer Memory Card International Association (PCMCIA) was an industry association created in 1989 to promote a standard for memory cards in PCs. The specification for PCMCIA type I cards, later renamed PC Cards, was first released in 1990.

Displays reached 640x480 (VGA) resolution by 1988 (Compaq SLT/286), and color screens started becoming a common upgrade in 1991, with increases in resolution and screen size occurring frequently until the introduction of 17"" screen laptops in 2003. Hard drives started to be used in portables, encouraged by the introduction of 3.5"" drives in the late 1980s, and became common in laptops starting with the introduction of 2.5"" and smaller drives around 1990; capacities have typically lagged behind those of physically larger desktop drives.
Resolutions of laptop webcams are 720p (HD), or 480p in lower-end laptops. The earliest-known laptops with 1080p (Full HD) webcams like the Samsung 700G7C were released in the early 2010s.
Optical disc drives became common in full-size laptops around 1997: initially CD-ROM drives, supplanted by CD-R, then DVD, then Blu-ray drives with writing capability. Starting around 2011, the trend shifted against internal optical drives, and as of 2022, they have largely disappeared, though are still readily available as external peripherals.
In 2021, Dell showed Concept Luna, which is a concept for a laptop that can be easily dissassembled.


== Etymology ==
The terms laptop and notebook both trace their origins to the early 1980s, coined to describe portable computers in a size class smaller than the contemporary mainstream units (so-called ""luggables"") but larger than pocket computers. The etymologist William Safire traced the origin of laptop to some time before 1984; the earliest attestation of laptop found by the Oxford English Dictionary dates to 1983. The word is modeled after the term desktop, as in desktop computer. Notebook, meanwhile, emerged earlier in 1982 to describe Epson's HX-20 portable, whose dimensions roughly correspond to a letter-sized pad of paper.: 9  Notebooks emerged as their own separate market from laptops with the release of the NEC UltraLite in 1988.: 16  Notebooks and laptops continued to occupy distinct market segments into the mid-1990s, but ergonomic considerations and customer preference for larger screens soon led to notebooks converging with laptops in the late 1990s. Today, the terms laptop and notebook are synonymous, with laptop being the more common term in most English-speaking territories.


== Types of laptops ==

Since the 1970s introduction of portable computers, their forms have changed significantly, spawning a variety of visually and technologically differing subclasses. Excepting distinct legal trademark around terms (notably Ultrabook), hard distinctions between these classes were rare, and their usage has varied over time and between sources. Since the late 2010s, more specific terms have become less commonly used, with sizes distinguished largely by the size of the screen.


=== Smaller and larger laptops ===

There were in the past a number of marketing categories for smaller and larger laptop computers; these included ""notebook"" and ""subnotebook"" models, low cost ""netbooks"", and ""ultra-mobile PCs"" where the size class overlapped with devices like smartphone and handheld tablets, and ""Desktop replacement"" laptops for machines notably larger and heavier than typical to operate more powerful processors or graphics hardware. All of these terms have fallen out of favor as the size of mainstream laptops has gone down and their capabilities have gone up; except for niche models, laptop sizes tend to be distinguished by the size of the screen, and for more powerful models, by any specialized purpose the machine is intended for, such as a ""gaming laptop"" or a ""mobile workstation"" for professional use.


=== Convertible, hybrid, 2-in-1 ===

The latest trend of technological convergence in the portable computer industry spawned a broad range of devices, which combined features of several previously separate device types. The hybrids, convertibles, and 2-in-1s emerged as crossover devices, which share traits of both tablets and laptops. All such devices have a touchscreen display designed to allow users to work in a tablet mode, using either multi-touch gestures or a stylus/digital pen.
Convertibles are devices with the ability to conceal a hardware keyboard. Keyboards on such devices can be flipped, rotated, or slid behind the back of the chassis, thus transforming from a laptop into a tablet. Hybrids have a keyboard detachment mechanism, and due to this feature, all critical components are situated in the part with the display. 2-in-1s can have a hybrid or a convertible form, often dubbed 2-in-1 detachable and 2-in-1 convertibles respectively, but are distinguished by the ability to run a desktop OS, such as Windows 10. 2-in-1s are often marketed as laptop replacement tablets.
2-in-1s are often very thin, around 10 millimetres (0.39 in), and light devices with a long battery life. 2-in-1s are distinguished from mainstream tablets as they feature an x86-architecture CPU (typically a low- or ultra-low-voltage model), such as the Intel Core i5, run a full-featured desktop OS like Windows 10, and have a number of typical laptop I/O ports, such as USB 3 and Mini DisplayPort.
2-in-1s are designed to be used not only as a media consumption device but also as valid desktop or laptop replacements, due to their ability to run desktop applications, such as Adobe Photoshop. It is possible to connect multiple peripheral devices, such as a mouse, keyboard, and several external displays to a modern 2-in-1.
Microsoft Surface Pro-series devices and Surface Book are examples of modern 2-in-1 detachable, whereas Lenovo Yoga-series computers are a variant of 2-in-1 convertibles. While the older Surface RT and Surface 2 have the same chassis design as the Surface Pro, their use of ARM processors and Windows RT do not classify them as 2-in-1s, but as hybrid tablets. Similarly, a number of hybrid laptops run a mobile operating system, such as Android. These include Asus's Transformer Pad devices, examples of hybrids with a detachable keyboard design, which do not fall in the category of 2-in-1s.


=== Rugged laptop ===

A rugged laptop is designed to reliably operate in harsh usage conditions such as strong vibrations, extreme temperatures, and wet or dusty environments. Rugged laptops are bulkier, heavier, and much more expensive than regular laptops, and thus are seldom seen in regular consumer use.


== Hardware ==

The basic components of laptops function identically to their desktop counterparts. Traditionally they were miniaturized and adapted to mobile use, The design restrictions on power, size, and cooling of laptops limit the maximum performance of laptop parts compared to that of desktop components, although that difference has increasingly narrowed.
In general, laptop components are not intended to be replaceable or upgradable by the end-user, except for components that can be detached; in the past, batteries and optical drives were commonly exchangeable. Some laptops feature socketed processors with sockets such as the Socket G2, but many laptops use processors that are soldered to the motherboard. Many laptops come with RAM and storage that is soldered to the motherboard and cannot be easily replaced. This restriction is one of the major differences between laptops and desktop computers, because the large ""tower"" cases used in desktop computers are designed so that new motherboards, hard disks, sound cards, RAM, and other components can be added. Memory and storage can often be upgraded with some disassembly, but with the most compact laptops, there may be no upgradeable components at all.
The following sections summarizes the differences and distinguishing features of laptop components in comparison to desktop personal computer parts.


=== Display ===
The typical laptop has a screen that, when unfolded, is upright to the user.


==== Screen technology ====
Laptop screens most commonly employ liquid-crystal display (LCD) technology, although use of OLED panels has risen substantially since 2020. The display interfaces with the motherboard using the embedded DisplayPort protocol via the Low-voltage differential signaling (LVDS) 30 or 40 pin connector. The panels are mainly manufactured by AU Optronics, BOE Technology, LG Display or Samsung Display.


==== Surface finish ====
Externally, it can be a glossy or a matte (anti-glare) screen.


==== Sizes ====
In the past, there was a broader range of marketing terms (both formal and informal) to distinguish between different sizes of laptops. These included Netbooks, subnotebooks, Ultra-mobile PC, and Desktop replacement computers; these are sometimes still used informally, although they are essentially dead in terms of manufacturer marketing.
As of 2021, mainstream consumer laptops tend to come with 11"", 13"" or 15""-16"" screens; 14"" models are more popular among business machines. Larger and smaller models are available, but less common – there is no clear dividing line in minimum or maximum size. Machines small enough to be handheld (screens in the 6–8"" range) can be marketed either as very small laptops or ""handheld PCs"", while the distinction between the largest laptops and ""All-in-One"" desktops is whether they fold for travel.


==== Resolution ====
Having a higher resolution display allows more items to fit onscreen at a time, improving the user's ability to multitask, although at the higher resolutions on smaller screens, the resolution may only serve to display sharper graphics and text rather than increasing the usable area. Since the introduction of the MacBook Pro with Retina display in 2012, there has been an increase in the availability of ""HiDPI"" (or high Pixel density) displays; as of 2022, this is generally considered to be anything higher than 1920 pixels wide. This has increasingly converged around 4K (3840-pixel-wide) resolutions.
External displays can be connected to most laptops, with most models supporting at least one. The use of technology such as USB4 (section Alternate Mode partner specifications). DisplayPort Alt Mode has been utilized to charge a laptop and provide display output over one USB-C Cable.


==== Refresh rates ====
Most laptop displays have a maximum refresh rate of 60 Hz. The Dell M17x and Samsung 700G7A, both released in 2011, were among the first laptops to feature a 120 Hz refresh rate, and more such laptops have appeared in the years since.


=== Central processing unit (CPU) ===
A laptop's CPU  has advanced power-saving features and produces less heat than one intended purely for desktop use. Mainstream laptop CPUs made after 2018 have at least two processor cores, often four cores, and sometimes more, with 6 and 8 cores becoming more common.
For the low price and mainstream performance, there is no longer a significant performance difference between laptop and desktop CPUs, but at the high end, the fastest desktop CPUs still substantially outperform the fastest laptop processors, at the expense of massively higher power consumption and heat generation; the fastest laptop processors top out at 56 watts of heat, while the fastest desktop processors top out at 150 watts (and often need water cooling).
There has been a wide range of CPUs designed for laptops available from both Intel, AMD, and other manufacturers. On non-x86 architectures, Motorola and IBM produced the chips for the former PowerPC-based Apple laptops (iBook and PowerBook). Between around 2000 to 2014, most full-size laptops had socketed, replaceable CPUs; on thinner models, the CPU was soldered on the motherboard and was not replaceable or upgradable without replacing the motherboard. Since 2015, Intel has not offered new laptop CPU models with pins to be interchangeable, preferring ball grid array chip packages which have to be soldered; and as of 2021, only a few rare models using desktop parts.
In the past, some laptops have used a desktop processor instead of the laptop version and have had high-performance gains at the cost of greater weight, heat, and limited battery life; this is not unknown as of 2022, but since around 2010, the practice has been restricted to small-volume gaming models. Laptop CPUs are rarely able to be overclocked; most use locked processors. Even on gaming models where unlocked processors are available, the cooling system in most laptops is often very close to its limits and there is rarely headroom for an overclocking–related operating temperature increase.


=== Graphics processing unit (GPU) ===
On most laptops, the GPU is integrated into the CPU to conserve power and space. This was introduced by Intel with the Core i-series of mobile processors in 2010, followed by similar AMD APU processors in January 2011.
Before that, lower-end machines tended to use graphics processors integrated into the system chipset, while higher-end machines had a separate graphics processor. In the past, laptops lacking a separate graphics processor were limited in their utility for gaming and professional applications involving 3D graphics, but the capabilities of CPU-integrated graphics have converged with the low-end of dedicated graphics processors since the mid-2010s. For laptops possessing limited onboard graphics capability but sufficient I/O throughput, an external GPU (eGPU) can provide additional graphics power at the cost of physical space and portability.
Higher-end laptops intended for gaming or professional 3D work still come with dedicated (and in some cases even dual) graphics processors on the motherboard or as an internal expansion card. Since 2011, these almost always involve switchable graphics so that when there is no demand for the higher performance dedicated graphics processor, the more power-efficient integrated graphics processor will be used. Nvidia Optimus and AMD Hybrid Graphics are examples of this sort of system of switchable graphics.
Traditionally, the system RAM on laptops (as well as on desktop computers) was physically separate from the graphics memory used by the GPU. Apple's M series SoCs feature a unified pool of memory for both the system and the GPU; this approach can produce substantial efficiency gains for some applications but comes at the cost of eGPU support.


=== Memory ===
Since around the year 2000, most laptops have used SO-DIMM slots in which RAM is mounted, although, as of 2021, an increasing number of models use memory soldered to the motherboard, either alongside SO-DIMM slots or without any slots and soldering all memory to the motherboard, but a new form factor, the CAMM module, is slated to fix the size and timing limitation. Before 2000, most laptops used proprietary memory modules if their memory was upgradable.
In the early 2010s, high end laptops such as the 2011 Samsung 700G7A have passed the 10 GB RAM barrier, featuring 16 GB of RAM.
When upgradeable, memory slots are sometimes accessible from the bottom of the laptop for ease of upgrading; in other cases, accessing them requires significant disassembly. Most laptops have two memory slots, although some will have only one, either for cost savings or because some amount of memory is soldered. Some high-end models have four slots; these are usually mobile engineering workstations, although a few high-end models intended for gaming do as well.
As of 2021, 8 GB RAM is most common, with lower-end models occasionally having 4 GB. Higher-end laptops may come with 16 GB of RAM or more.


=== Internal storage ===
The earliest laptops most often used floppy disk for storage, although a few used either RAM disk or tape, by the late 1980s hard disk drives had become the standard form of storage.
Between 1990 and 2009, almost all laptops typically had a hard disk drive (HDD) for storage; since then, solid-state drives (SSD) have gradually come to supplant hard drives in all but some inexpensive consumer models. Solid-state drives are faster and more power-efficient, as well as eliminating the hazard of drive and data corruption caused by a laptop's physical impacts, as they use no mechanical parts such as a rotational platter. In many cases, they are more compact as well. Initially, in the late 2000s, SSDs were substantially more expensive than HDDs, but as of 2021 prices on smaller capacity (under 1 terabyte) drives have converged; larger capacity drives remain more expensive than comparable-sized HDDs.
Since around 1990, where a hard drive is present it will typically be a 2.5-inch drive; some very compact laptops support even smaller 1.8-inch HDDs, and a very small number used 1"" Microdrives. Some SSDs are built to match the size/shape of a laptop hard drive, but increasingly they have been replaced with smaller mSATA or M.2 cards. SSDs using the newer and much faster NVM Express standard for connecting are only available as cards.
As of 2022, many laptops no longer contain space for a 2.5"" drive, accepting only M.2 cards; a few of the smallest have storage soldered to the motherboard. For those that can, they can typically contain a single 2.5-inch drive, but a small number of laptops with a screen wider than 15 inches can house two drives.
A variety of external HDDs or NAS data storage servers with support of RAID technology can be attached to virtually any laptop over such interfaces as USB, FireWire, eSATA, or Thunderbolt, or over a wired or wireless network to further increase space for the storage of data. Many laptops also incorporate a SD or microSD card slot. This enables users to download digital pictures from an SD card onto a laptop, thus enabling them to delete the SD card's contents to free up space for taking new pictures.


=== Removable media drive ===
Optical disc drives capable of playing CD-ROMs, compact discs (CD), DVDs, and in some cases, Blu-ray discs (BD), were nearly universal on full-sized models between the mid-1990s and the early 2010s. As of 2021, drives are uncommon in compact or premium laptops; they remain available in some bulkier models, but the trend towards thinner and lighter machines is gradually eliminating these drives and players – when needed they can be connected via USB instead.


=== Speaker ===
Laptops are usually have built-in speakers and built-in microphones. However, integrated speakers may be small and of restricted sound quality to conserve space.


=== Inputs ===

An alphanumeric keyboard is used to enter text, data, and other commands (e.g., function keys). A touchpad (also called a trackpad), a pointing stick, or both, are used to control the position of the cursor on the screen, and an integrated keyboard is used for typing. Some touchpads have buttons separate from the touch surface, while others share the surface. A quick double-tap is typically registered as a click, and operating systems may recognize multi-finger touch gestures.
An external keyboard and mouse may be connected using a USB port or wirelessly, via Bluetooth or similar technology. Some laptops have multitouch touchscreen displays, either available as an option or standard. Most laptops have webcams and microphones, which can be used to communicate with other people with both moving images and sound, via web conferencing or video-calling software.
Laptops typically have USB ports and a combined headphone/microphone jack, for use with headphones, a combined headset, or an external mic. Many laptops have a card reader for reading digital camera SD cards.


=== Input/output (I/O) ports ===
On a typical laptop there are several USB ports; if they use only the older USB connectors instead of USB-C, they will typically have an external monitor port (VGA, DVI, HDMI or Mini DisplayPort or occasionally more than one), an audio in/out port (often in form of a single socket) is common. It is possible to connect up to three external displays to a 2014-era laptop via a single Mini DisplayPort, using multi-stream transport technology.
Apple, in a 2015 version of its MacBook, transitioned from a number of different I/O ports to a single USB-C port. This port can be used both for charging and connecting a variety of devices through the use of aftermarket adapters. Apple has since transitioned back to using a number of different ports. Google, with its updated version of Chromebook Pixel, shows a similar transition trend towards USB-C, although keeping older USB Type-A ports for a better compatibility with older devices. Although being common until the end of the 2000s decade, Ethernet network port are rarely found on modern laptops, due to widespread use of wireless networking, such as Wi-Fi. Legacy ports such as a PS/2 keyboard/mouse port, serial port, parallel port, or FireWire are provided on some models, but they are increasingly rare. On Apple's systems, and on a handful of other laptops, there are also Thunderbolt ports, but Thunderbolt 3 uses USB-C. Laptops typically have a headphone jack, so that the user can connect headphones or amplified speaker systems for listening to music or other audio.


=== Expansion cards ===
In the past, a PC Card (formerly PCMCIA) or ExpressCard slot for expansion was often present on laptops to allow adding and removing functionality, even when the laptop is powered on; these are becoming increasingly rare since the introduction of USB 3.0. Some internal subsystems such as Ethernet, Wi-Fi, or a wireless cellular modem can be implemented as replaceable internal expansion cards, usually accessible under an access cover on the bottom of the laptop. The standard for such cards is PCI Express, which comes in both mini and even smaller M.2 sizes. In newer laptops, it is not uncommon to also see Micro SATA (mSATA) functionality on PCI Express Mini or M.2 card slots allowing the use of those slots for SATA-based solid-state drives.
Mobile PCI Express Module (MXM) is a type of expansion card that is used for graphics cards.


=== Battery and power supply ===

Since the late 1990s, laptops have typically used lithium ion or lithium polymer batteries, These replaced the older nickel metal-hydride typically used in the 1990s, and nickel–cadmium batteries used in most of the earliest laptops. A few of the oldest laptops used non-rechargeable batteries, or lead–acid batteries.
Battery life is highly variable by model and workload and can range from one hour to nearly a day. A battery's performance gradually decreases over time; a noticeable reduction in capacity is typically evident after two to three years of regular use, depending on the charging and discharging pattern and the design of the battery. Innovations in laptops and batteries have seen situations in which the battery can provide up to 24 hours of continued operation, assuming average power consumption levels. An example is the HP EliteBook 6930p when used with its ultra-capacity battery.
Laptops with removable batteries may support larger replacement batteries with extended capacity.
A laptop's battery is charged using an external power supply, which is plugged into a wall outlet. The power supply outputs a DC voltage typically in the range of 7.2—24 volts. The power supply is usually external and connected to the laptop through a DC connector cable. In most cases, it can charge the battery and power the laptop simultaneously. When the battery is fully charged, the laptop continues to run on power supplied by the external power supply, avoiding battery use. If the used power supply is not strong enough to power computing components and charge the battery simultaneously, the battery may charge in a shorter period of time if the laptop is turned off or sleeping. The charger typically adds about 400 grams (0.88 lb) to the overall transporting weight of a laptop, although some models are substantially heavier or lighter. Most 2016-era laptops use a smart battery, a rechargeable battery pack with a built-in battery management system (BMS). The smart battery can internally measure voltage and current, and deduce charge level and State of Health (SoH) parameters, indicating the state of the cells.


=== Power connectors ===

Historically, DC connectors, typically cylindrical/barrel-shaped coaxial power connectors have been used in laptops. Some vendors such as Lenovo made intermittent use of a rectangular connector.
Some connector heads feature a center pin to allow the end device to determine the power supply type by measuring the resistance between it and the connector's negative pole (outer surface). Vendors may block charging if a power supply is not recognized as original part, which could deny the legitimate use of universal third-party chargers.
With the advent of USB-C, portable electronics made increasing use of it for both power delivery and data transfer. Its support for 20 V (common laptop power supply voltage) and 5 A typically suffices for low to mid-end laptops, but some with higher power demands such as gaming laptops depend on dedicated DC connectors to handle currents beyond 5 A without risking overheating, some even above 10 A. Additionally, dedicated DC connectors are more durable and less prone to wear and tear from frequent reconnection, as their design is less delicate.
=== Cooling ===
Waste heat from the operation is difficult to remove in the compact internal space of a laptop. The earliest laptops used passive cooling; this gave way to heat sinks placed directly on the components to be cooled, but when these hot components are deep inside the device, a large space-wasting air duct is needed to exhaust the heat. Modern laptops instead rely on heat pipes to rapidly move waste heat towards the edges of the device, to allow for a much smaller and compact fan and heat sink cooling system. Waste heat is usually exhausted away from the device operator towards the rear or sides of the device. Multiple air intake paths are used since some intakes can be blocked, such as when the device is placed on a soft conforming surface like a chair cushion. Secondary device temperature monitoring may reduce performance or trigger an emergency shutdown if it is unable to dissipate heat, such as if the laptop were to be left running and placed inside a carrying case. Aftermarket cooling pads with external fans can be used with laptops to reduce operating temperatures.


=== Docking station ===

A docking station (sometimes referred to simply as a dock) is a laptop accessory that contains multiple ports and in some cases expansion slots or bays for fixed or removable drives. A laptop connects and disconnects to a docking station, typically through a single large proprietary connector. A docking station is an especially popular laptop accessory in a corporate computing environment, due to a possibility of a docking station transforming a laptop into a full-featured desktop replacement, yet allowing for its easy release. This ability can be advantageous to ""road warrior"" employees who have to travel frequently for work, and yet who also come into the office. If more ports are needed, or their position on a laptop is inconvenient, one can use a cheaper passive device known as a port replicator. These devices mate to the connectors on the laptop, such as through USB or FireWire.


=== Charging trolleys ===
Laptop charging trolleys, also known as laptop trolleys or laptop carts, are mobile storage containers to charge multiple laptops, netbooks, and tablet computers at the same time. The trolleys are used in schools that have replaced their traditional static computer labs suites of desktop equipped with ""tower"" computers, but do not have enough plug sockets in an individual classroom to charge all of the devices. The trolleys can be wheeled between rooms and classrooms so that all students and teachers in a particular building can access fully charged IT equipment.
Laptop charging trolleys are also used to deter and protect against opportunistic and organized theft. Schools, especially those with open plan designs, are often prime targets for thieves who steal high-value items. Laptops, netbooks, and tablets are among the highest–value portable items in a school. Moreover, laptops can easily be concealed under clothing and stolen from buildings. Many types of laptop–charging trolleys are designed and constructed to protect against theft. They are generally made out of steel, and the laptops remain locked up while not in use. Although the trolleys can be moved between areas from one classroom to another, they can often be mounted or locked to the floor, support pillars, or walls to prevent thieves from stealing the laptops, especially overnight.


=== Solar panels ===

In some laptops, solar panels are able to generate enough solar power for the laptop to operate. The One Laptop Per Child Initiative released the OLPC XO-1 laptop which was tested and successfully operated by use of solar panels. Presently, they are designing an OLPC XO-3 laptop with these features. The OLPC XO-3 can operate with 2 watts of electricity because its renewable energy resources generate a total of 4 watts. Samsung has also designed the NC215S solar–powered notebook that will be sold commercially in the U.S. market.


=== Accessories ===
A common accessory for laptops is a laptop sleeve, laptop skin, or laptop case, which provides a degree of protection from scratches. Sleeves, which are distinguished by being relatively thin and flexible, are most commonly made of neoprene, with sturdier ones made of low-resilience polyurethane. Some laptop sleeves are wrapped in ballistic nylon to provide some measure of waterproofing. Bulkier and sturdier cases can be made of metal with polyurethane padding inside and may have locks for added security. Metal, padded cases also offer protection against impacts and drops. Another common accessory is a laptop cooler, a device that helps lower the internal temperature of the laptop either actively or passively. A common active method involves using electric fans to draw heat away from the laptop, while a passive method might involve propping the laptop up on some type of pad so it can receive more airflow. Some stores sell laptop pads that enable a reclining person on a bed to use a laptop.


=== Modularity ===

Some of the components of earlier models of laptops can easily be replaced without opening completely its bottom part, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc.
Some of the components of recent models of laptops reside inside. Replacing most of its components, such as keyboard, battery, hard disk, memory modules, CPU cooling fan, etc., requires removal of its either top or bottom part, removal of the motherboard, and returning them.
In some types, solder and glue are used to mount components such as RAM, storage, and batteries, making repairs additionally difficult.


=== Obsolete features ===

Features that certain early models of laptops used to have that are not available in most current laptops include:

Reset (""cold restart"") button in a hole (needed a thin metal tool to press)
Instant power off button in a hole (needed a thin metal tool to press)
Integrated charger or power adapter inside the laptop
Dedicated Media buttons (Internet, Volume, Play, Pause, Next, Previous)
Floppy disk drive
Serial port
Parallel port
Modem
IEEE 1394 port
Docking port
Shared PS/2 input device port
IrDA
S-video port
S/PDIF audio port
PC Card / PCMCIA slot
ExpressCard slot
CD/DVD Drives (starting with 2013 models)
VGA port (starting with 2013 models)


== Characteristics ==


=== Advantages ===

Portability is usually the first feature mentioned in any comparison of laptops versus desktop PCs. Physical portability allows a laptop to be used in many places—not only at home and the office but also during commuting and flights, in coffee shops, in lecture halls and libraries, at clients' locations or a meeting room, etc. Within a home, portability enables laptop users to move their devices from room to room. Portability offers several distinct advantages:
Productivity: Using a laptop in places where a desktop PC cannot be used can help employees and students to increase their productivity on work or school tasks, such as an office worker reading their work e-mails during an hour-long commute by train, or a student doing their homework at the university coffee shop during a break between lectures, for example.
Up-to-date information: Using a single laptop prevents fragmentation of files across multiple PC's as the files exist in a single location and are always up-to-date.
Connectivity: A key advantage of laptops is that they almost always have integrated connectivity features such as Wi-Fi and Bluetooth, and sometimes connection to cellular networks either through native integration or use of a hotspot.  Wi-Fi networks and laptop programs are especially widespread at university campuses.
Other advantages of laptops:

Size: Laptops are smaller than desktop PCs. This is beneficial when space is at a premium, for example in small apartments and student dorms. When not in use, a laptop can be closed and put away in a desk drawer.
Low power consumption: Laptops are several times more power-efficient than desktops. A typical laptop uses 10–100 W, compared to 200–800W for desktops. This could be particularly beneficial for large businesses, which run hundreds of personal computers thus economies of scale, and homes where there is a computer running 24/7 (such as a home media server, print server, etc.).
Quiet: Laptops are typically much quieter than desktops, due both to the components (often silent solid-state drives replacing hard drives) and to less heat production leading to the use of fewer, sometimes no cooling fans. The latter has given rise to laptops that have no moving parts, resulting in complete silence during use.
Battery: a charged laptop can continue to be used in case of a power outage and is not affected by short power interruptions and blackouts, an issue that is present with desktop PC's.
All-in-One: designed to be portable, most modern laptops have all components integrated into the chassis. For desktops (excluding all-in-ones) this is usually divided into the desktop ""tower"" (the unit with the CPU, hard drive, power supply, etc.), keyboard, mouse, display screen, and optional peripherals such as speakers.


=== Disadvantages ===
Compared to desktop PCs, laptops have disadvantages in the following areas:

Performance
The majority of laptops released in 2022 are capable of common tasks such as web browsing, video playback, and office applications, even at the low end. That said, performance of desktops often surpass comparably priced laptops. The upper limits of performance of laptops remain lower than desktops, due to mostly practical reasons, such as decreased battery life, increased size and heat, etc.
Upgradeability
The upgradeability of laptops is very limited compared to thoroughly standardized desktops, due to technical and economic reasons. In general, hard drives and memory can be upgraded easily. Due to the integrated nature of laptops, however, the motherboard, CPU, and graphics, are seldom officially upgradeable. Some efforts towards industry standard parts and layouts have been attempted, such as Common Building Block, but the industry remains largely proprietary and fragmented. There is no industry-wide standard form factor for laptops; Moreover, starting with 2013 models, laptops have become increasingly integrated (soldered) with the motherboard for most of its components (CPU, SSD, RAM, etc.) to reduce size and upgradeability prospects.
Durability
 Laptops are less durable than desktops/PCs. However, the durability of the laptop depends on the user if proper maintenance is done then the laptop can work longer.Because of their portability, laptops are subject to more wear and physical damage than desktops, additionally hindered by their integrated nature. A liquid spill onto the keyboard, while a minor issue with a desktop system, can damage the internals of a laptop and destroy the computer, result in a costly repair or entire replacement of laptops. One study found that a laptop is three times more likely to break during the first year of use than a desktop. To maintain a laptop, it is recommended to clean it every three months for dirt, debris, dust, and food particles. Most cleaning kits consist of a lint-free or microfiber cloth for the screen and keyboard, compressed air for getting dust out of the cooling fan, and a cleaning solution. Harsh chemicals such as bleach should not be used to clean a laptop, as they can damage it.
Heating and cooling
Laptops rely on extremely compact cooling systems involving a fan and heat sink that can fail from blockage caused by accumulated airborne dust and debris. Most laptops do not have any type of removable dust collection filter over the air intake for these cooling systems, resulting in a system that gradually conducts more heat and noise as the years pass. In some cases, the laptop starts to overheat even at idle load levels. This dust is usually stuck inside where the fan and heat sink meet, where it can not be removed by a casual cleaning and vacuuming. Most of the time, compressed air can dislodge the dust and debris but may not entirely remove it. After the device is turned on, the loose debris is reaccumulated into the cooling system by the fans. Complete disassembly is usually required to clean the laptop entirely. However, preventative maintenance such as regular cleaning of the heat sink via compressed air can prevent dust build-up on the heat sink. Many laptops are difficult to disassemble by the average user and contain components that are sensitive to electrostatic discharge (ESD).
Battery life
Battery life is limited because the capacity drops with time, eventually warranting replacement after as little as 2–3 years. A new battery typically stores enough energy to run the laptop for five to six hours or more, depending on usage and the battery size. The battery is often easily replaceable and a higher capacity model may be obtained for longer charging and discharging time. Some laptops do not have the usual removable battery and have to be brought to the service center of their manufacturer or a third-party laptop service center to have their battery replaced. Replacement batteries can also be expensive, depending on the availability of the parts. Desktop PC's do not face similar problems since they are reliant on long lasting power supplies.
Security and privacy
Because they are valuable, commonly used, portable, and easy to hide in a backpack or other type of bag, laptops are often stolen. Every day, over 1,600 laptops go missing from U.S. airports. The cost of stolen business or personal data, and of the resulting problems (identity theft, credit card fraud, breach of privacy), can be many times the value of the stolen laptop itself. Consequently, the physical protection of laptops and the safeguarding of data contained on them are both of great importance. Some laptops, primarily professional and educational devices, have a Kensington security slot, which can be used to tether them with a security cable and lock. In addition, modern operating systems have features such as Activation Lock or similar that prevents the use of the device without credentials. As of 2015, some laptops also have additional security elements added, including biometric security components such as Windows Hello or Touch ID.Software such as GadgetTrak and  Find My Mac have been engineered to help people locate and recover their stolen laptops in the event of theft. Setting one's laptop with a password on its firmware (protection against going to firmware setup or booting), internal HDD/SSD (protection against accessing it and loading an operating system on it afterward), and every user account of the operating system are additional security measures that a user should do. Fewer than 5% of lost or stolen laptops are recovered by the companies that own them, however, that number may decrease due to a variety of companies and software solutions specializing in laptop recovery. In the 2010s, the common availability of webcams on laptops raised privacy concerns. In Robbins v. Lower Merion School District (Eastern District of Pennsylvania 2010), school-issued laptops loaded with special software enabled staff from two high schools to take secret webcam shots of students at home, via their students' laptops.


==== Ergonomics and health effects ====
Wrists
Prolonged use of laptops can cause repetitive strain injury because of their small, flat keyboard and trackpad pointing devices. Usage of separate, external ergonomic keyboards and pointing devices is recommended to prevent injury when working for long periods of time; they can be connected to a laptop easily by USB, Bluetooth or via a docking station. Some health standards require ergonomic keyboards at workplaces.
Neck and spine
A laptop's integrated screen often requires users to lean over for a better view, which can cause neck or spinal injuries. A larger and higher-quality external screen can be connected to almost any laptop to alleviate this and to provide additional screen space for more productive work. Another solution is to use a computer stand.
Possible effect on fertility
A study by State University of New York researchers found that heat generated from laptops can increase the temperature of the lap of male users when balancing the computer on their lap, potentially putting sperm count at risk. The study, which included roughly two dozen men between the ages of 21 and 35, found that the sitting position required to balance a laptop can increase scrotum temperature by as much as 2.1 °C (4 °F). However, further research is needed to determine whether this directly affects male sterility. A later 2010 study of 29 males published in Fertility and Sterility found that men who kept their laptops on their laps experienced scrotal hyperthermia (overheating) in which their scrotal temperatures increased by up to 2.0 °C (4 °F). The resulting heat increase, which could not be offset by a laptop cushion, may increase male infertility. A common practical solution to this problem is to place the laptop on a table or desk or to use a book or pillow between the body and the laptop. Another solution is to obtain a cooling unit for the laptop. These are usually USB powered and consist of a hard thin plastic case housing one, two, or three cooling fans – with the entire assembly designed to sit under the laptop in question – which results in the laptop remaining cool to the touch, and greatly reduces laptop heat buildup.
Thighs
Heat generated from using a laptop on the lap can also cause skin discoloration on the thighs known as ""toasted skin syndrome"".


== Sales ==


=== Manufacturers ===

There are many laptop brands and manufacturers. Several major brands that offer notebooks in various classes are listed in the adjacent box. The major brands usually offer good service and support, including well-executed documentation and driver downloads that remain available for many years after a particular laptop model is no longer produced. Capitalizing on service, support, and brand image, laptops from major brands are more expensive than laptops by smaller brands and ODMs. Some brands specialize in a particular class of laptops, such as gaming laptops (Alienware), high-performance laptops (HP Envy), netbooks (EeePC) and laptops for children (OLPC).
Many brands, including the major ones, do not design and do not manufacture their laptops. Instead, a small number of Original Design Manufacturers (ODMs) design new models of laptops, and the brands choose the models to be included in their lineup. In 2006, 7 major ODMs manufactured 7 of every 10 laptops in the world, with the largest one (Quanta Computer) having 30% of the world market share. Therefore, identical models are available both from a major label and from a low-profile ODM in-house brand.


=== Adoption by users ===
Battery-powered portable computers had just 2% worldwide market share in 1986. However, laptops have become increasingly popular, both for business and personal use. Around 109 million notebook PCs shipped worldwide in 2007, a growth of 33% compared to 2006. In 2008 it was estimated that 145.9 million notebooks were sold, and that the number would grow in 2009 to 177.7 million. The third quarter of 2008 was the first time when worldwide notebook PC shipments exceeded desktops, with 38.6 million units versus 38.5 million units. Due to the advent of tablets and affordable laptops, many computer users now have laptops due to the convenience offered by the device.


=== Price ===
Before 2008, laptops were very expensive. In May 2005, the average notebook sold for $1,131 while desktops sold for an average of $696. Around 2008, however, prices of laptops decreased substantially due to low-cost netbooks, drawing an average US$689 at U.S. retail stores in August 2008. Starting with the 2010s, laptops have decreased substantially in price at the low end due to inexpensive and low power Arm processors, less demanding operating systems such as ChromeOS, and SoC's. As of 2023, a new laptop can be obtained for $299.


== Disposal ==

The list of materials that go into a laptop computer is long, and many of the substances used, such as beryllium, lead, chromium, and mercury compounds, are toxic or carcinogenic to humans. Although these toxins are relatively harmless when the laptop is in use, concerns that discarded laptops cause a serious health and environmental risks when improperly discarded have arisen. The Waste Electrical and Electronic Equipment Directive (WEEE Directive) in Europe specified that all laptop computers must be recycled by law. Similarly, the U.S. Environmental Protection Agency (EPA) has outlawed landfill dumping or the incinerating of discarded laptop computers.
Most laptop computers begin the recycling process with a method known as Demanufacturing, this involves the physical separation of the components of the laptop. These components are then either grouped into materials (e.g. plastic, metal and glass) for recycling or more complex items that require more advanced materials separation (e.g.) circuit boards, hard drives and batteries.
Corporate laptop recycling can require an additional process known as data destruction. The data destruction process ensures that all information or data that has been stored on a laptop hard drive can never be retrieved again. Below is an overview of some of the data protection and environmental laws and regulations applicable for laptop recycling data destruction:


== Extreme use ==

The ruggedized Grid Compass computer was used since the early days of the Space Shuttle program. The first commercial laptop used in space was a Macintosh portable in 1990 on Space Shuttle mission STS-41 and again in 1991 aboard STS-43. Apple and other laptop computers continue to be flown aboard crewed spaceflights, though the only long-duration flight certified computer for the International Space Station is the ThinkPad. As of 2011, over 100 ThinkPads were aboard the ISS. Laptops used aboard the International Space Station and other spaceflights are generally the same ones that can be purchased by the general public but needed modifications are made to allow them to be used safely and effectively in a weightless environment such as updating the cooling systems to function without relying on hot air rising and accommodation for the lower cabin air pressure. Laptops operating in harsh usage environments and conditions, such as strong vibrations, extreme temperatures, and wet or dusty conditions differ from those used in space in that they are custom designed for the task and do not use commercial off-the-shelf hardware.


== See also ==


== References ==

 Media related to Laptops at Wikimedia Commons"
web development ,No Wikipedia page found for web development 
hrithik roshan,"Hrithik Roshan (pronounced [ɾɪt̪ɪk ɾoʃən]; born 10 January 1974) is an Indian actor who works in Hindi cinema. He has portrayed a variety of characters and is known for his dancing skills. One of the highest-paid actors in India, he has won many awards, including six Filmfare Awards, of which four were for Best Actor. Starting from 2012, he has appeared in Forbes India's Celebrity 100 several times based on his income and popularity.
Roshan has frequently collaborated with his father, Rakesh Roshan. He made brief appearances as a child actor in several films in the 1980s and later worked as an assistant director on four of his father's films. His first leading role was in the box-office success Kaho Naa... Pyaar Hai (2000), for which he received several awards. Performances in the 2000 terrorism drama Fiza and the 2001 ensemble family drama Kabhi Khushi Kabhie Gham... consolidated his reputation but were followed by several poorly received films.
The 2003 science fiction film Koi... Mil Gaya, for which Roshan won two Filmfare Awards, was a turning point in his film career; he later starred as the titular superhero in its sequels: Krrish (2006) and Krrish 3 (2013). He earned praise for his portrayal of a thief in Dhoom 2 (2006), Mughal emperor Akbar in Jodhaa Akbar (2008) and a quadriplegic in Guzaarish (2010). He achieved further commercial success with the comedy-drama Zindagi Na Milegi Dobara (2011), the revenge drama Agneepath (2012), the biopic Super 30 (2019), and action films directed by Siddharth Anand—Bang Bang! (2014), War (2019) and Fighter (2024).
Roshan has also performed on stage and debuted on television with the dance reality show Just Dance (2011). As a judge on the latter, he became the highest-paid film star on Indian television at that time. He is involved with a number of humanitarian causes, endorses several brands and products and has launched his own clothing line. Roshan was married for fourteen years to Sussanne Khan, with whom he has two children.


== Early life and background ==
Roshan was born on 10 January 1974 in Bombay to the Roshan family, prominent in Hindi cinema. He is of Punjabi and Bengali descent on his paternal side. Hrithik's paternal grandmother Ira Roshan was a Bengali. His father, film director Rakesh Roshan, is the son of music director Roshanlal Nagrath; his mother, Pinkie, is the daughter of producer and director J. Om Prakash. His uncle, Rajesh, is a music composer. Roshan has an older sister, Sunaina, and was educated at the Bombay Scottish School. Roshan belongs to a Hindu family, though he considers himself more spiritual than religious.

Roshan felt isolated as a child; he was born with an extra thumb fused to the one on his right hand, which led some of his peers to avoid him. He has stammered since the age of six; this caused him problems at school, and he feigned injury and illness to avoid oral tests. He was helped by daily speech therapy.
Roshan's grandfather, Prakash first brought him on-screen at the age of six in the film Aasha (1980); he danced in a song enacted by Jeetendra, for which Prakash paid him ₹100. Roshan made uncredited appearances in various family film projects, including his father's production Aap Ke Deewane (1980). In Prakash's Aas Paas (1981), he appeared in the song ""Shehar Main Charcha Hai"". The actor's only speaking role during this period came when he was 12; he was seen as Govinda, the title character's adopted son, in Prakash's Bhagwaan Dada (1986). Roshan decided that he wanted to be a full-time actor, but his father insisted that he focus on his studies. In his early 20s, he was diagnosed with scoliosis that would not allow him to dance or perform stunts. Initially devastated, he eventually decided to become an actor anyway. Around a year after the diagnosis, he took a chance by jogging on a beach when he was caught in a downpour. There was no pain, and becoming more confident, he was able to increase his pace with no adverse effects. Roshan sees this day as ""the turning point of [his] life.""
Roshan attended Sydenham College, where he took part in dance and music festivals while studying, graduating in commerce. Roshan assisted his father on four films—Khudgarz (1987), King Uncle (1993), Karan Arjun (1995) and Koyla (1997)—while also sweeping the floor and making tea for the crew. After pack-up, Roshan would enact Shah Rukh Khan's scenes from Koyla and film himself to make a judgement about his performance as an actor. While he assisted his father, he studied acting under Kishore Namit Kapoor.


== Film career ==


=== 2000–2002: Debut, success and setback ===
Roshan was originally scheduled to make his screen debut as a lead actor opposite Preity Zinta in a cancelled film – Shekhar Kapur's Tara Rum Pum Pum. Instead, he starred in his father's romantic drama Kaho Naa... Pyaar Hai (2000) opposite another debutante, Ameesha Patel. Roshan played dual roles: Rohit, an aspiring singer brutally killed after witnessing a murder, and Raj, an NRI who falls in love with Patel's character. To prepare, he trained with the actor Salman Khan to bulk up physically, worked to improve his diction and took lessons in acting, singing, dancing, fencing and riding. With global revenues of ₹800 million (US$9.6 million), Kaho Naa... Pyaar Hai became one of the highest-grossing Indian films of 2000. His performance was acclaimed by critics; Suggu Kanchana on Rediff.com wrote, ""[Roshan] is good. The ease and style with which he dances, emotes, fights, makes one forget this is his debut film ... He seems to be the most promising among the recent lot of star sons we have been subjected to."" For the role, Roshan received Best Male Debut and Best Actor Awards at the annual Filmfare Awards, IIFA Awards, and Zee Cine Awards. He became the first actor to win both Filmfare Best Debut and Best Actor awards the same year. The film established Roshan as a prominent actor in Bollywood. The actor found life hard after his overnight success, particularly the demands on his time.
In his second release, Khalid Mohammed's crime drama Fiza, Roshan played Amaan, an innocent Muslim boy who becomes a terrorist after the 1992–93 Bombay riots. Roshan appeared in the film to expand his horizons as an actor. Co-starring Karisma Kapoor and Jaya Bachchan, Fiza was moderately successful at the box office, and Roshan's performance earned him a second nomination for Best Actor at the Filmfare ceremony. Taran Adarsh of Bollywood Hungama praised him as the production's prime asset, commending his ""body language, his diction, his expressions, [and] his overall persona."" Roshan next appeared in Vidhu Vinod Chopra's action drama Mission Kashmir (2000) alongside Sanjay Dutt, Preity Zinta, and Jackie Shroff. Set in the valley of Kashmir during the Indo-Pakistani conflicts, the film addressed the topics of terrorism and crime, and was a financial success. Roshan was drawn to his complex role of a young man traumatised by the discovery that his adoptive father had been responsible for the death of his entire birth family. In Adarsh's opinion, Roshan ""brightens up the screen with his magnetic presence. His body language, coupled with his expressions, is sure to win him plaudits.""

In 2001, Roshan appeared in two films, the first of which was Subhash Ghai's Yaadein, a romantic drama which paired him with Kareena Kapoor and reunited him with Shroff. Although highly anticipated, Yaadein was reviled by critics; in The Hindu, Ziya Us Salam criticised the director for relying on Roshan's commercial appeal. Roshan next had a supporting role in Karan Johar's ensemble melodrama Kabhi Khushi Kabhie Gham... alongside Amitabh Bachchan, Jaya Bachchan, Shah Rukh Khan, Kajol and Kareena Kapoor. He was cast as Rohan Raichand—the younger son of Bachchan's character who plots to reunite him with his adopted son (played by Khan)—after Johar had watched a rough cut of Kaho Naa... Pyaar Hai. Kabhi Khushi Kabhie Gham... finished as India's highest-grossing film of the year, and among the most successful Bollywood films in the overseas market, earning  ₹1.36 billion (US$16 million) worldwide. Writing for Rediff.com, Anjum N described Roshan as ""the surprise scene-stealer"", praising him for holding his own against the established actors. Roshan received a nomination for the Filmfare Award for Best Supporting Actor for his performance.
In 2002 Vikram Bhatt's romance Aap Mujhe Achche Lagne Lage reunited him with Ameesha Patel but failed at the box office, as did Arjun Sablok's romance Na Tum Jaano Na Hum (2002), in which he co-starred with Saif Ali Khan and Esha Deol. Roshan's final role that year was in a Yash Raj Films production, the high-profile Mujhse Dosti Karoge! co-starring Rani Mukerji and Kareena Kapoor. The romantic comedy was heavily promoted before its release and made money internationally, though not in India. In another commercial failure, Sooraj R. Barjatya's Main Prem Ki Diwani Hoon, Roshan was cast alongside Kareena Kapoor for the fourth time, and Abhishek Bachchan. The press labelled Roshan a ""one-trick pony"" and suggested that the failure of these films would end his career.


=== 2003–2008: Revival and awards success ===

Roshan's career began to revive with a starring role in Koi... Mil Gaya (2003). The film, directed and produced by his father, centers on his character Rohit Mehra, a developmentally disabled young man, who comes in contact with an extraterrestrial being—a role that required him to lose nearly 8 kilograms (18 lb). Roshan recalls the experience of starring in the film fondly: ""I could live my childhood [again]. I could eat as many chocolates as I wanted. I became a baby and everybody was so caring towards me."" In the book Film Sequels, Carolyn Jess-Cooke drew similarities between the character and Forrest Gump, portrayed by Tom Hanks in the titular film, but this idea was dismissed by Roshan. Film critics were polarised on their view of the film—some of them negatively compared its storyline to the 1982 Hollywood release E.T. the Extra-Terrestrial—but were unanimous in their praise for Roshan. In a 2010 retrospective of the Top 80 Iconic Performances of Bollywood, Filmfare noted ""how flesh and blood Hrithik's act is. Simply because he believes he is the part. Watch him laugh, cry or bond with his remote controlled alien friend and note his nuanced turn."" A Rediff.com critic agreed that Roshan was ""the turbojet that propels the film to the realm of the extraordinary."" Koi... Mil Gaya was one of the most popular Bollywood films of the year, earning ₹823.3 million (US$9.9 million) worldwide and Roshan won both Filmfare Awards for Best Actor and Best Actor (Critics).

The following year, Roshan collaborated with Amitabh Bachchan and Preity Zinta on Farhan Akhtar's Lakshya (2004), a fictionalised coming-of-age story set against events from the 1999 Kargil War. He also featured in the item number ""Main Aisa Kyun Hoon"" (choreographed by Prabhu Deva) which proved popular with audiences. Roshan found it ""one of the most challenging films"" of his career at the time and said it made him respect soldiers. Although trade journalists expected the film to do well commercially, it failed to attract a wide audience. Over the years, it has attained a cult status in India. For the film, Roshan earned Best Actor nominations at the Filmfare and Zee Cine ceremony. Manish Gajjar of the BBC praised Roshan's versatility and his transformation from a carefree youth to a determined and courageous soldier. Reviewing the film in 2016, Tatsam Mukherjee of India Today described his performance as career-best, highlighting his scene before the climax.
Roshan was not seen on screen again until 2006, with three new releases, including a cameo at the end of the year in the romance I See You. He co-starred with Naseeruddin Shah and Priyanka Chopra in his father's superhero production Krrish. A follow-up to his family's production Koi... Mil Gaya, it saw him play dual roles—the title superhero and his character from the original film. Before production, Roshan travelled to China to train with Tony Ching for the cable work that would be needed to make his character fly. Among the several injuries he sustained during production, Roshan tore the hamstring in his right leg and broke his thumb and toe. Krrish became the third-highest-grossing Bollywood film of 2006 with a worldwide revenue of ₹1.26 billion (US$15 million). It garnered him Best Actor awards at the 2007 Screen and the International Indian Film Academy Awards. Ronnie Scheib of Variety considered Roshan a prime asset of the film, noting that he ""pulls off the pic's wilder absurdities with considerable panache.""
For his role as an enigmatic master thief in Dhoom 2 (2006)—an action sequel co-starring Aishwarya Rai, Bipasha Basu and Abhishek Bachchan—Roshan won his third Filmfare Award for Best Actor. The film critic Rajeev Masand called him ""the heart, the soul, and the spirit of the film"", and praised his stunts, concluding that he ""holds the film together and even manages to take your attention away from its many flaws"". Bored by playing the ""good guy"", Roshan was excited to play an anti-hero who lacks heroic attributes, for the first time. At the request of the film's producer Aditya Chopra, Roshan lost 12 pounds (5.4 kg) for the role; he also learnt skateboarding, snow boarding, rollerblading and sand surfing. With earnings of ₹1.5 billion (US$18 million), Dhoom 2 became the highest grossing Indian film at that time, a distinction that was held for two years. In the 2007 melodrama Om Shanti Om, he made a cameo alongside several Bollywood stars.
In 2008, Roshan was cast in Ashutosh Gowariker's Jodhaa Akbar, a partly fictionalised account of a marriage of convenience between the Mughal emperor Akbar (played by Roshan) and the Rajput princess Jodha Bai (played by Rai). Gowariker believed Roshan possessed the regal bearing and physique required to play the role of a king. For the role, Roshan learned sword-fighting and horse-riding, and also took Urdu lessons. Jodhaa Akbar earned  ₹1.2 billion (US$14 million) worldwide. Roshan's performance earned him his fourth Filmfare Best Actor Award. Critics were generally appreciative of Roshan's performance. Raja Sen of Rediff.com thought that Roshan ""proves a very good Akbar. There are times when his inflection seems too modern, but the actor gives the performance his all, slipping into the skin of the character and staying there."" Roshan ended 2008 with an appearance in the popular item number ""Krazzy 4"" from the film of same name.


=== 2009–2012: Critical acclaim ===
Following a small role in Zoya Akhtar's Luck by Chance in 2009, Roshan starred in and recorded ""Kites in the Sky"" for the multi-national romantic thriller Kites (2010). In the film, produced by his father, he played a man running a green card scam in Las Vegas in which he has married 11 different women in exchange for money. Kites opened on a record-breaking 3000 screens, and became the first Bollywood film to break into the North American top 10. However, the film eventually underperformed at India's box office and received negative reviews from critics. The website Box Office India attributed this failure to its multilingual dialogues. In a review for Rediff.com, Matthew Schneeberger thought that Roshan ""overacts. A lot. In Kites, he nails a few scenes, but bungles many more, particularly the film's catastrophically bad ending.""

Roshan then collaborated with director Sanjay Leela Bhansali on the drama Guzaarish (2010) in which he had the role of Ethan Mascarenhas, a former magician suffering from quadriplegia, who after years of struggle, files an appeal for euthanasia. Roshan had reservations about the role but agreed to the project after reading the film's story. To understand his role better, he interacted with paraplegic patients. In his own words, ""I used to spend six hours with the patients, initially once a week and then once a month. I used to go to understand what they go through, what they think, what their needs are. They have taught me a lot of things."" He also trained with a Ukrainian magician to perform the film's magic stunts, and put on weight to look the part. The film failed at the box office, though it and Roshan's performance were positively received by critics. A writer for Zee News praised the chemistry between Roshan and Rai, adding that they ""break the Bollywood mould of stereotypes."" Roshan received the Zee Cine Award for Best Actor (Critics) and nominations for Filmfare, IIFA and Zee Cine Award for Best Actor.
In 2011, Roshan appeared in Zoya Akhtar's ensemble comedy-drama Zindagi Na Milegi Dobara alongside Abhay Deol and Farhan Akhtar as three friends who embark on a bachelor trip where they overcome their insecurities. Zoya cast Roshan in the role of an uptight workaholic as she considers him her favourite actor. For the film's soundtrack, Roshan recorded the song ""Señorita"" with his co-stars and María del Mar Fernández. Zindagi Na Milegi Dobara was released to positive reviews and Roshan's performance was praised. Rajeev Masand wrote, ""Hrithik Roshan once again brings real depth to his character with a spectacular performance. He's shy and restrained, then lets go with such fantastic intensity that you make the inward journey with his character."" The film grossed ₹1.53 billion (US$18 million) worldwide and became Roshan's first commercial success in three years. Later that year, he made a special appearance in Farhan's Don 2.
Roshan's only screen appearance in 2012 was in Karan Malhotra's Agneepath, a retelling of the 1990 film of the same name. Cast alongside Rishi Kapoor, Sanjay Dutt and Priyanka Chopra, Roshan reinterpreted the character Vijay Deenanath Chauhan (originally played by Amitabh Bachchan), a common man who seeks revenge against an unscrupulous man for framing and murdering his father. Roshan was initially sceptical of taking up a role earlier played by Bachchan, and thought hard before accepting. He did not watch the original film for inspiration as he found his role to be completely different. In one of several accidents to happen during production, Roshan suffered a painful back injury. He deemed Agneepath ""the hardest [project] I've ever worked in my life"" owing to the exhaustion he felt while filming. The film broke Bollywood's highest opening-day earnings record, and had a worldwide gross of ₹1.93 billion (US$23 million). A Firstpost reviewer thought Roshan ""breathes fire and soul into Agneepath"". The actor received a third consecutive Stardust Award for Best Actor in a Drama, having won previously for Guzaarish and Zindagi Na Milegi Dobara.


=== 2013–present: Commercial success with limited work ===
Roshan appeared in the third instalment of the Krrish film series—Krrish 3 (2013) which also starred Priyanka Chopra, Vivek Oberoi and Kangana Ranaut. During production, Roshan was injured when he fell down, which resulted in back pain. Critics thought that the film was entertaining but lacking in originality, though Roshan's performance garnered praise. The editor Komal Nahta lauded Roshan for playing three different characters in the film. Krrish 3 grossed ₹3.93 billion (US$47 million) worldwide, becoming one of the highest-grossing Indian films of all time. Roshan received a fourth and fifth consecutive Filmfare nomination for his performances in Krrish 3, and the 2014 action comedy Bang Bang!, a remake of the 2010 Hollywood release Knight and Day and one of the most expensive Bollywood films. Playing the role of an eccentric secret agent who plots to track down a terrorist, Roshan became the first actor to perform a flyboarding stunt in film. While filming in Thailand, Roshan suffered a head injury from a stunt accident and underwent brain surgery at the Hinduja Hospital performed by Dr. B. K. Misra to relieve subacute-subdural hematoma. Writing for Bollywood news website Koimoi, critic Mohar Basu noted that Roshan was ""pitch perfect"" and ""breez[ed] through his part brilliantly."" The film earned ₹3.4 billion (US$41 million) in global ticket sales, making it among the highest-grossing Indian films.

For playing the role of a farmer in 2016 BC who travels to Mohenjo-daro in Ashutosh Gowariker's Mohenjo Daro (2016), Roshan was paid ₹500 million (US$6.0 million), a record-breaking remuneration for an Indian actor. He underwent a three-month training to achieve the ""lithe"" and ""agile"" physique required for his role. Despite being a highly anticipated release, it failed commercially, and critics were generally unenthusiastic. Dismissing the film as an ""unintentional comedy"", Anupama Chopra wrote that Roshan ""pours his soul into every scene. But the burden of carrying this leaden, cartoon-like narrative proves too much even for his Herculean shoulders."" Roshan was next seen alongside Yami Gautam in Sanjay Gupta's Kaabil (2017), a romantic thriller about a blind man who avenges the rape of his blind wife. To ensure authenticity in his portrayal, Roshan locked himself in a room for four days and avoided contact with people. Reviews for the film were generally positive with particular praise for Roshan's performance. Meena Iyer of The Times of India found his performance to be his best to date, and Shubhra Gupta on The Indian Express considered him ""the only bright spot in this dispirited mess of a movie."" The film accumulated ₹1.96 billion (US$23 million) worldwide.
After two years of screen absence, Roshan starred in two films in 2019, first in Vikas Bahl's biographical film Super 30, based on the mathematician Anand Kumar and his eponymous educational program. For the role, Roshan hired a trainer from Bhagalpur to learn Bihari accent. The film was released to mixed reviews but was a commercial success, grossing ₹2 billion (US$24 million) worldwide. While NDTV's Saibal Chatterjee found Roshan miscast in his role, Michael Gomes of Khaleej Times called it one of his best performances. Roshan found his biggest commercial success in the highest-grossing Bollywood film of 2019, the ₹4.75 billion (US$57 million)-earning action thriller War. The film, Roshan's first with Yash Raj Films since Dhoom 2, tells the story of an Indian soldier (Tiger Shroff) tasked with eliminating his former mentor (Roshan) who has gone rogue. Reviews for the film and the performances were positive; Rajeev Masand praised Roshan and Shroff for their commitment to the action, ""bringing swag to the big stylish sequences and a visceral energy to the one-on-one punch-ups in the movie"".
Roshan's next release was three years later in Vikram Vedha (2022), a remake of the Tamil film of the same name. The film tells the story of Vikram, a police inspector (Saif Ali Khan) who sets out to track down and kill Vedha (Roshan), a gangster. It received positive reviews from critics. Rachana Dubey of The Times of India praised Roshan's performance, writing that he ""is menacing, ruthless and extremely emotional in parts"". The film did not perform well commercially, leading Roshan to question the kind of roles he would do in the future. Roshan starred in Siddharth Anand's action film Fighter (2024), with Deepika Padukone and Anil Kapoor. For their roles as Indian Air Force officers, Roshan and Padukone underwent martial arts training. Ganesh Aaglave of Firstpost praised the emotional depth of Roshan's performance and his dialogue delivery. It emerged as a modest commercial success. He will next lead the sequel War 2 set in the YRF Spy Universe.


== Other work ==
Roshan has performed on stage, appeared on television, and launched a clothing line. His first tour (Heartthrobs: Live in Concert (2002) with Kareena Kapoor, Karisma Kapoor, Arjun Rampal and Aftab Shivdasani) was successful in the United States and Canada. At the end of that year, he danced on stage with Amitabh Bachchan, Sanjay Dutt, Kareena Kapoor, Rani Mukerji and Shah Rukh Khan at Kings Park Stadium in Durban, South Africa in the show Now or Never.  In 2011, Roshan served as a judge alongside Farah Khan and Vaibhavi Merchant for the dance competition reality show, Just Dance. He became the highest-paid film star on Indian television after he was paid ₹20 million (US$240,000) per episode. The show ran from June to October 2011. In November 2013, Roshan launched his clothing line, the casual wear brand HRx.

Roshan is vocal about his childhood stammer. He actively supports the Dilkhush Special School for mentally challenged children in Mumbai. In 2008, he donated ₹2 million (US$24,000) to the Nanavati Hospital for the treatment of stammering children. Roshan set up a charity foundation in 2009 that aims to work for handicapped people. He donates roughly ₹700,000 (US$8,400) for charity every month, and believes that people should publicise their philanthropic work to set an example for others. In 2013, he took part in a festivity at Ghatkopar, whose proceeds went to an NGO supporting tribal girls suffering from malnutrition and starvation. Also that year, he donated ₹2.5 million (US$30,000) to help the victims of the 2013 North India floods.
Alongside other Bollywood stars, Roshan played a football match for charity organised by Aamir Khan's daughter, Ira, in 2014. The following year, he appeared with Sonam Kapoor in the music video for ""Dheere Dheere"", whose profits were donated to charity. Later that year, Roshan became the Indian brand ambassador for UNICEF and the Global Goals campaign's World's Largest Lesson that aims to educate children in over 100 countries about the Sustainable Development Goals. In 2016, Roshan and other Bollywood actors made donations for building homes for families affected by the 2015 South Indian floods.
Following his debut film, Roshan signed on for endorsement deals with Coca-Cola, Tamarind and Hero Honda, all for three years and for at least ₹30 million (US$360,000). As of 2010, he is celebrity endorser for such brands and products as Provogue, Parle Hide and Seek, Reliance Communications and Hero Honda and recently roshan has completed six years with Rado. The Times of India reported that Roshan received ₹12 million (US$140,000) to ₹15 million (US$180,000) for each endorsement, making him one of the highest-paid male celebrity endorsers. In 2016, Duff & Phelps estimated his brand value to be US$34.1 million, the eighth highest of Indian celebrities. In 2017, Roshan was signed as the brand ambassador of a Health and wellness startup Cure.fit and is touted as one of the largest endorsement deal signed by an Indian startup.


== Personal life ==

On 20 December 2000, Roshan married Sussanne Khan in a private ceremony in Bangalore. Despite their religious difference—Roshan is a Hindu and Khan is a Muslim—Roshan says that he equally valued her beliefs. The couple has two sons, Hrehaan (born in 2006) and Hridhaan (born in 2008). Roshan and Sussanne separated in December 2013 and their divorce was finalised in November 2014. Both maintained that they parted amicably. 
In 2016 Roshan had filed a lawsuit against Krrish 3 co-star Kangana Ranaut for cyber stalking and harassment. Denying the charges, Ranaut filed a counter-charge claiming that his lawsuit was an attempt to cover up an affair. Owing to a lack of evidence, the Mumbai Police closed the case later that year. As of 2023, Roshan is dating actress Saba Azad.
Roshan had considered quitting the film industry after two assailants fired bullets at his father in 2000. Later that December, he was involved in a controversy when Nepalese newspapers and Jamim Shah's Channel Nepal accused him of stating in a Star Plus interview that he hated Nepal and its people. This led to protests in the country, a ban on screening of his films, and four people's deaths after street violence. Nepalese people threatened to ""bury [him] alive"" if he ever visited the country. Star Plus, for its part, stated that Roshan ""did not touch upon Nepal."" The violence calmed down after Roshan wrote a two-page rejoinder in which he denied having made any claim against the country. Nepali actress Manisha Koirala helped distribute it to newspapers and a local television station.


== Artistry and media image ==
As the son of the filmmaker Rakesh, Roshan faced the media spotlight from a young age. Discussing nepotism in Bollywood, Shama Rana views him as one of several actors who managed film careers with the help of family relations in the industry. On the other hand, Roshan is acknowledged in the media for his devotion to his work and for his ability to commit heavily to each role. He insists on learning any necessary skills and performing stunts himself, and is particularly known for his professionalism. The director Ashutosh Gowariker praised Roshan when he continued filming Mohenjo Daro despite several injuries and being in a troubled state of mind. Zoya Akhtar, who considers Roshan her favourite actor, and directed him in Zindagi Na Milegi Dobara, remarks on his ability to display a range of emotions on screen.

In an attempt to avoid typecasting, Roshan takes on diverse parts. He looks at the scripts as a platform to inspire with the strength and courage of his characters and to make his audiences smile.  Roshan was noted by critics for his versatility in portraying a variety of characters in Koi... Mil Gaya (2003), Lakshya (2004), Jodhaa Akbar (2008), and Guzaarish (2010). Box Office India ranked him first on its top actors listing in 2000 and later included him in 2003, 2004, 2006 and 2007. Roshan topped Rediff.com's list of best Bollywood actors in 2003, and was ranked fourth in 2006. Filmfare magazine included two of his performances—from Koi... Mil Gaya and Lakshya—on its 2010 list of 80 Iconic Performances. In March 2011, Roshan placed fourth on Rediff.com's list of Top 10 Actors of 2000–2010.
Roshan's dancing ability has drawn praise from the media, an opinion he disagrees with. The Los Angeles Times finds him to be ""a sensational dancer"" who ""has the dashing, chiseled looks of a silent movie matinee idol."" Although author Nandana Bose attributed Roshan's popularity to Bollywood's preoccupation with traditional hypermasculinity, she wrote that he sets himself apart by showcasing dancing talent and ""transnational, transfigurative corporeality"". According to Bose, Roshan's stardom is marked by his ability to seamlessly transition between roles, from dancing star to credible superhero, shaping the industry's landscape. Some critics believe that he is only able to dance and act in his father's films. Sanya Panwar of Hindustan Times criticised his inclination towards ""glamorous, albeit empty"" and stereotypical parts.
Roshan has established himself as a sex symbol and a style icon in India. In 2006, Roshan was one of the four Bollywood actors, along with Priyanka Chopra, Kajol and Shah Rukh Khan, whose miniature dolls were launched in the United Kingdom, under the name of ""Bollywood Legends"". He topped The Times of India's listing of 50 Most Desirable Men in 2010 and ranked among the top five for the next five years. In 2010 and 2012, the Indian edition of GQ included him in their listing of Bollywood's best dressed men. A life-size, wax figure of him was installed at London's Madame Tussauds museum in January 2011, making him the fifth Indian actor to have been replicated as a wax statue there. Versions of the statue were installed at Madame Tussauds' museums in New York, Washington and other cities in the world. Roshan regularly features in the magazine Eastern Eye's listing of the 50 Sexiest Asian Men. He topped the list in 2011, 2012 and 2014, and featured among the top five in 2010, 2013 and 2015 to 2018.
Roshan is among Bollywood's highest-paid actors. Discussing his success ratio at the box office in a 2014 article, Daily News and Analysis credited him as ""the most bankable star"" in Bollywood. He was named the second most powerful Indian film star by Forbes in 2001. He ranked fourth in Filmfare Power List in 2007. In a 2009 poll conducted by Daily News and Analysis Roshan was voted one of India's most popular icons. At the 2009 FICCI-IIFA Awards, Roshan was one of the ten recipients of the most powerful Bollywood entertainers of the 2000s. From 2012 to 2019, Roshan was placed on Forbes India's Celebrity 100—a list based on the income and popularity of Indian celebrities—peaking at ninth position in 2014 with an annual income of ₹850 million (US$10 million).


== Awards and nominations ==


== See also ==
List of awards and nominations received by Hrithik Roshan


== References ==


== Literary sources ==
Bharat, Meenakshi; Kumar, Nirmal (2012). Filming the Line of Control: The Indo–Pak Relationship through the Cinematic Lens. Routledge. ISBN 978-1-136-51605-4.
Bose, Nandana (2020). ""From Superman to Shahenshah: Stardom and the Transnational Corporeality of Hrithik Roshan"". In Sen, Meheli; Basu, Anustup (eds.). Figurations in Indian Film. Palgrave Macmillan. pp. 158–169. ISBN 978-1-1373-4978-1.
Dawar, Ramesh (2006). Bollywood: Yesterday, Today, Tomorrow. Star Publications. ISBN 978-1-905863-01-3.
Jess-Cooke, Carolyn (2009). Film Sequels: Theory and Practice from Hollywood to Bollywood: Theory and Practice from Hollywood to Bollywood. Edinburgh University Press. ISBN 978-0-7486-3133-9.
Krämer, Lucia (2016). Bollywood in Britain: Cinema, Brand, Discursive Complex. Bloomsbury Publishing. ISBN 978-1-5013-0758-4.
Outlook. Hathway Investments Pvt Limited. 2005.


== External links ==
Hrithik Roshan at IMDb
Hrithik Roshan at Bollywood Hungama
Hrithik Roshan at Rotten Tomatoes
Hrithik Roshan on Instagram"
Albert Einstein,"Albert Einstein ( EYEN-styne; German: [ˈalbɛɐt ˈʔaɪnʃtaɪn] ; 14 March 1879 – 18 April 1955) was a German-born theoretical physicist who is widely held as one of the most influential scientists. Best known for developing the theory of relativity, Einstein also made important contributions to quantum mechanics. His mass–energy equivalence formula E = mc2, which arises from special relativity, has been called ""the world's most famous equation"". He received the 1921 Nobel Prize in Physics ""for his services to theoretical physics, and especially for his discovery of the law of the photoelectric effect"", a pivotal step in the development of quantum theory.
Born in the German Empire, Einstein moved to Switzerland in 1895, forsaking his German citizenship (as a subject of the Kingdom of Württemberg) the following year. In 1897, at the age of seventeen, he enrolled in the mathematics and physics teaching diploma program at the Swiss federal polytechnic school in Zürich, graduating in 1900. In 1901, he acquired Swiss citizenship, which he kept for the rest of his life. In 1903, he secured a permanent position at the Swiss Patent Office in Bern. In 1905, he submitted a successful PhD dissertation to the University of Zurich. In 1914, he moved to Berlin in order to join the Prussian Academy of Sciences and the Humboldt University of Berlin. In 1917, he became director of the Kaiser Wilhelm Institute for Physics; he also became a German citizen again, this time as a subject of the Kingdom of Prussia. In 1933, while Einstein was visiting the United States, Adolf Hitler came to power in Germany. Horrified by the Nazi war of extermination against his fellow Jews, Einstein decided to remain in the US, and was granted American citizenship in 1940. On the eve of World War II, he endorsed a letter to President Franklin D. Roosevelt alerting him to the potential German nuclear weapons program and recommended that the US begin similar research. Einstein supported the Allies but generally viewed the idea of nuclear weapons with great dismay.
Einstein's work is also known for its influence on the philosophy of science. In 1905, he published four groundbreaking papers, sometimes described as his annus mirabilis (miracle year). These papers outlined a theory of the photoelectric effect, explained Brownian motion, introduced his special theory of relativity—a theory which addressed the inability of classical mechanics to account satisfactorily for the behavior of the electromagnetic field—and demonstrated that if the special theory is correct, mass and energy are equivalent to each other. In 1915, he proposed a general theory of relativity that extended his system of mechanics to incorporate gravitation. A cosmological paper that he published the following year laid out the implications of general relativity for the modeling of the structure and evolution of the universe as a whole.
In the middle part of his career, Einstein made important contributions to statistical mechanics and quantum theory. Especially notable was his work on the quantum physics of radiation, in which light consists of particles, subsequently called photons. With the Indian physicist Satyendra Nath Bose, he laid the groundwork for Bose-Einstein statistics. For much of the last phase of his academic life, Einstein worked on two endeavors that proved ultimately unsuccessful. First, he advocated against quantum theory's introduction of fundamental randomness into science's picture of the world, objecting that ""God does not play dice"". Second, he attempted to devise a unified field theory by generalizing his geometric theory of gravitation to include electromagnetism too. As a result, he became increasingly isolated from the mainstream modern physics. His intellectual achievements and originality made Einstein broadly synonymous with genius. In 1999, he was named Time's Person of the Century. In a 1999 poll of 130 leading physicists worldwide by the British journal Physics World, Einstein was ranked the greatest physicist of all time.


== Life and career ==


=== Childhood, youth and education ===

Albert Einstein was born in Ulm, in the Kingdom of Württemberg in the German Empire, on 14 March 1879. His parents, secular Ashkenazi Jews, were Hermann Einstein, a salesman and engineer, and Pauline Koch. In 1880, the family moved to Munich's borough of Ludwigsvorstadt-Isarvorstadt, where Einstein's father and his uncle Jakob founded Elektrotechnische Fabrik J. Einstein & Cie, a company that manufactured electrical equipment based on direct current. He often related a formative event from his youth, when he was sick in bed and his father brought him a compass. This sparked his lifelong fascination with electromagnetism. He realized that ""Something deeply hidden had to be behind things.""
Albert attended St. Peter's Catholic elementary school  in Munich from the age of five. When he was eight, he was transferred to the Luitpold Gymnasium, where he received advanced primary and then secondary school education.
In 1894, Hermann and Jakob's company tendered for a contract to install electric lighting in Munich, but without success—they lacked the capital that would have been required to update their technology from direct current to the more efficient, alternating current alternative. The failure of their bid forced them to sell their Munich factory and search for new opportunities elsewhere. The Einstein family moved to Italy, first to Milan and a few months later to Pavia, where they settled in Palazzo Cornazzani. Einstein, then fifteen, stayed behind in Munich in order to finish his schooling. His father wanted him to study electrical engineering, but he was a fractious pupil who found the Gymnasium's regimen and teaching methods far from congenial. He later wrote that the school's policy of strict rote learning was harmful to creativity. At the end of December 1894, a letter from a doctor persuaded the Luitpold's authorities to release him from its care, and he joined his family in Pavia. While in Italy as a teenager, he wrote an essay entitled ""On the Investigation of the State of the Ether in a Magnetic Field"".
Einstein excelled at physics and mathematics from an early age, and soon acquired the mathematical expertise normally only found in a child several years his senior. He began teaching himself algebra, calculus and Euclidean geometry when he was twelve; he made such rapid progress that he discovered an original proof of the Pythagorean theorem before his thirteenth birthday. A family tutor, Max Talmud, said that only a short time after he had given the twelve year old Einstein a geometry textbook, the boy ""had worked through the whole book. He thereupon devoted himself to higher mathematics ... Soon the flight of his mathematical genius was so high I could not follow."" Einstein recorded that he had ""mastered integral and differential calculus"" while still just fourteen. His love of algebra and geometry was so great that at twelve, he was already confident that nature could be understood as a ""mathematical structure"".

At thirteen, when his range of enthusiasms had broadened to include music and philosophy, Talmud introduced Einstein to Kant's Critique of Pure Reason. Kant became his favorite philosopher; according to Talmud, ""At the time he was still a child, only thirteen years old, yet Kant's works, incomprehensible to ordinary mortals, seemed to be clear to him.""

In 1895, at the age of sixteen, Einstein sat the entrance examination for the federal polytechnic school (later the Eidgenössische Technische Hochschule, ETH) in Zürich, Switzerland. He failed to reach the required standard in the general part of the test, but performed with distinction in physics and mathematics. On the advice of the polytechnic's principal, he completed his secondary education at the Argovian cantonal school (a gymnasium) in Aarau, Switzerland, graduating in 1896. While lodging in Aarau with the family of Jost Winteler, he fell in love with Winteler's daughter, Marie. (His sister, Maja, later married Winteler's son Paul.)
In January 1896, with his father's approval, Einstein renounced his citizenship of the German Kingdom of Württemberg in order to avoid conscription into military service. The Matura (graduation for the successful completion of higher secondary schooling), awarded to him in September 1896, acknowledged him to have performed well across most of the curriculum, allotting him a top grade of 6 for history, physics, algebra, geometry, and descriptive geometry. At seventeen, he enrolled in the four-year mathematics and physics teaching diploma program at the federal polytechnic school. Marie Winteler, a year older than him, took up a teaching post in Olsberg, Switzerland.
The five other polytechnic school freshmen following the same course as Einstein included just one woman, a twenty year old Serbian, Mileva Marić. Over the next few years, the pair spent many hours discussing their shared interests and learning about topics in physics that the polytechnic school's lectures did not cover. In his letters to Marić, Einstein confessed that exploring science with her by his side was much more enjoyable than reading a textbook in solitude. Eventually the two students became not only friends but also lovers.
Historians of physics are divided on the question of the extent to which Marić contributed to the insights of Einstein's annus mirabilis publications. There is at least some evidence that he was influenced by her scientific ideas, but there are scholars who doubt whether her impact on his thought was of any great significance at all.


=== Marriages, relationships and children ===

Correspondence between Einstein and Marić, discovered and published in 1987, revealed that in early 1902, while Marić was visiting her parents in Novi Sad, she gave birth to a daughter, Lieserl. When Marić returned to Switzerland it was without the child, whose fate is uncertain. A letter of Einstein's that he wrote in September 1903 suggests that the girl was either given up for adoption or died of scarlet fever in infancy.
Einstein and Marić married in January 1903. In May 1904, their son Hans Albert was born in Bern, Switzerland. Their son Eduard was born in Zürich in July 1910. In letters that Einstein wrote to Marie Winteler in the months before Eduard's arrival, he described his love for his wife as ""misguided"" and mourned the ""missed life"" that he imagined he would have enjoyed if he had married Winteler instead: ""I think of you in heartfelt love every spare minute and am so unhappy as only a man can be.""
In 1912, Einstein entered into a relationship with Elsa Löwenthal, who was both his first cousin on his mother's side and his second cousin on his father's. When Marić learned of his infidelity soon after moving to Berlin with him in April 1914, she returned to Zürich, taking Hans Albert and Eduard with her. Einstein and Marić were granted a divorce on 14 February 1919 on the grounds of having lived apart for five years. As part of the divorce settlement, Einstein agreed that if he were to win a Nobel Prize, he would give the money that he received to Marić; he won the prize two years later.
Einstein married Löwenthal in 1919. In 1923, he began a relationship with a secretary named Betty Neumann, the niece of his close friend Hans Mühsam. Löwenthal nevertheless remained loyal to him, accompanying him when he emigrated to the United States in 1933. In 1935, she was diagnosed with heart and kidney problems. She died in December 1936.
A volume of Einstein's letters released by Hebrew University of Jerusalem in 2006 added further names to the catalog of women with whom he was romantically involved. They included Margarete Lebach (a married Austrian), Estella Katzenellenbogen (the rich owner of a florist business), Toni Mendel (a wealthy Jewish widow) and Ethel Michanowski (a Berlin socialite), with whom he spent time and from whom he accepted gifts while married to Löwenthal. After being widowed, Einstein was briefly in a relationship with Margarita Konenkova, thought by some to be a Russian spy; her husband, the Russian sculptor Sergei Konenkov, created the bronze bust of Einstein at the Institute for Advanced Study at Princeton.
Following an episode of acute mental illness at about the age of twenty, Einstein's son Eduard was diagnosed with schizophrenia. He spent the remainder of his life either in the care of his mother or in temporary confinement in an asylum. After her death, he was committed permanently to Burghölzli, the Psychiatric University Hospital in Zürich.


=== 1902–1909: Assistant at the Swiss Patent Office ===
Einstein graduated from the federal polytechnic school in 1900, duly certified as competent to teach mathematics and physics. His successful acquisition of Swiss citizenship in February 1901 was not followed by the usual sequel of conscription; the Swiss authorities deemed him medically unfit for military service. He found that Swiss schools too appeared to have no use for him, failing to offer him a teaching position despite the almost two years that he spent applying for one. Eventually it was with the help of Marcel Grossmann's father that he secured a post in Bern at the Swiss Patent Office, as an assistant examiner – level III.
Patent applications that landed on Einstein's desk for his evaluation included ideas for a gravel sorter and an electric typewriter. His employers were pleased enough with his work to make his position permanent in 1903, although they did not think that he should be promoted until he had ""fully mastered machine technology"". It is conceivable that his labors at the patent office had a bearing on his development of his special theory of relativity. He arrived at his revolutionary ideas about space, time and light through thought experiments about the transmission of signals and the synchronization of clocks, matters which also figured in some of the inventions submitted to him for assessment.
In 1902, Einstein and some friends whom he had met in Bern formed a group that held regular meetings to discuss science and philosophy. Their choice of a name for their club, the Olympia Academy, was an ironic comment upon its far from Olympian status. Sometimes they were joined by Marić, who limited her participation in their proceedings to careful listening. The thinkers whose works they reflected upon included Henri Poincaré, Ernst Mach and David Hume, all of whom significantly influenced Einstein's own subsequent ideas and beliefs.


=== 1900–1905: First scientific papers ===

Einstein's first paper, ""Folgerungen aus den Capillaritätserscheinungen"" (""Conclusions drawn from the phenomena of capillarity""), in which he proposed a model of intermolecular attraction that he afterwards disavowed as worthless, was published in the journal Annalen der Physik in 1901. His 24-page doctoral dissertation also addressed a topic in molecular physics. Titled ""Eine neue Bestimmung der Moleküldimensionen"" (""A New Determination of Molecular Dimensions"") and dedicated to his friend Marcel Grossman, it was completed on 30 April 1905 and approved by Professor Alfred Kleiner of the University of Zurich three months later. (Einstein was formally awarded his PhD on 15 January 1906.) Four other pieces of work that Einstein completed in 1905—his famous papers on the photoelectric effect, Brownian motion, his special theory of relativity and the equivalence of mass and energy—have led to the year being celebrated as an annus mirabilis for physics akin to 1666 (the year in which Isaac Newton experienced his greatest epiphanies). The publications deeply impressed Einstein's contemporaries.


=== 1908–1933: Early academic career ===
Einstein's sabbatical as a civil servant approached its end in 1908, when he secured a junior teaching position at the University of Bern. In 1909, a lecture on relativistic electrodynamics that he gave at the University of Zurich, much admired by Alfred Kleiner, led to Zürich's luring him away from Bern with a newly created associate professorship. Promotion to a full professorship followed in April 1911, when he accepted a chair at the German Charles-Ferdinand University in Prague, a move which required him to become an Austrian citizen of the Austro-Hungarian Empire. His time in Prague saw him producing eleven research papers.

 
In July 1912, he returned to his alma mater, the ETH Zurich, to take up a chair in theoretical physics. His teaching activities there centred on thermodynamics and analytical mechanics, and his research interests included the molecular theory of heat, continuum mechanics and the development of a relativistic theory of gravitation. In his work on the latter topic, he was assisted by his friend, Marcel Grossmann, whose knowledge of the kind of mathematics required was greater than his own.
In the spring of 1913, two German visitors, Max Planck and Walther Nernst, called upon Einstein in Zürich in the hope of persuading him to relocate to Berlin. They offered him membership of the Prussian Academy of Sciences, the directorship of the planned Kaiser Wilhelm Institute for Physics and a chair at the Humboldt University of Berlin that would allow him to pursue his research supported by a professorial salary but with no teaching duties to burden him. Their invitation was all the more appealing to him because Berlin happened to be the home of his latest girlfriend, Elsa Löwenthal. He duly joined the Academy on 24 July 1913, and moved into an apartment in the Berlin district of Dahlem on 1 April 1914. He was installed in his Humboldt University position shortly thereafter.
The outbreak of the First World War in July 1914 marked the beginning of Einstein's gradual estrangement from the nation of his birth. When the ""Manifesto of the Ninety-Three"" was published in October 1914—a document signed by a host of prominent German thinkers that justified Germany's belligerence—Einstein was one of the few German intellectuals to distance himself from it and sign the alternative, eirenic ""Manifesto to the Europeans"" instead. However, this expression of his doubts about German policy did not prevent him from being elected to a two-year term as president of the German Physical Society in 1916. When the Kaiser Wilhelm Institute for Physics opened its doors the following year—its foundation delayed because of the war—Einstein was appointed its first director, just as Planck and Nernst had promised.
Einstein was elected a Foreign Member of the Royal Netherlands Academy of Arts and Sciences in 1920, and a Foreign Member of the Royal Society in 1921. In 1922, he was awarded the 1921 Nobel Prize in Physics ""for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect"". At this point some physicists still regarded the general theory of relativity skeptically, and the Nobel citation displayed a degree of doubt even about the work on photoelectricity that it acknowledged: it did not assent to Einstein's notion of the particulate nature of light, which only won over the entire scientific community when S. N. Bose derived the Planck spectrum in 1924. That same year, Einstein was elected an International Honorary Member of the American Academy of Arts and Sciences. Britain's closest equivalent of the Nobel award, the Royal Society's Copley Medal, was not hung around Einstein's neck until 1925. He was elected an International Member of the American Philosophical Society in 1930.
Einstein resigned from the Prussian Academy in March 1933. His accomplishments in Berlin had included the completion of the general theory of relativity, proving the Einstein–de Haas effect, contributing to the quantum theory of radiation, and the development of Bose–Einstein statistics.


=== 1919: Putting general relativity to the test ===

In 1907, Einstein reached a milestone on his long journey from his special theory of relativity to a new idea of gravitation with the formulation of his equivalence principle, which asserts that an observer in an infinitesimally small box falling freely in a gravitational field would be unable to find any evidence that the field exists. In 1911, he used the principle to estimate the amount by which a ray of light from a distant star would be bent by the gravitational pull of the Sun as it passed close to the Sun's photosphere (that is, the Sun's apparent surface). He reworked his calculation in 1913, having now found a way to model gravitation with the Riemann curvature tensor of a non-Euclidean four-dimensional spacetime. By the fall of 1915, his reimagining of the mathematics of gravitation in terms of Riemannian geometry was complete, and he applied his new theory not just to the behavior of the Sun as a gravitational lens but also to another astronomical phenomenon, the precession of the perihelion of Mercury (a slow drift in the point in Mercury's elliptical orbit at which it approaches the Sun most closely). A total eclipse of the Sun that took place on 29 May 1919 provided an opportunity to put his theory of gravitational lensing to the test, and observations performed by Sir Arthur Eddington yielded results that were consistent with his calculations. Eddington's work was reported at length in newspapers around the world. On 7 November 1919, for example, the leading British newspaper, The Times, printed a banner headline that read: ""Revolution in Science – New Theory of the Universe – Newtonian Ideas Overthrown"".


=== 1921–1923: Coming to terms with fame ===

With Eddington's eclipse observations widely reported not just in academic journals but by the popular press as well, Einstein became ""perhaps the world's first celebrity scientist"", a genius who had shattered a paradigm that had been basic to physicists' understanding of the universe since the seventeenth century.
Einstein began his new life as an intellectual icon in America, where he arrived on 2 April 1921. He was welcomed to New York City by Mayor John Francis Hylan, and then spent three weeks giving lectures and attending receptions. He spoke several times at Columbia University and Princeton, and in Washington, he visited the White House with representatives of the National Academy of Sciences. He returned to Europe via London, where he was the guest of the philosopher and statesman Viscount Haldane. He used his time in the British capital to meet several people prominent in British scientific, political or intellectual life, and to deliver a lecture at King's College. In July 1921, he published an essay, ""My First Impression of the U.S.A."", in which he sought to sketch the American character, much as had Alexis de Tocqueville in Democracy in America (1835). He wrote of his transatlantic hosts in highly approving terms: ""What strikes a visitor is the joyous, positive attitude to life ... The American is friendly, self-confident, optimistic, and without envy.""
In 1922, Einstein's travels were to the old world rather than the new. He devoted six months to a tour of Asia that saw him speaking in Japan, Singapore and Sri Lanka (then known as Ceylon). After his first public lecture in Tokyo, he met Emperor Yoshihito and his wife at the Imperial Palace, with thousands of spectators thronging the streets in the hope of catching a glimpse of him. (In a letter to his sons, he wrote that Japanese people seemed to him to be generally modest, intelligent and considerate, and to have a true appreciation of art. But his picture of them in his diary was less flattering: ""[the] intellectual needs of this nation seem to be weaker than their artistic ones – natural disposition?"" His journal also contains views of China and India which were uncomplimentary. Of Chinese people, he wrote that ""even the children are spiritless and look obtuse... It would be a pity if these Chinese supplant all other races. For the likes of us the mere thought is unspeakably dreary"".) He was greeted with even greater enthusiasm on the last leg of his tour, in which he spent twelve days in Mandatory Palestine, newly entrusted to British rule by the League of Nations in the aftermath of the First World War. Sir Herbert Samuel, the British High Commissioner, welcomed him with a degree of ceremony normally only accorded to a visiting head of state, including a cannon salute. One reception held in his honor was stormed by people determined to hear him speak: he told them that he was happy that Jews were beginning to be recognized as a force in the world.
Einstein's decision to tour the eastern hemisphere in 1922 meant that he was unable to go to Stockholm in the December of that year to participate in the Nobel prize ceremony. His place at the traditional Nobel banquet was taken by a German diplomat, who gave a speech praising him not only as a physicist but also as a campaigner for peace. A two-week visit to Spain that he undertook in 1923 saw him collecting another award, a membership of the Spanish Academy of Sciences signified by a diploma handed to him by King Alfonso XIII. (His Spanish trip also gave him a chance to meet a fellow Nobel laureate, the neuroanatomist Santiago Ramón y Cajal.)


=== 1922–1932: Serving the League of Nations ===

From 1922 until 1932, with the exception of a few months in 1923 and 1924, Einstein was a member of the Geneva-based International Committee on Intellectual Cooperation of the League of Nations, a group set up by the League to encourage scientists, artists, scholars, teachers and other people engaged in the life of the mind to work more closely with their counterparts in other countries. He was appointed as a German delegate rather than as a representative of Switzerland because of the machinations of two Catholic activists, Oskar Halecki and Giuseppe Motta. By persuading Secretary General Eric Drummond to deny Einstein the place on the committee reserved for a Swiss thinker, they created an opening for Gonzague de Reynold, who used his League of Nations position as a platform from which to promote traditional Catholic doctrine. Einstein's former physics professor Hendrik Lorentz and the Polish chemist Marie Curie were also members of the committee.


=== 1925: Touring South America ===
In March and April 1925, Einstein and his wife visited South America, where they spent about a week in Brazil, a week in Uruguay and a month in Argentina. Their tour was suggested by Jorge Duclout (1856–1927) and Mauricio Nirenstein (1877–1935) with the support of several Argentine scholars, including Julio Rey Pastor, Jakob Laub, and Leopoldo Lugones. and was financed primarily by the Council of the University of Buenos Aires and the Asociación Hebraica Argentina (Argentine Hebraic Association) with a smaller contribution from the Argentine-Germanic Cultural Institution.


=== 1930–1931: Touring the US ===
In December 1930, Einstein began another significant sojourn in the United States, drawn back to the US by the offer of a two month research fellowship at the California Institute of Technology. Caltech supported him in his wish that he should not be exposed to quite as much attention from the media as he had experienced when visiting the US in 1921, and he therefore declined all the invitations to receive prizes or make speeches that his admirers poured down upon him. But he remained willing to allow his fans at least some of the time with him that they requested.
After arriving in New York City, Einstein was taken to various places and events, including Chinatown, a lunch with the editors of The New York Times, and a performance of Carmen at the Metropolitan Opera, where he was cheered by the audience on his arrival. During the days following, he was given the keys to the city by Mayor Jimmy Walker and met Nicholas Murray Butler, the president of Columbia University, who described Einstein as ""the ruling monarch of the mind"". Harry Emerson Fosdick, pastor at New York's Riverside Church, gave Einstein a tour of the church and showed him a full-size statue that the church made of Einstein, standing at the entrance. Also during his stay in New York, he joined a crowd of 15,000 people at Madison Square Garden during a Hanukkah celebration.

Einstein next traveled to California, where he met Caltech president and Nobel laureate Robert A. Millikan. His friendship with Millikan was ""awkward"", as Millikan ""had a penchant for patriotic militarism"", where Einstein was a pronounced pacifist. During an address to Caltech's students, Einstein noted that science was often inclined to do more harm than good.
This aversion to war also led Einstein to befriend author Upton Sinclair and film star Charlie Chaplin, both noted for their pacifism. Carl Laemmle, head of Universal Studios, gave Einstein a tour of his studio and introduced him to Chaplin. They had an instant rapport, with Chaplin inviting Einstein and his wife, Elsa, to his home for dinner. Chaplin said Einstein's outward persona, calm and gentle, seemed to conceal a ""highly emotional temperament"", from which came his ""extraordinary intellectual energy"".
Chaplin's film City Lights was to premiere a few days later in Hollywood, and Chaplin invited Einstein and Elsa to join him as his special guests. Walter Isaacson, Einstein's biographer, described this as ""one of the most memorable scenes in the new era of celebrity"". Chaplin visited Einstein at his home on a later trip to Berlin and recalled his ""modest little flat"" and the piano at which he had begun writing his theory. Chaplin speculated that it was ""possibly used as kindling wood by the Nazis"".


=== 1933: Emigration to the US ===

In February 1933, while on a visit to the United States, Einstein knew he could not return to Germany with the rise to power of the Nazis under Germany's new chancellor, Adolf Hitler.
While at American universities in early 1933, he undertook his third two-month visiting professorship at the California Institute of Technology in Pasadena. In February and March 1933, the Gestapo repeatedly raided his family's apartment in Berlin. He and his wife Elsa returned to Europe in March, and during the trip, they learned that the German Reichstag had passed the Enabling Act on 23 March, transforming Hitler's government into a de facto legal dictatorship, and that they would not be able to proceed to Berlin. Later on, they heard that their cottage had been raided by the Nazis and Einstein's personal sailboat confiscated. Upon landing in Antwerp, Belgium on 28 March, Einstein immediately went to the German consulate and surrendered his passport, formally renouncing his German citizenship. The Nazis later sold his boat and converted his cottage into a Hitler Youth camp.


==== Refugee status ====

In April 1933, Einstein discovered that the new German government had passed laws barring Jews from holding any official positions, including teaching at universities. Historian Gerald Holton describes how, with ""virtually no audible protest being raised by their colleagues"", thousands of Jewish scientists were suddenly forced to give up their university positions and their names were removed from the rolls of institutions where they were employed.
A month later, Einstein's works were among those targeted by the German Student Union in the Nazi book burnings, with Nazi propaganda minister Joseph Goebbels proclaiming, ""Jewish intellectualism is dead."" One German magazine included him in a list of enemies of the German regime with the phrase, ""not yet hanged"", offering a $5,000 bounty on his head. In a subsequent letter to physicist and friend Max Born, who had already emigrated from Germany to England, Einstein wrote, ""... I must confess that the degree of their brutality and cowardice came as something of a surprise."" After moving to the US, he described the book burnings as a ""spontaneous emotional outburst"" by those who ""shun popular enlightenment"", and ""more than anything else in the world, fear the influence of men of intellectual independence"".
Einstein was now without a permanent home, unsure where he would live and work, and equally worried about the fate of countless other scientists still in Germany. Aided by the Academic Assistance Council, founded in April 1933 by British Liberal politician William Beveridge to help academics escape Nazi persecution, Einstein was able to leave Germany. He rented a house in De Haan, Belgium, where he lived for a few months. In late July 1933, he visited England for about six weeks at the invitation of the British Member of Parliament Commander Oliver Locker-Lampson, who had become friends with him in the preceding years. Locker-Lampson invited him to stay near his Cromer home in a secluded wooden cabin on Roughton Heath in the Parish of Roughton, Norfolk. To protect Einstein, Locker-Lampson had two bodyguards watch over him; a photo of them carrying shotguns and guarding Einstein was published in the Daily Herald on 24 July 1933.

Locker-Lampson took Einstein to meet Winston Churchill at his home, and later, Austen Chamberlain and former Prime Minister Lloyd George. Einstein asked them to help bring Jewish scientists out of Germany. British historian Martin Gilbert notes that Churchill responded immediately, and sent his friend, physicist Frederick Lindemann, to Germany to seek out Jewish scientists and place them in British universities. Churchill later observed that as a result of Germany having driven the Jews out, they had lowered their ""technical standards"" and put the Allies' technology ahead of theirs.
Einstein later contacted leaders of other nations, including Turkey's Prime Minister, İsmet İnönü, to whom he wrote in September 1933, requesting placement of unemployed German-Jewish scientists. As a result of Einstein's letter, Jewish invitees to Turkey eventually totaled over ""1,000 saved individuals"".
Locker-Lampson also submitted a bill to parliament to extend British citizenship to Einstein, during which period Einstein made a number of public appearances describing the crisis brewing in Europe. In one of his speeches he denounced Germany's treatment of Jews, while at the same time he introduced a bill promoting Jewish citizenship in Palestine, as they were being denied citizenship elsewhere. In his speech he described Einstein as a ""citizen of the world"" who should be offered a temporary shelter in the UK. Both bills failed, however, and Einstein then accepted an earlier offer from the Institute for Advanced Study, in Princeton, New Jersey, US, to become a resident scholar.


==== Resident scholar at the Institute for Advanced Study ====

On 3 October 1933, Einstein delivered a speech on the importance of academic freedom before a packed audience at the Royal Albert Hall in London, with The Times reporting he was wildly cheered throughout. Four days later he returned to the US and took up a position at the Institute for Advanced Study, noted for having become a refuge for scientists fleeing Nazi Germany. At the time, most American universities, including Harvard, Princeton and Yale, had minimal or no Jewish faculty or students, as a result of their Jewish quotas, which lasted until the late 1940s.
Einstein was still undecided about his future. He had offers from several European universities, including Christ Church, Oxford, where he stayed for three short periods between May 1931 and June 1933 and was offered a five-year research fellowship (called a ""studentship"" at Christ Church), but in 1935, he arrived at the decision to remain permanently in the United States and apply for citizenship.
Einstein's affiliation with the Institute for Advanced Study would last until his death in 1955. He was one of the four first selected (along with John von Neumann, Kurt Gödel, and Hermann Weyl) at the new Institute. He soon developed a close friendship with Gödel; the two would take long walks together discussing their work. Bruria Kaufman, his assistant, later became a physicist. During this period, Einstein tried to develop a unified field theory and to refute the accepted interpretation of quantum physics, both unsuccessfully. He lived in Princeton at his home from 1935 onwards. The Albert Einstein House was made a National Historic Landmark in 1976.


==== World War II and the Manhattan Project ====

In 1939, a group of Hungarian scientists that included émigré physicist Leó Szilárd attempted to alert Washington to ongoing Nazi atomic bomb research. The group's warnings were discounted. Einstein and Szilárd, along with other refugees such as Edward Teller and Eugene Wigner, ""regarded it as their responsibility to alert Americans to the possibility that German scientists might win the race to build an atomic bomb, and to warn that Hitler would be more than willing to resort to such a weapon."" To make certain the US was aware of the danger, in July 1939, a few months before the beginning of World War II in Europe, Szilárd and Wigner visited Einstein to explain the possibility of atomic bombs, which Einstein, a pacifist, said he had never considered. He was asked to lend his support by writing a letter, with Szilárd, to President Roosevelt, recommending the US pay attention and engage in its own nuclear weapons research.
The letter is believed to be ""arguably the key stimulus for the U.S. adoption of serious investigations into nuclear weapons on the eve of the U.S. entry into World War II"". In addition to the letter, Einstein used his connections with the Belgian royal family and the Belgian queen mother to get access with a personal envoy to the White House's Oval Office. Some say that as a result of Einstein's letter and his meetings with Roosevelt, the US entered the ""race"" to develop the bomb, drawing on its ""immense material, financial, and scientific resources"" to initiate the Manhattan Project.
For Einstein, ""war was a disease ... [and] he called for resistance to war."" By signing the letter to Roosevelt, some argue he went against his pacifist principles. In 1954, a year before his death, Einstein said to his old friend, Linus Pauling, ""I made one great mistake in my life—when I signed the letter to President Roosevelt recommending that atom bombs be made; but there was some justification—the danger that the Germans would make them ..."" In 1955, Einstein and ten other intellectuals and scientists, including British philosopher Bertrand Russell, signed a manifesto highlighting the danger of nuclear weapons. In 1960 Einstein was included posthumously as a charter member of the World Academy of Art and Science (WAAS), an organization founded by distinguished scientists and intellectuals who committed themselves to the responsible and ethical advances of science, particularly in light of the development of nuclear weapons.


==== US citizenship ====

Einstein became an American citizen in 1940. Not long after settling into his career at the Institute for Advanced Study in Princeton, New Jersey, he expressed his appreciation of the meritocracy in American culture compared to Europe. He recognized the ""right of individuals to say and think what they pleased"" without social barriers. As a result, individuals were encouraged, he said, to be more creative, a trait he valued from his early education.
Einstein joined the National Association for the Advancement of Colored People (NAACP) in Princeton, where he campaigned for the civil rights of African Americans. He considered racism America's ""worst disease"", seeing it as ""handed down from one generation to the next"". As part of his involvement, he corresponded with civil rights activist W. E. B. Du Bois and was prepared to testify on his behalf during his trial as an alleged foreign agent in 1951. When Einstein offered to be a character witness for Du Bois, the judge decided to drop the case.
In 1946, Einstein visited Lincoln University in Pennsylvania, a historically black college, where he was awarded an honorary degree. Lincoln was the first university in the United States to grant college degrees to African Americans; alumni include Langston Hughes and Thurgood Marshall. Einstein gave a speech about racism in America, adding, ""I do not intend to be quiet about it."" A resident of Princeton recalls that Einstein had once paid the college tuition for a black student. Einstein has said, ""Being a Jew myself, perhaps I can understand and empathize with how black people feel as victims of discrimination"".


=== Personal views ===


==== Political views ====

In 1918, Einstein was one of the signatories of the founding proclamation of the German Democratic Party, a liberal party. Later in his life, Einstein's political view was in favor of socialism and critical of capitalism, which he detailed in his essays such as ""Why Socialism?"". His opinions on the Bolsheviks also changed with time. In 1925, he criticized them for not having a ""well-regulated system of government"" and called their rule a ""regime of terror and a tragedy in human history"". He later adopted a more moderated view, criticizing their methods but praising them, which is shown by his 1929 remark on Vladimir Lenin:

In Lenin I honor a man, who in total sacrifice of his own person has committed his entire energy to realizing social justice. I do not find his methods advisable. One thing is certain, however: men like him are the guardians and renewers of mankind's conscience.
Einstein offered and was called on to give judgments and opinions on matters often unrelated to theoretical physics or mathematics. He strongly advocated the idea of a democratic global government that would check the power of nation-states in the framework of a world federation. He wrote ""I advocate world government because I am convinced that there is no other possible way of eliminating the most terrible danger in which man has ever found himself."" The FBI created a secret dossier on Einstein in 1932; by the time of his death, it was 1,427 pages long.
Einstein was deeply impressed by Mahatma Gandhi, with whom he corresponded. He described Gandhi as ""a role model for the generations to come"". The initial connection was established on 27 September 1931, when Wilfrid Israel took his Indian guest V. A. Sundaram to meet his friend Einstein at his summer home in the town of Caputh. Sundaram was Gandhi's disciple and special envoy, whom Wilfrid Israel met while visiting India and visiting the Indian leader's home in 1925. During the visit, Einstein wrote a short letter to Gandhi that was delivered to him through his envoy, and Gandhi responded quickly with his own letter. Although in the end Einstein and Gandhi were unable to meet as they had hoped, the direct connection between them was established through Wilfrid Israel.


==== Relationship with Zionism ====

Einstein was a figurehead leader in the establishment of the Hebrew University of Jerusalem, which opened in 1925. Earlier, in 1921, he was asked by the biochemist and president of the World Zionist Organization, Chaim Weizmann, to help raise funds for the planned university. He made suggestions for the creation of an Institute of Agriculture, a Chemical Institute and an Institute of Microbiology in order to fight the various ongoing epidemics such as malaria, which he called an ""evil"" that was undermining a third of the country's development. He also promoted the establishment of an Oriental Studies Institute, to include language courses given in both Hebrew and Arabic.
Einstein was not a nationalist and opposed the creation of an independent Jewish state. He felt that the waves of arriving Jews of the Aliyah could live alongside existing Arabs in Palestine. The state of Israel was established without his help in 1948; Einstein was limited to a marginal role in the Zionist movement. Upon the death of Israeli president Weizmann in November 1952, Prime Minister David Ben-Gurion offered Einstein the largely ceremonial position of President of Israel at the urging of Ezriel Carlebach. The offer was presented by Israel's ambassador in Washington, Abba Eban, who explained that the offer ""embodies the deepest respect which the Jewish people can repose in any of its sons"". Einstein wrote that he was ""deeply moved"", but ""at once saddened and ashamed"" that he could not accept it.


==== Religious and philosophical views ====

Per Lee Smolin, ""I believe what allowed Einstein to achieve so much was primarily a moral quality. He simply cared far more than most of his colleagues that the laws of physics have to explain everything in nature coherently and consistently."" Einstein expounded his spiritual outlook in a wide array of writings and interviews. He said he had sympathy for the impersonal pantheistic God of Baruch Spinoza's philosophy. He did not believe in a personal god who concerns himself with fates and actions of human beings, a view which he described as naïve. He clarified, however, that ""I am not an atheist"", preferring to call himself an agnostic, or a ""deeply religious nonbeliever"". He wrote that ""A spirit is manifest in the laws of the universe—a spirit vastly superior to that of man, and one in the face of which we with our modest powers must feel humble. In this way the pursuit of science leads to a religious feeling of a special sort.""
Einstein was primarily affiliated with non-religious humanist and Ethical Culture groups in both the UK and US. He served on the advisory board of the First Humanist Society of New York, and was an honorary associate of the Rationalist Association, which publishes New Humanist in Britain. For the 75th anniversary of the New York Society for Ethical Culture, he stated that the idea of Ethical Culture embodied his personal conception of what is most valuable and enduring in religious idealism. He observed, ""Without 'ethical culture' there is no salvation for humanity.""

In a German-language letter to philosopher Eric Gutkind, dated 3 January 1954, Einstein wrote:The word God is for me nothing more than the expression and product of human weaknesses, the Bible a collection of honorable, but still primitive legends which are nevertheless pretty childish. No interpretation no matter how subtle can (for me) change this. ... For me the Jewish religion like all other religions is an incarnation of the most childish superstitions. And the Jewish people to whom I gladly belong and with whose mentality I have a deep affinity have no different quality for me than all other people. ... I cannot see anything 'chosen' about them.
Einstein had been sympathetic toward vegetarianism for a long time. In a letter in 1930 to Hermann Huth, vice-president of the German Vegetarian Federation (Deutsche Vegetarier-Bund), he wrote:Although I have been prevented by outward circumstances from observing a strictly vegetarian diet, I have long been an adherent to the cause in principle. Besides agreeing with the aims of vegetarianism for aesthetic and moral reasons, it is my view that a vegetarian manner of living by its purely physical effect on the human temperament would most beneficially influence the lot of mankind.
He became a vegetarian himself only during the last part of his life. In March 1954 he wrote in a letter: ""So I am living without fats, without meat, without fish, but am feeling quite well this way. It almost seems to me that man was not born to be a carnivore.""


==== Love of music ====

Einstein developed an appreciation for music at an early age. In his late journals he wrote:

If I were not a physicist, I would probably be a musician. I often think in music. I live my daydreams in music. I see my life in terms of music ... I get most joy in life out of music.
His mother played the piano reasonably well and wanted her son to learn the violin, not only to instill in him a love of music but also to help him assimilate into German culture. According to conductor Leon Botstein, Einstein began playing when he was 5. However, he did not enjoy it at that age.
When he turned 13, he discovered the violin sonatas of Mozart, whereupon he became enamored of Mozart's compositions and studied music more willingly. Einstein taught himself to play without ""ever practicing systematically"". He said that ""love is a better teacher than a sense of duty"". At the age of 17, he was heard by a school examiner in Aarau while playing Beethoven's violin sonatas. The examiner stated afterward that his playing was ""remarkable and revealing of 'great insight'"". What struck the examiner, writes Botstein, was that Einstein ""displayed a deep love of the music, a quality that was and remains in short supply. Music possessed an unusual meaning for this student.""
Music took on a pivotal and permanent role in Einstein's life from that period on. Although the idea of becoming a professional musician himself was not on his mind at any time, among those with whom Einstein played chamber music were a few professionals, including Kurt Appelbaum, and he performed for private audiences and friends. Chamber music had also become a regular part of his social life while living in Bern, Zürich, and Berlin, where he played with Max Planck and his son, among others. He is sometimes erroneously credited as the editor of the 1937 edition of the Köchel catalog of Mozart's work; that edition was prepared by Alfred Einstein, who may have been a distant relation.
In 1931, while engaged in research at the California Institute of Technology, he visited the Zoellner family conservatory in Los Angeles, where he played some of Beethoven and Mozart's works with members of the Zoellner Quartet. Near the end of his life, when the young Juilliard Quartet visited him in Princeton, he played his violin with them, and the quartet was ""impressed by Einstein's level of coordination and intonation"".


=== Death ===
On 17 April 1955, Einstein experienced internal bleeding caused by the rupture of an abdominal aortic aneurysm, which had previously been reinforced surgically by Rudolph Nissen in 1948. He took the draft of a speech he was preparing for a television appearance commemorating the state of Israel's seventh anniversary with him to the hospital, but he did not live to complete it.
Einstein refused surgery, saying, ""I want to go when I want. It is tasteless to prolong life artificially. I have done my share; it is time to go. I will do it elegantly."" He died in the Princeton Hospital early the next morning at the age of 76, having continued to work until near the end.
During the autopsy, the pathologist Thomas Stoltz Harvey removed Einstein's brain for preservation without the permission of his family, in the hope that the neuroscience of the future would be able to discover what made Einstein so intelligent. Einstein's remains were cremated in Trenton, New Jersey, and his ashes were scattered at an undisclosed location.
In a memorial lecture delivered on 13 December 1965 at UNESCO headquarters, nuclear physicist J. Robert Oppenheimer summarized his impression of Einstein as a person: ""He was almost wholly without sophistication and wholly without worldliness ... There was always with him a wonderful purity at once childlike and profoundly stubborn.""
Einstein bequeathed his personal archives, library, and intellectual assets to the Hebrew University of Jerusalem in Israel.


== Scientific career ==
Throughout his life, Einstein published hundreds of books and articles. He published more than 300 scientific papers and 150 non-scientific ones. On 5 December 2014, universities and archives announced the release of Einstein's papers, comprising more than 30,000 unique documents.  In addition to the work he did by himself he also collaborated with other scientists on additional projects including the Bose–Einstein statistics, the Einstein refrigerator and others.


=== Statistical mechanics ===


==== Thermodynamic fluctuations and statistical physics ====

Einstein's first paper submitted in 1900 to Annalen der Physik was on capillary attraction. It was published in 1901 with the title ""Folgerungen aus den Capillaritätserscheinungen"", which translates as ""Conclusions from the capillarity phenomena"". Two papers he published in 1902–1903 (thermodynamics) attempted to interpret atomic phenomena from a statistical point of view. These papers were the foundation for the 1905 paper on Brownian motion, which showed that Brownian movement can be construed as firm evidence that molecules exist. His research in 1903 and 1904 was mainly concerned with the effect of finite atomic size on diffusion phenomena.


==== Theory of critical opalescence ====

Einstein returned to the problem of thermodynamic fluctuations, giving a treatment of the density variations in a fluid at its critical point. Ordinarily the density fluctuations are controlled by the second derivative of the free energy with respect to the density. At the critical point, this derivative is zero, leading to large fluctuations. The effect of density fluctuations is that light of all wavelengths is scattered, making the fluid look milky white. Einstein relates this to Rayleigh scattering, which is what happens when the fluctuation size is much smaller than the wavelength, and which explains why the sky is blue. Einstein quantitatively derived critical opalescence from a treatment of density fluctuations, and demonstrated how both the effect and Rayleigh scattering originate from the atomistic constitution of matter.


=== 1905 – Annus Mirabilis papers ===
The Annus Mirabilis papers are four articles pertaining to the photoelectric effect (which gave rise to quantum theory), Brownian motion, the special theory of relativity, and E = mc2 that Einstein published in the Annalen der Physik scientific journal in 1905. These four works contributed substantially to the foundation of modern physics and changed views on space, time, and matter. The four papers are:


=== Special relativity ===

Einstein's ""Zur Elektrodynamik bewegter Körper"" (""On the Electrodynamics of Moving Bodies"") was received on 30 June 1905 and published 26 September of that same year. It reconciled conflicts between Maxwell's equations (the laws of electricity and magnetism) and the laws of Newtonian mechanics by introducing changes to the laws of mechanics. Observationally, the effects of these changes are most apparent at high speeds (where objects are moving at speeds close to the speed of light). The theory developed in this paper later became known as Einstein's special theory of relativity.
This paper predicted that, when measured in the frame of a relatively moving observer, a clock carried by a moving body would appear to slow down, and the body itself would contract in its direction of motion. This paper also argued that the idea of a luminiferous aether—one of the leading theoretical entities in physics at the time—was superfluous.
In his paper on mass–energy equivalence, Einstein produced E = mc2 as a consequence of his special relativity equations. Einstein's 1905 work on relativity remained controversial for many years, but was accepted by leading physicists, starting with Max Planck.
Einstein originally framed special relativity in terms of kinematics (the study of moving bodies). In 1908, Hermann Minkowski reinterpreted special relativity in geometric terms as a theory of spacetime. Einstein adopted Minkowski's formalism in his 1915 general theory of relativity.


=== General relativity ===


==== General relativity and the equivalence principle ====

General relativity (GR) is a theory of gravitation that was developed by Einstein between 1907 and 1915. According to it, the observed gravitational attraction between masses results from the warping of spacetime by those masses. General relativity has developed into an essential tool in modern astrophysics; it provides the foundation for the current understanding of black holes, regions of space where gravitational attraction is so strong that not even light can escape.
As Einstein later said, the reason for the development of general relativity was that the preference of inertial motions within special relativity was unsatisfactory, while a theory which from the outset prefers no state of motion (even accelerated ones) should appear more satisfactory. Consequently, in 1907 he published an article on acceleration under special relativity. In that article titled ""On the Relativity Principle and the Conclusions Drawn from It"", he argued that free fall is really inertial motion, and that for a free-falling observer the rules of special relativity must apply. This argument is called the equivalence principle. In the same article, Einstein also predicted the phenomena of gravitational time dilation, gravitational redshift and gravitational lensing.
In 1911, Einstein published another article ""On the Influence of Gravitation on the Propagation of Light"" expanding on the 1907 article, in which he estimated the amount of deflection of light by massive bodies. Thus, the theoretical prediction of general relativity could for the first time be tested experimentally.


==== Gravitational waves ====
In 1916, Einstein predicted gravitational waves, ripples in the curvature of spacetime which propagate as waves, traveling outward from the source, transporting energy as gravitational radiation. The existence of gravitational waves is possible under general relativity due to its Lorentz invariance which brings the concept of a finite speed of propagation of the physical interactions of gravity with it. By contrast, gravitational waves cannot exist in the Newtonian theory of gravitation, which postulates that the physical interactions of gravity propagate at infinite speed.
The first, indirect, detection of gravitational waves came in the 1970s through observation of a pair of closely orbiting neutron stars, PSR B1913+16. The explanation for the decay in their orbital period was that they were emitting gravitational waves. Einstein's prediction was confirmed on 11 February 2016, when researchers at LIGO published the first observation of gravitational waves, detected on Earth on 14 September 2015, nearly one hundred years after the prediction.


==== Hole argument and Entwurf theory ====
While developing general relativity, Einstein became confused about the gauge invariance in the theory. He formulated an argument that led him to conclude that a general relativistic field theory is impossible. He gave up looking for fully generally covariant tensor equations and searched for equations that would be invariant under general linear transformations only.
In June 1913, the Entwurf ('draft') theory was the result of these investigations. As its name suggests, it was a sketch of a theory, less elegant and more difficult than general relativity, with the equations of motion supplemented by additional gauge fixing conditions. After more than two years of intensive work, Einstein realized that the hole argument was mistaken and abandoned the theory in November 1915.


==== Physical cosmology ====

In 1917, Einstein applied the general theory of relativity to the structure of the universe as a whole. He discovered that the general field equations predicted a universe that was dynamic, either contracting or expanding. As observational evidence for a dynamic universe was lacking at the time, Einstein introduced a new term, the cosmological constant, into the field equations, in order to allow the theory to predict a static universe. The modified field equations predicted a static universe of closed curvature, in accordance with Einstein's understanding of Mach's principle in these years. This model became known as the Einstein World or Einstein's static universe.
Following the discovery of the recession of the galaxies by Edwin Hubble in 1929, Einstein abandoned his static model of the universe, and proposed two dynamic models of the cosmos, the Friedmann–Einstein universe of 1931 and the Einstein–de Sitter universe of 1932. In each of these models, Einstein discarded the cosmological constant, claiming that it was ""in any case theoretically unsatisfactory"".
In many Einstein biographies, it is claimed that Einstein referred to the cosmological constant in later years as his ""biggest blunder"", based on a letter George Gamow claimed to have received from him. The astrophysicist Mario Livio has cast doubt on this claim.
In late 2013, a team led by the Irish physicist Cormac O'Raifeartaigh discovered evidence that, shortly after learning of Hubble's observations of the recession of the galaxies, Einstein considered a steady-state model of the universe. In a hitherto overlooked manuscript, apparently written in early 1931, Einstein explored a model of the expanding universe in which the density of matter remains constant due to a continuous creation of matter, a process that he associated with the cosmological constant. As he stated in the paper, ""In what follows, I would like to draw attention to a solution to equation (1) that can account for Hubbel's [sic] facts, and in which the density is constant over time"" ... ""If one considers a physically bounded volume, particles of matter will be continually leaving it. For the density to remain constant, new particles of matter must be continually formed in the volume from space.""
It thus appears that Einstein considered a steady-state model of the expanding universe many years before Hoyle, Bondi and Gold. However, Einstein's steady-state model contained a fundamental flaw and he quickly abandoned the idea.


==== Energy momentum pseudotensor ====

General relativity includes a dynamical spacetime, so it is difficult to see how to identify the conserved energy and momentum. Noether's theorem allows these quantities to be determined from a Lagrangian with translation invariance, but general covariance makes translation invariance into something of a gauge symmetry. The energy and momentum derived within general relativity by Noether's prescriptions do not make a real tensor for this reason.
Einstein argued that this is true for a fundamental reason: the gravitational field could be made to vanish by a choice of coordinates. He maintained that the non-covariant energy momentum pseudotensor was, in fact, the best description of the energy momentum distribution in a gravitational field. While the use of non-covariant objects like pseudotensors was criticized by Erwin Schrödinger and others, Einstein's approach has been echoed by physicists including Lev Landau and Evgeny Lifshitz.


==== Wormholes ====
In 1935, Einstein collaborated with Nathan Rosen to produce a model of a wormhole, often called Einstein–Rosen bridges. His motivation was to model elementary particles with charge as a solution of gravitational field equations, in line with the program outlined in the paper ""Do Gravitational Fields play an Important Role in the Constitution of the Elementary Particles?"". These solutions cut and pasted Schwarzschild black holes to make a bridge between two patches. Because these solutions included spacetime curvature without the presence of a physical body, Einstein and Rosen suggested that they could provide the beginnings of a theory that avoided the notion of point particles. However, it was later found that Einstein–Rosen bridges are not stable.


==== Einstein–Cartan theory ====

In order to incorporate spinning point particles into general relativity, the affine connection needed to be generalized to include an antisymmetric part, called the torsion. This modification was made by Einstein and Cartan in the 1920s.


==== Equations of motion ====

In general relativity, gravitational force is reimagined as curvature of spacetime. A curved path like an orbit is not the result of a force deflecting a body from an ideal straight-line path, but rather the body's attempt to fall freely through a background that is itself curved by the presence of other masses. A remark by John Archibald Wheeler that has become proverbial among physicists summarizes the theory: ""Spacetime tells matter how to move; matter tells spacetime how to curve."" The Einstein field equations cover the latter aspect of the theory, relating the curvature of spacetime to the distribution of matter and energy. The geodesic equation covers the former aspect, stating that freely falling bodies follow lines that are as straight as possible in a curved spacetime. Einstein regarded this as an ""independent fundamental assumption"" that had to be postulated in addition to the field equations in order to complete the theory. Believing this to be a shortcoming in how general relativity was originally presented, he wished to derive it from the field equations themselves. Since the equations of general relativity are non-linear, a lump of energy made out of pure gravitational fields, like a black hole, would move on a trajectory which is determined by the Einstein field equations themselves, not by a new law. Accordingly, Einstein proposed that the field equations would determine the path of a singular solution, like a black hole, to be a geodesic. Both physicists and philosophers have often repeated the assertion that the geodesic equation can be obtained from applying the field equations to the motion of a gravitational singularity, but this claim remains disputed.


=== Old quantum theory ===


==== Photons and energy quanta ====

In a 1905 paper, Einstein postulated that light itself consists of localized particles (quanta). Einstein's light quanta were nearly universally rejected by all physicists, including Max Planck and Niels Bohr. This idea only became universally accepted in 1919, with Robert Millikan's detailed experiments on the photoelectric effect, and with the measurement of Compton scattering.
Einstein concluded that each wave of frequency f is associated with a collection of photons with energy hf each, where h is the Planck constant. He did not say much more, because he was not sure how the particles were related to the wave. But he did suggest that this idea would explain certain experimental results, notably the photoelectric effect. Light quanta were dubbed photons by Gilbert N. Lewis in 1926.


==== Quantized atomic vibrations ====

In 1907, Einstein proposed a model of matter where each atom in a lattice structure is an independent harmonic oscillator. In the Einstein model, each atom oscillates independently—a series of equally spaced quantized states for each oscillator. Einstein was aware that getting the frequency of the actual oscillations would be difficult, but he nevertheless proposed this theory because it was a particularly clear demonstration that quantum mechanics could solve the specific heat problem in classical mechanics. Peter Debye refined this model.


==== Bose–Einstein statistics ====

In 1924, Einstein received a description of a statistical model from Indian physicist Satyendra Nath Bose, based on a counting method that assumed that light could be understood as a gas of indistinguishable particles. Einstein noted that Bose's statistics applied to some atoms as well as to the proposed light particles, and submitted his translation of Bose's paper to the Zeitschrift für Physik. Einstein also published his own articles describing the model and its implications, among them the Bose–Einstein condensate phenomenon that some particulates should appear at very low temperatures. It was not until 1995 that the first such condensate was produced experimentally by Eric Allin Cornell and Carl Wieman using ultra-cooling equipment built at the NIST–JILA laboratory at the University of Colorado at Boulder. Bose–Einstein statistics are now used to describe the behaviors of any assembly of bosons. Einstein's sketches for this project may be seen in the Einstein Archive in the library of the Leiden University.


==== Wave–particle duality ====

Although the patent office promoted Einstein to Technical Examiner Second Class in 1906, he had not given up on academia. In 1908, he became a Privatdozent at the University of Bern. In ""Über die Entwicklung unserer Anschauungen über das Wesen und die Konstitution der Strahlung"" (""The Development of our Views on the Composition and Essence of Radiation""), on the quantization of light, and in an earlier 1909 paper, Einstein showed that Max Planck's energy quanta must have well-defined momenta and act in some respects as independent, point-like particles. This paper introduced the photon concept and inspired the notion of wave–particle duality in quantum mechanics. Einstein saw this wave–particle duality in radiation as concrete evidence for his conviction that physics needed a new, unified foundation.


==== Zero-point energy ====
In a series of works completed from 1911 to 1913, Planck reformulated his 1900 quantum theory and introduced the idea of zero-point energy in his ""second quantum theory"". Soon, this idea attracted the attention of Einstein and his assistant Otto Stern. Assuming the energy of rotating diatomic molecules contains zero-point energy, they then compared the theoretical specific heat of hydrogen gas with the experimental data. The numbers matched nicely. However, after publishing the findings, they promptly withdrew their support, because they no longer had confidence in the correctness of the idea of zero-point energy.


==== Stimulated emission ====
In 1917, at the height of his work on relativity, Einstein published an article in Physikalische Zeitschrift that proposed the possibility of stimulated emission, the physical process that makes possible the maser and the laser.
This article showed that the statistics of absorption and emission of light would only be consistent with Planck's distribution law if the emission of light into a mode with n photons would be enhanced statistically compared to the emission of light into an empty mode. This paper was enormously influential in the later development of quantum mechanics, because it was the first paper to show that the statistics of atomic transitions had simple laws.


==== Matter waves ====
Einstein discovered Louis de Broglie's work and supported his ideas, which were received skeptically at first. In another major paper from this era, Einstein observed that de Broglie waves could explain the quantization rules of Bohr and Sommerfeld. This paper would inspire Schrödinger's work of 1926.


=== Quantum mechanics ===


==== Einstein's objections to quantum mechanics ====

Einstein played a major role in developing quantum theory, beginning with his 1905 paper on the photoelectric effect. However, he became displeased with modern quantum mechanics as it had evolved after 1925, despite its acceptance by other physicists. He was skeptical that the randomness of quantum mechanics was fundamental rather than the result of determinism, stating that God ""is not playing at dice"". Until the end of his life, he continued to maintain that quantum mechanics was incomplete.


==== Bohr versus Einstein ====

 The Bohr–Einstein debates were a series of public disputes about quantum mechanics between Einstein and Niels Bohr, who were two of its founders. Their debates are remembered because of their importance to the philosophy of science. Their debates would influence later interpretations of quantum mechanics.


==== Einstein–Podolsky–Rosen paradox ====

Einstein never fully accepted quantum mechanics. While he recognized that it made correct predictions, he believed a more fundamental description of nature must be possible. Over the years he presented multiple arguments to this effect, but the one he preferred most dated to a debate with Bohr in 1930. Einstein suggested a thought experiment in which two objects are allowed to interact and then moved apart a great distance from each other. The quantum-mechanical description of the two objects is a mathematical entity known as a wavefunction. If the wavefunction that describes the two objects before their interaction is given, then the Schrödinger equation provides the wavefunction that describes them after their interaction. But because of what would later be called quantum entanglement, measuring one object would lead to an instantaneous change of the wavefunction describing the other object, no matter how far away it is. Moreover, the choice of which measurement to perform upon the first object would affect what wavefunction could result for the second object. Einstein reasoned that no influence could propagate from the first object to the second instantaneously fast. Indeed, he argued, physics depends on being able to tell one thing apart from another, and such instantaneous influences would call that into question. Because the true ""physical condition"" of the second object could not be immediately altered by an action done to the first, Einstein concluded, the wavefunction could not be that true physical condition, only an incomplete description of it.
A more famous version of this argument came in 1935, when Einstein published a paper with Boris Podolsky and Nathan Rosen that laid out what would become known as the EPR paradox. In this thought experiment, two particles interact in such a way that the wavefunction describing them is entangled. Then, no matter how far the two particles were separated, a precise position measurement on one particle would imply the ability to predict, perfectly, the result of measuring the position of the other particle. Likewise, a precise momentum measurement of one particle would result in an equally precise prediction for of the momentum of the other particle, without needing to disturb the other particle in any way. They argued that no action taken on the first particle could instantaneously affect the other, since this would involve information being transmitted faster than light, which is forbidden by the theory of relativity. They invoked a principle, later known as the ""EPR criterion of reality"", positing that: ""If, without in any way disturbing a system, we can predict with certainty (i.e., with probability equal to unity) the value of a physical quantity, then there exists an element of reality corresponding to that quantity."" From this, they inferred that the second particle must have a definite value of both position and of momentum prior to either quantity being measured. But quantum mechanics considers these two observables incompatible and thus does not associate simultaneous values for both to any system. Einstein, Podolsky, and Rosen therefore concluded that quantum theory does not provide a complete description of reality.
In 1964, John Stewart Bell carried the analysis of quantum entanglement much further. He deduced that if measurements are performed independently on the two separated particles of an entangled pair, then the assumption that the outcomes depend upon hidden variables within each half implies a mathematical constraint on how the outcomes on the two measurements are correlated. This constraint would later be called a Bell inequality. Bell then showed that quantum physics predicts correlations that violate this inequality. Consequently, the only way that hidden variables could explain the predictions of quantum physics is if they are ""nonlocal"", which is to say that somehow the two particles are able to interact instantaneously no matter how widely they ever become separated. Bell argued that because an explanation of quantum phenomena in terms of hidden variables would require nonlocality, the EPR paradox ""is resolved in the way which Einstein would have liked least"".
Despite this, and although Einstein personally found the argument in the EPR paper overly complicated, that paper became among the most influential papers published in Physical Review. It is considered a centerpiece of the development of quantum information theory.


=== Unified field theory ===

Encouraged by his success with general relativity, Einstein sought an even more ambitious geometrical theory that would treat gravitation and electromagnetism as aspects of a single entity. In 1950, he described his unified field theory in a Scientific American article titled ""On the Generalized Theory of Gravitation"". His attempt to find the most fundamental laws of nature won him praise but not success: a particularly conspicuous blemish of his model was that it did not accommodate the strong and weak nuclear forces, neither of which was well understood until many years after his death. Although most researchers now believe that Einstein's approach to unifying physics was mistaken, his goal of a theory of everything is one to which his successors still aspire.


=== Other investigations ===

Einstein conducted other investigations that were unsuccessful and abandoned. These pertain to force, superconductivity, and other research.


=== Collaboration with other scientists ===

In addition to longtime collaborators Leopold Infeld, Nathan Rosen, Peter Bergmann and others, Einstein also had some one-shot collaborations with various scientists.


==== Einstein–de Haas experiment ====

In 1908, Owen Willans Richardson predicted that a change in the magnetic moment of a free body will cause this body to rotate. This effect is a consequence of the conservation of angular momentum and is strong enough to be observable in ferromagnetic materials. Einstein and Wander Johannes de Haas published two papers in 1915 claiming the first experimental observation of the effect. Measurements of this kind demonstrate that the phenomenon of magnetization is caused by the alignment (polarization) of the angular momenta of the electrons in the material along the axis of magnetization. These measurements also allow the separation of the two contributions to the magnetization: that which is associated with the spin and with the orbital motion of the electrons. The Einstein-de Haas experiment is the only experiment concived, realized and published by Albert Einstein himself.
A complete original version of the Einstein-de Haas experimental equipment was donated by Geertruida de Haas-Lorentz, wife of de Haas and daughter of Lorentz, to the Ampère Museum in Lyon France in 1961 where it is currently on display. It was lost among the museum's holdings and was rediscovered in 2023.


==== Einstein as an inventor ====
In 1926, Einstein and his former student Leó Szilárd co-invented (and in 1930, patented) the Einstein refrigerator. This absorption refrigerator was then revolutionary for having no moving parts and using only heat as an input. On 11 November 1930, U.S. patent 1,781,541 was awarded to Einstein and Leó Szilárd for the refrigerator. Their invention was not immediately put into commercial production, but the most promising of their patents were acquired by the Swedish company Electrolux.
Einstein also invented an electromagnetic pump, sound reproduction device, and several other household devices.


== Non-scientific legacy ==

While traveling, Einstein wrote daily to his wife Elsa and adopted stepdaughters Margot and Ilse. The letters were included in the papers bequeathed to the Hebrew University of Jerusalem. Margot Einstein permitted the personal letters to be made available to the public, but requested that it not be done until twenty years after her death (she died in 1986). Barbara Wolff, of the Hebrew University's Albert Einstein Archives, told the BBC that there are about 3,500 pages of private correspondence written between 1912 and 1955.
Einstein's right of publicity was litigated in 2015 in a federal district court in California. Although the court initially held that the right had expired, that ruling was immediately appealed, and the decision was later vacated in its entirety. The underlying claims between the parties in that lawsuit were ultimately settled. The right is enforceable, and the Hebrew University of Jerusalem is the exclusive representative of that right. Corbis, successor to The Roger Richman Agency, licenses the use of his name and associated imagery, as agent for the university.
Mount Einstein in the Chugach Mountains of Alaska was named in 1955.
Mount Einstein in New Zealand's Paparoa Range was named after him in 1970 by the Department of Scientific and Industrial Research.


== In popular culture ==

Einstein became one of the most famous scientific celebrities after the confirmation of his general theory of relativity in 1919. Although most of the public had little understanding of his work, he was widely recognized and admired. In the period before World War II, The New Yorker published a vignette in their ""The Talk of the Town"" feature saying that Einstein was so well known in America that he would be stopped on the street by people wanting him to explain ""that theory"". Eventually he came to cope with unwanted enquirers by pretending to be someone else: ""Pardon me, sorry! Always I am mistaken for Professor Einstein.""
Einstein has been the subject of or inspiration for many novels, films, plays, and works of music. He is a favorite model for depictions of absent-minded professors; his expressive face and distinctive hairstyle have been widely copied and exaggerated. Time magazine's Frederic Golden wrote that Einstein was ""a cartoonist's dream come true"".
Many popular quotations are often misattributed to him.


== Awards and honors ==

Einstein received numerous awards and honors, and in 1922, he was awarded the 1921 Nobel Prize in Physics ""for his services to Theoretical Physics, and especially for his discovery of the law of the photoelectric effect"". None of the nominations in 1921 met the criteria set by Alfred Nobel, so the 1921 prize was carried forward and awarded to Einstein in 1922.
Einsteinium, a synthetic chemical element, was named in his honor in 1955, a few months after his death.


== Publications ==


=== Scientific ===


=== Others ===


== See also ==


== Notes ==


== References ==


=== Works cited ===


== Further reading ==


== External links ==

Works by Albert Einstein at Project Gutenberg
Works by or about Albert Einstein at the Internet Archive
Works by Albert Einstein at LibriVox (public domain audiobooks) 
Einstein's Personal Correspondence: Religion, Politics, The Holocaust, and Philosophy Shapell Manuscript Foundation
Federal Bureau of Investigation file on Albert Einstein
Einstein and his love of music, Physics World
Albert Einstein on Nobelprize.org  including the Nobel Lecture 11 July 1923 Fundamental ideas and problems of the theory of relativity
Albert Einstein Archives Online (80,000+ Documents) Archived 11 August 2011 at the Wayback Machine (MSNBC, 19 March 2012)
Einstein's declaration of intention for American citizenship on the World Digital Library
Albert Einstein Collection at Brandeis University
The Collected Papers of Albert Einstein ""Digital Einstein"" at Princeton University
Newspaper clippings about Albert Einstein in the 20th Century Press Archives of the ZBW
Home page of Albert Einstein at The Institute for Advanced Study
Albert – The Digital Repository of the IAS, which contains many digitized original documents and photographs
Albert Einstein at IMDb"
World War II,"World War I or the First World War (28 July 1914 – 11 November 1918), also known as the Great War, was a global conflict between two coalitions: the Allies (or Entente) and the Central Powers. Fighting took place mainly in Europe and the Middle East, as well as in parts of Africa and the Asia-Pacific, and in Europe was characterised by trench warfare and the use of artillery, machine guns, and chemical weapons (gas). World War I was one of the deadliest conflicts in history, resulting in an estimated 9 million military dead and 23 million wounded, plus up to 8 million civilian deaths from causes including genocide. The movement of large numbers of people was a major factor in the Spanish flu pandemic, which killed millions.
The causes of World War I included the rise of Germany and decline of the Ottoman Empire, which disturbed the long-standing balance of power in Europe, as well as economic competition between nations triggered by industrialisation and imperialism. Growing tensions between the great powers and in the Balkans reached a breaking point on 28 June 1914, when a Bosnian Serb named Gavrilo Princip assassinated Archduke Franz Ferdinand, heir to the Austro-Hungarian throne. Austria-Hungary held Serbia responsible, and declared war on 28 July. After Russia mobilised in Serbia's defence, Germany declared war on Russia; by 4 August, France and the United Kingdom were drawn in, with the Ottomans joining in November. Germany's strategy in 1914 was to quickly defeat France, then to transfer its forces to the east. However, this failed, and by the end of the year the Western Front consisted of a continuous line of trenches stretching from the English Channel to Switzerland. The Eastern Front was more dynamic, but neither side gained a decisive advantage, despite costly offensives. Italy, Bulgaria, Romania, Greece and others joined in from 1915 onward.
In April 1917, the United States entered the war on the Allied side following Germany's resumption of unrestricted submarine warfare against Atlantic shipping. Later that year, the Bolsheviks seized power in the Russian October Revolution; Soviet Russia signed an armistice with the Central Powers in December, followed by a separate peace in March 1918. That month, Germany launched an offensive in the west, which despite initial successes left the German Army exhausted and demoralised. A successful Allied counter-offensive from August 1918 caused a collapse of the German front line. By early November, Bulgaria, the Ottoman Empire and Austria-Hungary had each signed armistices with the Allies, leaving Germany isolated. Facing a revolution at home, Kaiser Wilhelm II abdicated on 9 November, and the war ended with the Armistice of 11 November 1918.
The Paris Peace Conference of 1919–1920 imposed settlements on the defeated powers, most notably the Treaty of Versailles, by which Germany lost significant territories, was disarmed, and was required to pay large war reparations to the Allies. The dissolution of the Russian, German, Austro-Hungarian, and Ottoman Empires redrew national boundaries and resulted in the creation of new independent states, including Poland, Finland, the Baltic states, Czechoslovakia, and Yugoslavia. The League of Nations was established to maintain world peace, but its failure to manage instability during the interwar period contributed to the outbreak of World War II in 1939.


== Names ==
Before World War II, the events of 1914–1918 were generally known as the Great War or simply the World War. In August 1914, the magazine The Independent wrote ""This is the Great War. It names itself"". In October 1914, the Canadian magazine Maclean's similarly wrote, ""Some wars name themselves. This is the Great War."" Contemporary Europeans also referred to it as ""the war to end war"" and it was also described as ""the war to end all wars"" due to their perception of its unparalleled scale, devastation, and loss of life. The first recorded use of the term First World War was in September 1914 by German biologist and philosopher Ernst Haeckel who stated, ""There is no doubt that the course and character of the feared 'European War' ... will become the first world war in the full sense of the word.""


== Background ==


=== Political and military alliances ===

For much of the 19th century, the major European powers maintained a tenuous balance of power, known as the Concert of Europe. After 1848, this was challenged by Britain's withdrawal into so-called splendid isolation, the decline of the Ottoman Empire, New Imperialism, and the rise of Prussia under Otto von Bismarck. Victory in the 1870–1871 Franco-Prussian War allowed Bismarck to consolidate a German Empire. Post-1871, the primary aim of French policy was to avenge this defeat, but by the early 1890s, this had switched to the expansion of the French colonial empire.
In 1873, Bismarck negotiated the League of the Three Emperors, which included Austria-Hungary, Russia and Germany. After the 1877–1878 Russo-Turkish War, the League was dissolved due to Austrian concerns over the expansion of Russian influence in the Balkans, an area they considered to be of vital strategic interest. Germany and Austria-Hungary then formed the 1879 Dual Alliance, which became the Triple Alliance when Italy joined in 1882. For Bismarck, the purpose of these agreements was to isolate France by ensuring the three Empires resolve any disputes between themselves. In 1887, Bismarck set up the Reinsurance Treaty, a secret agreement between Germany and Russia to remain neutral if either were attacked by France or Austria-Hungary.

For Bismarck, peace with Russia was the foundation of German foreign policy but in 1890, he was forced to retire by Wilhelm II. The latter was persuaded not to renew the Reinsurance Treaty by his new Chancellor, Leo von Caprivi. This gave France an opening to agree the Franco-Russian Alliance in 1894, which was then followed by the 1904 Entente Cordiale with Britain. The Triple Entente was completed by the 1907 Anglo-Russian Convention. While not formal alliances, by settling long-standing colonial disputes in Asia and Africa, British support for France or Russia in any future conflict became a possibility. This was accentuated by British and Russian support for France against Germany during the 1911 Agadir Crisis.


=== Arms race ===

German economic and industrial strength continued to expand rapidly post-1871. Backed by Wilhelm II, Admiral Alfred von Tirpitz sought to use this growth to build an Imperial German Navy, that could compete with the British Royal Navy. This policy was based on the work of US naval author Alfred Thayer Mahan, who argued that possession of a blue-water navy was vital for global power projection; Tirpitz had his books translated into German, while Wilhelm made them required reading for his advisors and senior military personnel.
However, it was also an emotional decision, driven by Wilhelm's simultaneous admiration for the Royal Navy and desire to surpass it. Bismarck thought that the British would not interfere in Europe, as long as its maritime supremacy remained secure, but his dismissal in 1890 led to a change in policy and an Anglo-German naval arms race began. Despite the vast sums spent by Tirpitz, the launch of HMS Dreadnought in 1906 gave the British a technological advantage. Ultimately, the race diverted huge resources into creating a German navy large enough to antagonise Britain, but not defeat it; in 1911, Chancellor Theobald von Bethmann Hollweg acknowledged defeat, leading to the Rüstungswende or 'armaments turning point', when he switched expenditure from the navy to the army.
This decision was not driven by a reduction in political tensions but by German concern over Russia's quick recovery from its defeat in the Russo-Japanese War and subsequent 1905 Russian Revolution. Economic reforms led to a significant post-1908 expansion of railways and transportation infrastructure, particularly in its western border regions. Since Germany and Austria-Hungary relied on faster mobilisation to compensate for their numerical inferiority compared to Russia, the threat posed by the closing of this gap was more important than competing with the Royal Navy. After Germany expanded its standing army by 170,000 troops in 1913, France extended compulsory military service from two to three years; similar measures were taken by the Balkan powers and Italy, which led to increased expenditure by the Ottomans and Austria-Hungary. Absolute figures are difficult to calculate due to differences in categorising expenditure since they often omit civilian infrastructure projects like railways which had logistical importance and military use. It is known, however, that from 1908 to 1913, military spending by the six major European powers increased by over 50% in real terms.


=== Conflicts in the Balkans ===

The years before 1914 were marked by a series of crises in the Balkans, as other powers sought to benefit from the Ottoman decline. While Pan-Slavic and Orthodox Russia considered itself the protector of Serbia and other Slav states, they preferred the strategically vital Bosporus straits to be controlled by a weak Ottoman government, rather than an ambitious Slav power like Bulgaria. Russia had ambitions in northeastern Anatolia while its clients had overlapping claims in the Balkans. These competing interests divided Russian policy-makers and added to regional instability.
Austrian statesmen viewed the Balkans as essential for the continued existence of their Empire and saw Serbian expansion as a direct threat. The 1908–1909 Bosnian Crisis began when Austria annexed the former Ottoman territory of Bosnia and Herzegovina, which it had occupied since 1878. Timed to coincide with the Bulgarian Declaration of Independence from the Ottoman Empire, this unilateral action was denounced by the European powers, but accepted as there was no consensus on how to resolve the situation. Some historians see this as a significant escalation, ending any chance of Austria cooperating with Russia in the Balkans, while also damaging diplomatic relations between Serbia and Italy.
Tensions increased after the 1911–1912 Italo-Turkish War demonstrated Ottoman weakness and led to the formation of the Balkan League, an alliance of Serbia, Bulgaria, Montenegro, and Greece. The League quickly overran most of the Ottomans' territory in the Balkans during the 1912–1913 First Balkan War, much to the surprise of outside observers. The Serbian capture of ports on the Adriatic resulted in partial Austrian mobilisation, starting on 21 November 1912, including units along the Russian border in Galicia. The Russian government decided not to mobilise in response, unprepared to precipitate a war.
The Great Powers sought to re-assert control through the 1913 Treaty of London, which had created an independent Albania while enlarging the territories of Bulgaria, Serbia, Montenegro and Greece. However, disputes between the victors sparked the 33-day Second Balkan War, when Bulgaria attacked Serbia and Greece on 16 June 1913; it was defeated, losing most of Macedonia to Serbia and Greece, and Southern Dobruja to Romania. The result was that even countries which benefited from the Balkan Wars, such as Serbia and Greece, felt cheated of their ""rightful gains"", while for Austria it demonstrated the apparent indifference with which other powers viewed their concerns, including Germany. This complex mix of resentment, nationalism and insecurity helps explain why the pre-1914 Balkans became known as the ""powder keg of Europe"".


== Prelude ==


=== Sarajevo assassination ===

On 28 June 1914, Archduke Franz Ferdinand of Austria, heir presumptive to Emperor Franz Joseph I of Austria, visited Sarajevo, the capital of the recently annexed Bosnia and Herzegovina. Cvjetko Popović, Gavrilo Princip, Nedeljko Čabrinović, Trifko Grabež, Vaso Čubrilović (Bosnian Serbs) and Muhamed Mehmedbašić (from the Bosniaks community), from the movement known as Young Bosnia, took up positions along the Archduke's motorcade route, to assassinate him. Supplied with arms by extremists within the Serbian Black Hand intelligence organisation, they hoped his death would free Bosnia from Austrian rule.
Čabrinović threw a grenade at the Archduke's car and injured two of his aides. The other assassins were also unsuccessful. An hour later, as Ferdinand was returning from visiting the injured officers in hospital, his car took a wrong turn into a street where Gavrilo Princip was standing. He fired two pistol shots, fatally wounding Ferdinand and his wife Sophie.
According to historian Zbyněk Zeman, in Vienna ""the event almost failed to make any impression whatsoever. On 28 and 29 June, the crowds listened to music and drank wine, as if nothing had happened."" Nevertheless, the impact of the murder of the heir to the throne was significant, and has been described by historian Christopher Clark as a ""9/11 effect, a terrorist event charged with historic meaning, transforming the political chemistry in Vienna"".


=== Expansion of violence in Bosnia and Herzegovina ===

Austro-Hungarian authorities encouraged subsequent anti-Serb riots in Sarajevo. Violent actions against ethnic Serbs were also organised outside Sarajevo, in other cities in Austro-Hungarian-controlled Bosnia and Herzegovina, Croatia and Slovenia. Austro-Hungarian authorities in Bosnia and Herzegovina imprisoned approximately 5,500 prominent Serbs, 700 to 2,200 of whom died in prison. A further 460 Serbs were sentenced to death. A predominantly Bosniak special militia known as the Schutzkorps was established, and carried out the persecution of Serbs.


=== July Crisis ===

The assassination initiated the July Crisis, a month of diplomatic manoeuvring between Austria-Hungary, Germany, Russia, France and Britain. Believing that Serbian intelligence helped organise Franz Ferdinand's murder, Austrian officials wanted to use the opportunity to end their interference in Bosnia and saw war as the best way of achieving this. However, the Foreign Ministry had no solid proof of Serbian involvement. On 23 July, Austria delivered an ultimatum to Serbia, listing ten demands made intentionally unacceptable to provide an excuse for starting hostilities.
Serbia ordered general mobilization on 25 July, but accepted all the terms, except for those empowering Austrian representatives to suppress ""subversive elements"" inside Serbia, and take part in the investigation and trial of Serbians linked to the assassination. Claiming this amounted to rejection, Austria broke off diplomatic relations and ordered partial mobilisation the next day; on 28 July, they declared war on Serbia and began shelling Belgrade. Russia ordered general mobilization in support of Serbia on 30 July.
Anxious to ensure backing from the SPD political opposition by presenting Russia as the aggressor, German Chancellor Bethmann Hollweg delayed the commencement of war preparations until 31 July. That afternoon, the Russian government were handed a note requiring them to ""cease all war measures against Germany and Austria-Hungary"" within 12 hours. A further German demand for neutrality was refused by the French who ordered general mobilization but delayed declaring war. The German General Staff had long assumed they faced a war on two fronts; the Schlieffen Plan envisaged using 80% of the army to defeat France, then switching to Russia. Since this required them to move quickly, mobilization orders were issued that afternoon. Once the German ultimatum to Russia expired on the morning of 1 August, the two countries were at war. 
At a meeting on 29 July, the British cabinet had narrowly decided its obligations to Belgium under the 1839 Treaty of London did not require it to oppose a German invasion with military force; however, Prime Minister Asquith and his senior Cabinet ministers were already committed to supporting France, the Royal Navy had been mobilised, and public opinion was strongly in favour of intervention. On 31 July, Britain sent notes to Germany and France, asking them to respect Belgian neutrality; France pledged to do so, but Germany did not reply. Aware of German plans to attack through Belgium, French Commander-in-Chief Joseph Joffre asked his government for permission to cross the border and pre-empt such a move. To avoid violating Belgian neutrality, he was told any advance could come only after a German invasion. Instead, the French cabinet ordered its Army to withdraw 10 km behind the German frontier, to avoid provoking war. On 2 August, Germany occupied Luxembourg and exchanged fire with French units when German patrols entered French territory; on 3 August, they declared war on France and demanded free passage across Belgium, which was refused. Early on the morning of 4 August, the Germans invaded, and Albert I of Belgium called for assistance under the Treaty of London. Britain sent Germany an ultimatum demanding they withdraw from Belgium; when this expired at midnight, without a response, the two empires were at war.


== Progress of the war ==


=== Opening hostilities ===


==== Confusion among the Central Powers ====
Germany promised to support Austria-Hungary's invasion of Serbia, but interpretations of what this meant differed. Previously tested deployment plans had been replaced early in 1914, but those had never been tested in exercises. Austro-Hungarian leaders believed Germany would cover its northern flank against Russia.


==== Serbian campaign ====

Beginning on 12 August, the Austrians and Serbs clashed at the battles of the Cer and Kolubara; over the next two weeks, Austrian attacks were repulsed with heavy losses. As a result, Austria had to keep sizeable forces on the Serbian front, weakening their efforts against Russia. Serbia's victory against Austria-Hungary in the 1914 invasion has been called one of the major upset victories of the twentieth century. In 1915, the campaign saw the first use of anti-aircraft warfare after an Austrian plane was shot down with ground-to-air fire, as well as the first medical evacuation by the Serbian army.


==== German offensive in Belgium and France ====

Upon mobilisation, in accordance with the Schlieffen Plan, 80% of the German Army was located on the Western Front, with the remainder acting as a screening force in the East. Rather than a direct attack across their shared frontier, the German right wing would sweep through the Netherlands and Belgium, then swing south, encircling Paris and trapping the French army against the Swiss border. The plan's creator, Alfred von Schlieffen, head of the German General Staff from 1891 to 1906, estimated that this would take six weeks, after which the German army would transfer to the East and defeat the Russians.
The plan was substantially modified by his successor, Helmuth von Moltke the Younger. Under Schlieffen, 85% of German forces in the west were assigned to the right wing, with the remainder holding along the frontier. By keeping his left-wing deliberately weak, he hoped to lure the French into an offensive into the ""lost provinces"" of Alsace-Lorraine, which was the strategy envisaged by their Plan XVII. However, Moltke grew concerned that the French might push too hard on his left flank and as the German Army increased in size from 1908 to 1914, he changed the allocation of forces between the two wings to 70:30. He also considered Dutch neutrality essential for German trade and cancelled the incursion into the Netherlands, which meant any delays in Belgium threatened the viability of the plan. Historian Richard Holmes argues that these changes meant the right wing was not strong enough to achieve decisive success.

The initial German advance in the West was very successful. By the end of August, the Allied left, which included the British Expeditionary Force (BEF), was in full retreat, and the French offensive in Alsace-Lorraine was a disastrous failure, with casualties exceeding 260,000. German planning provided broad strategic instructions while allowing army commanders considerable freedom in carrying them out at the front, but von Kluck used this freedom to disobey orders, opening a gap between the German armies as they closed on Paris. The French army, reinforced by the British expeditionary corps, seized this opportunity to counter-attack and pushed the German army 40 to 80 km back. Both armies were then so exhausted that no decisive move could be implemented, so they settled in trenches, with the vain hope of breaking through as soon as they could build local superiority.
In 1911, the Russian Stavka agreed with the French to attack Germany within fifteen days of mobilisation, ten days before the Germans had anticipated, although it meant the two Russian armies that entered East Prussia on 17 August did so without many of their support elements.
By the end of 1914, German troops held strong defensive positions inside France, controlled the bulk of France's domestic coalfields, and inflicted 230,000 more casualties than it lost itself. However, communications problems and questionable command decisions cost Germany the chance of a decisive outcome, while it had failed to achieve the primary objective of avoiding a long, two-front war. As was apparent to several German leaders, this amounted to a strategic defeat; shortly after the First Battle of the Marne, Crown Prince Wilhelm told an American reporter ""We have lost the war. It will go on for a long time but lost it is already.""


==== Asia and the Pacific ====

On 30 August 1914, New Zealand occupied German Samoa (now Samoa). On 11 September, the Australian Naval and Military Expeditionary Force landed on the island of New Britain, then part of German New Guinea. On 28 October, the German cruiser SMS Emden sank the Russian cruiser Zhemchug in the Battle of Penang. Japan declared war on Germany before seizing territories in the Pacific, which later became the South Seas Mandate, as well as German Treaty ports on the Chinese Shandong peninsula at Tsingtao. After Vienna refused to withdraw its cruiser SMS Kaiserin Elisabeth from Tsingtao, Japan declared war on Austria-Hungary, and the ship was sunk in November 1914. Within a few months, Allied forces had seized all German territories in the Pacific, leaving only isolated commerce raiders and a few holdouts in New Guinea.


==== African campaigns ====

Some of the first clashes of the war involved British, French, and German colonial forces in Africa. On 6–7 August, French and British troops invaded the German protectorates of Togoland and Kamerun. On 10 August, German forces in South-West Africa attacked South Africa; sporadic and fierce fighting continued for the rest of the war. The German colonial forces in German East Africa, led by Colonel Paul von Lettow-Vorbeck, fought a guerrilla warfare campaign and only surrendered two weeks after the armistice took effect in Europe.


==== Indian support for the Allies ====

Before the war, Germany had attempted to use Indian nationalism and pan-Islamism to its advantage, a policy continued post-1914 by instigating uprisings in India, while the Niedermayer–Hentig Expedition urged Afghanistan to join the war on the side of Central Powers. However, contrary to British fears of a revolt in India, the outbreak of the war saw a reduction in nationalist activity. Leaders from the Indian National Congress and other groups believed support for the British war effort would hasten Indian Home Rule, a promise allegedly made explicit in 1917 by Edwin Montagu, the Secretary of State for India.
In 1914, the British Indian Army was larger than the British Army itself, and between 1914 and 1918 an estimated 1.3 million Indian soldiers and labourers served in Europe, Africa, and the Middle East. In all, 140,000 soldiers served on the Western Front and nearly 700,000 in the Middle East, with 47,746 killed and 65,126 wounded. The suffering engendered by the war, as well as the failure of the British government to grant self-government to India afterward, bred disillusionment, resulting in the campaign for full independence led by Mahatma Gandhi.


=== Western Front 1914 to 1916 ===


==== Trench warfare begins ====

Pre-war military tactics that had emphasised open warfare and individual riflemen proved obsolete when confronted with conditions prevailing in 1914. Technological advances allowed the creation of strong defensive systems largely impervious to massed infantry advances, such as barbed wire, machine guns and above all far more powerful artillery, which dominated the battlefield and made crossing open ground extremely difficult. Both sides struggled to develop tactics for breaching entrenched positions without heavy casualties. In time, technology enabled the production of new offensive weapons, such as gas warfare and the tank.
After the First Battle of the Marne in September 1914, Allied and German forces unsuccessfully tried to outflank each other, a series of manoeuvres later known as the ""Race to the Sea"". By the end of 1914, the opposing forces confronted each other along an uninterrupted line of entrenched positions from the Channel to the Swiss border. Since the Germans were normally able to choose where to stand, they generally held the high ground, while their trenches tended to be better built; those constructed by the French and English were initially considered ""temporary"", only needed until an offensive would destroy the German defences. Both sides tried to break the stalemate using scientific and technological advances. On 22 April 1915, at the Second Battle of Ypres, the Germans (violating the Hague Convention) used chlorine gas for the first time on the Western Front. Several types of gas soon became widely used by both sides and though it never proved a decisive, battle-winning weapon, it became one of the most feared and best-remembered horrors of the war.


==== Continuation of trench warfare ====

In February 1916, the Germans attacked French defensive positions at the Battle of Verdun, lasting until December 1916. Casualties were greater for the French, but the Germans bled heavily as well, with anywhere from 700,000 to 975,000 casualties between the two combatants. Verdun became a symbol of French determination and self-sacrifice.
The Battle of the Somme was an Anglo-French offensive from July to November 1916. The opening day on 1 July 1916 was the bloodiest single day in the history of the British Army, which suffered 57,500 casualties, including 19,200 dead. As a whole, the Somme offensive led to an estimated 420,000 British casualties, along with 200,000 French and 500,000 Germans. The diseases that emerged in the trenches were a major killer on both sides. The living conditions led to disease and infection, such as trench foot, lice, typhus, trench fever, and the 'Spanish flu'.


=== Naval war ===

At the start of the war, German cruisers were scattered across the globe, some of which were subsequently used to attack Allied merchant shipping. These were systematically hunted down by the Royal Navy, though not before causing considerable damage. One of the most successful was the SMS Emden, part of the German East Asia Squadron stationed at Qingdao, which seized or sank 15 merchantmen, a Russian cruiser and a French destroyer. Most of the squadron was returning to Germany when it sank two British armoured cruisers at the Battle of Coronel in November 1914, before being virtually destroyed at the Battle of the Falkland Islands in December. The SMS Dresden escaped with a few auxiliaries, but after the Battle of Más a Tierra, these too were either destroyed or interned.
Soon after the outbreak of hostilities, Britain began a naval blockade of Germany. This proved effective in cutting off vital supplies, though it violated accepted international law. Britain also mined international waters which closed off entire sections of the ocean, even to neutral ships. Since there was limited response to this tactic, Germany expected a similar response to its unrestricted submarine warfare.
The Battle of Jutland in May/June 1916 was the only full-scale clash of battleships during the war, and one of the largest in history. The clash was indecisive, though the Germans inflicted more damage than they received; thereafter the bulk of the German High Seas Fleet was confined to port.

German U-boats attempted to cut the supply lines between North America and Britain. The nature of submarine warfare meant that attacks often came without warning, giving the crews of the merchant ships little hope of survival. The United States launched a protest, and Germany changed its rules of engagement. After the sinking of the passenger ship RMS Lusitania in 1915, Germany promised not to target passenger liners, while Britain armed its merchant ships, placing them beyond the protection of the ""cruiser rules"", which demanded warning and movement of crews to ""a place of safety"" (a standard that lifeboats did not meet). Finally, in early 1917, Germany adopted a policy of unrestricted submarine warfare, realising the Americans would eventually enter the war. Germany sought to strangle Allied sea lanes before the United States could transport a large army overseas, but, after initial successes, eventually failed to do so.
The U-boat threat lessened in 1917, when merchant ships began travelling in convoys, escorted by destroyers. This tactic made it difficult for U-boats to find targets, which significantly lessened losses; after the hydrophone and depth charges were introduced, destroyers could potentially successfully attack a submerged submarine. Convoys slowed the flow of supplies since ships had to wait as convoys were assembled; the solution was an extensive program of building new freighters. Troopships were too fast for the submarines and did not travel the North Atlantic in convoys. The U-boats sunk more than 5,000 Allied ships, at the cost of 199 submarines.
World War I also saw the first use of aircraft carriers in combat, with HMS Furious launching Sopwith Camels in a successful raid against the Zeppelin hangars at Tondern in July 1918, as well as blimps for antisubmarine patrol.


=== Southern theatres ===


==== War in the Balkans ====

Faced with Russia in the east, Austria-Hungary could spare only one-third of its army to attack Serbia. After suffering heavy losses, the Austrians briefly occupied the Serbian capital, Belgrade. A Serbian counter-attack in the Battle of Kolubara succeeded in driving them from the country by the end of 1914. For the first 10 months of 1915, Austria-Hungary used most of its military reserves to fight Italy. German and Austro-Hungarian diplomats scored a coup by persuading Bulgaria to join the attack on Serbia. The Austro-Hungarian provinces of Slovenia, Croatia and Bosnia provided troops for Austria-Hungary. Montenegro allied itself with Serbia.

Bulgaria declared war on Serbia on 14 October 1915 and joined in the attack by the Austro-Hungarian army under Mackensen's army of 250,000 that was already underway. Serbia was conquered in a little more than a month, as the Central Powers, now including Bulgaria, sent in 600,000 troops in total. The Serbian army, fighting on two fronts and facing certain defeat, retreated into northern Albania. The Serbs suffered defeat in the Battle of Kosovo. Montenegro covered the Serbian retreat toward the Adriatic coast in the Battle of Mojkovac on 6–7 January 1916, but ultimately the Austrians also conquered Montenegro. The surviving Serbian soldiers were evacuated to Greece. After the conquest, Serbia was divided between Austro-Hungary and Bulgaria.
In late 1915, a Franco-British force landed at Salonica in Greece to offer assistance and to pressure its government to declare war against the Central Powers. However, the pro-German King Constantine I dismissed the pro-Allied government of Eleftherios Venizelos before the Allied expeditionary force arrived.
The Macedonian front was at first mostly static. French and Serbian forces retook limited areas of Macedonia by recapturing Bitola on 19 November 1916 following the costly Monastir offensive, which brought stabilisation of the front.

Serbian and French troops finally made a breakthrough in September 1918 in the Vardar offensive, after most German and Austro-Hungarian troops had been withdrawn. The Bulgarians were defeated at the Battle of Dobro Pole, and by 25 September British and French troops had crossed the border into Bulgaria proper as the Bulgarian army collapsed. Bulgaria capitulated four days later, on 29 September 1918. The German high command responded by despatching troops to hold the line, but these forces were too weak to re-establish a front.
The disappearance of the Macedonian front meant that the road to Budapest and Vienna was now opened to Allied forces. Hindenburg and Ludendorff concluded that the strategic and operational balance had now shifted decidedly against the Central Powers and, a day after the Bulgarian collapse, insisted on an immediate peace settlement.


==== Ottoman Empire ====

The Ottomans threatened Russia's Caucasian territories and Britain's communications with India via the Suez Canal. The Ottoman Empire took advantage of the European powers' preoccupation with the war and conducted large-scale ethnic cleansing of the Armenian, Greek, and Assyrian Christian populations—the Armenian genocide, Greek genocide, and Sayfo respectively.
The British and French opened overseas fronts with the Gallipoli (1915) and Mesopotamian campaigns (1914). In Gallipoli, the Ottoman Empire successfully repelled the British, French, and Australian and New Zealand Army Corps (ANZACs). In Mesopotamia, by contrast, after the defeat of the British defenders in the siege of Kut by the Ottomans (1915–1916), British Imperial forces reorganised and captured Baghdad in March 1917. The British were aided in Mesopotamia by local Arab and Assyrian fighters, while the Ottomans employed local Kurdish and Turcoman tribes.
The Suez Canal was defended from Ottoman attacks in 1915 and 1916; in August 1916, a German and Ottoman force was defeated at the Battle of Romani by the ANZAC Mounted Division and the 52nd (Lowland) Infantry Division. Following this victory, an Egyptian Expeditionary Force advanced across the Sinai Peninsula, pushing Ottoman forces back in the Battle of Magdhaba in December and the Battle of Rafa on the border between the Egyptian Sinai and Ottoman Palestine in January 1917.

Russian armies generally had success in the Caucasus campaign. Enver Pasha, supreme commander of the Ottoman armed forces, dreamed of re-conquering central Asia and areas that had been previously lost to Russia. He was, however, a poor commander. He launched an offensive against the Russians in the Caucasus in December 1914 with 100,000 troops, insisting on a frontal attack against mountainous Russian positions in winter. He lost 86% of his force at the Battle of Sarikamish. General Yudenich, the Russian commander from 1915 to 1916, drove the Turks out of most of the southern Caucasus.

The Ottoman Empire, with German support, invaded Persia (modern Iran) in December 1914 to cut off British and Russian access to petroleum reservoirs around Baku. Persia, ostensibly neutral, had long been under British and Russian influence. The Ottomans and Germans were aided by Kurdish and Azeri forces, together with a large number of major Iranian tribes, while the Russians and British had the support of Armenian and Assyrian forces. The Persian campaign lasted until 1918 and ended in failure for the Ottomans and their allies. However, the Russian withdrawal from the war in 1917 led Armenian and Assyrian forces to be cut off from supply lines, outnumbered, outgunned and isolated, forcing them to fight and flee towards British lines in northern Mesopotamia.
The Arab Revolt, instigated by the British Foreign Office, started in June 1916 with the Battle of Mecca, led by Sharif Hussein. The Sharif declared the independence of the Kingdom of Hejaz and, with British assistance, conquered much of Ottoman-held Arabia, resulting finally in the Ottoman surrender of Damascus. Fakhri Pasha, the Ottoman commander of Medina, resisted for more than 2+1⁄2 years during the siege of Medina before surrendering in January 1919.
The Senussi tribe, along the border of Italian Libya and British Egypt, incited and armed by the Turks, waged a small-scale guerrilla war against Allied troops. The British were forced to dispatch 12,000 troops to oppose them in the Senussi campaign. Their rebellion was finally crushed in mid-1916.
Total Allied casualties on the Ottoman fronts amounted to 650,000 men. Total Ottoman casualties were 725,000, with 325,000 dead and 400,000 wounded.


==== Italian Front ====

Though Italy joined the Triple Alliance in 1882, a treaty with its traditional Austrian enemy was so controversial that subsequent governments denied its existence and the terms were only made public in 1915. This arose from nationalist designs on Austro-Hungarian territory in Trentino, the Austrian Littoral, Rijeka and Dalmatia, considered vital to secure the borders established in 1866. In 1902, Rome secretly had agreed with France to remain neutral if the latter was attacked by Germany, effectively nullifying its role in the Triple Alliance.

When the war began in 1914, Italy argued the Triple Alliance was defensive and it was not obliged to support an Austrian attack on Serbia. Opposition to joining the Central Powers increased when Turkey became a member in September, since in 1911 Italy had occupied Ottoman possessions in Libya and the Dodecanese islands. To secure Italian neutrality, the Central Powers offered them Tunisia, while in return for an immediate entry into the war, the Allies agreed to their demands for Austrian territory and sovereignty over the Dodecanese. Although they remained secret, these provisions were incorporated into the April 1915 Treaty of London; Italy joined the Triple Entente and, on 23 May, declared war on Austria-Hungary, followed by Germany fifteen months later.
The pre-1914 Italian army was short of officers, trained men, adequate transport and modern weapons; by April 1915, some of these deficiencies had been remedied but it was still unprepared for the major offensive required by the Treaty of London. The advantage of superior numbers was offset by the difficult terrain; much of the fighting took place high in the Alps and Dolomites, where trench lines had to be cut through rock and ice and keeping troops supplied was a major challenge. These issues were exacerbated by unimaginative strategies and tactics. Between 1915 and 1917, the Italian commander, Luigi Cadorna, undertook a series of frontal assaults along the Isonzo, which made little progress and cost many lives; by the end of the war, Italian combat deaths totalled around 548,000.
In the spring of 1916, the Austro-Hungarians counterattacked in Asiago in the Strafexpedition, but made little progress and were pushed by the Italians back to Tyrol. Although Italy occupied southern Albania in May 1916, their main focus was the Isonzo front which, after the capture of Gorizia in August 1916, remained static until October 1917. After a combined Austro-German force won a major victory at Caporetto, Cadorna was replaced by Armando Diaz who retreated more than 100 kilometres (62 mi) before holding positions along the Piave River. A second Austrian offensive was repulsed in June 1918. On 24 October, Diaz launched the Battle of Vittorio Veneto and initially met stubborn resistance, but with Austria-Hungary collapsing, Hungarian divisions in Italy demanded they be sent home. When this was granted, many others followed and the Imperial army disintegrated, the Italians taking over 300,000 prisoners. On 3 November, the Armistice of Villa Giusti ended hostilities between Austria-Hungary and Italy which occupied Trieste and areas along the Adriatic Sea awarded to it in 1915.


=== Eastern Front ===


==== Initial actions ====

As previously agreed with French president Raymond Poincaré, Russian plans at the start of the war were to simultaneously advance into Austrian Galicia and East Prussia as soon as possible. Although their attack on Galicia was largely successful, and the invasions achieved their aim of forcing Germany to divert troops from the Western Front, the speed of mobilisation meant they did so without much of their heavy equipment and support functions. These weaknesses contributed to Russian defeats at Tannenberg and the Masurian Lakes in August and September 1914, forcing them to withdraw from East Prussia with heavy losses. By spring 1915, they had also retreated from Galicia, and the May 1915 Gorlice–Tarnów offensive allowed the Central Powers to invade Russian-occupied Poland.
Despite the successful June 1916 Brusilov offensive against the Austrians in eastern Galicia, shortages of supplies, heavy losses and command failures prevented the Russians from fully exploiting their victory. However, it was one of the most significant offensives of the war, diverting German resources from Verdun, relieving Austro-Hungarian pressure on the Italians, and convincing Romania to enter the war on the side of the Allies on 27 August. It also fatally weakened both the Austrian and Russian armies, whose offensive capabilities were badly affected by their losses and increased disillusion with the war that ultimately led to the Russian revolutions.
Meanwhile, unrest grew in Russia as the Tsar remained at the front, with the home front controlled by Empress Alexandra. Her increasingly incompetent rule and food shortages in urban areas led to widespread protests and the murder of her favourite, Grigori Rasputin, at the end of 1916.


==== Romanian participation ====

Despite secretly agreeing to support the Triple Alliance in 1883, Romania increasingly found itself at odds with the Central Powers over their support for Bulgaria in the Balkan Wars and the status of ethnic Romanian communities in Hungarian-controlled Transylvania, which comprised an estimated 2.8 million of the 5.0 million population. With the ruling elite split into pro-German and pro-Entente factions, Romania remained neutral for two years while allowing Germany and Austria to transport military supplies and advisors across Romanian territory.
In September 1914, Russia acknowledged Romanian rights to Austro-Hungarian territories including Transylvania and Banat, whose acquisition had widespread popular support, and Russian success against Austria led Romania to join the Entente in the August 1916 Treaty of Bucharest. Under the strategic plan known as Hypothesis Z, the Romanian army planned an offensive into Transylvania, while defending Southern Dobruja and Giurgiu against a possible Bulgarian counterattack. On 27 August 1916, they attacked Transylvania and occupied substantial parts of the province before being driven back by the recently formed German 9th Army, led by former Chief of Staff Erich von Falkenhayn. A combined German-Bulgarian-Turkish offensive captured Dobruja and Giurgiu, although the bulk of the Romanian army managed to escape encirclement and retreated to Bucharest, which surrendered to the Central Powers on 6 December 1916.
In the summer of 1917, a Central Powers offensive began in Romania under the command of August von Mackensen to knock Romania out of the war, resulting in the battles of Oituz, Mărăști and Mărășești where up to 1,000,000 Central Powers troops were present. The battles lasted from 22 July to 3 September and eventually, the Romanian army was victorious advancing 500 km2. August von Mackensen could not plan for another offensive as he had to transfer troops to the Italian Front. Following the Russian revolution, Romania found itself alone on the Eastern Front and signed the Treaty of Bucharest with the Central Powers, which recognised Romanian sovereignty over Bessarabia in return for ceding control of passes in the Carpathian Mountains to Austria-Hungary and leasing its oil wells to Germany. Although approved by Parliament, King Ferdinand I refused to sign it, hoping for an Allied victory in the west. Romania re-entered the war on 10 November 1918 on the side of the Allies and the Treaty of Bucharest was formally annulled by the Armistice of 11 November 1918.


=== Central Powers peace overtures ===
On 12 December 1916, after ten brutal months of the Battle of Verdun and a successful offensive against Romania, Germany attempted to negotiate a peace with the Allies. However, this attempt was rejected out of hand as a ""duplicitous war ruse"".

US president Woodrow Wilson attempted to intervene as a peacemaker, asking for both sides to state their demands. Lloyd George's War Cabinet considered the German offer to be a ploy to create divisions among the Allies. After initial outrage and much deliberation, they took Wilson's note as a separate effort, signalling that the US was on the verge of entering the war against Germany following the ""submarine outrages"". While the Allies debated a response to Wilson's offer, the Germans chose to rebuff it in favour of ""a direct exchange of views"". Learning of the German response, the Allied governments were free to make clear demands in their response of 14 January. They sought restoration of damages, the evacuation of occupied territories, reparations for France, Russia and Romania, and a recognition of the principle of nationalities. The Allies sought guarantees that would prevent or limit future wars. The negotiations failed and the Entente powers rejected the German offer on the grounds of honour, and noted Germany had not put forward any specific proposals.


=== Final years of the war ===


==== Russian Revolution and withdrawal ====

By the end of 1916, Russian casualties totalled nearly five million killed, wounded or captured, with major urban areas affected by food shortages and high prices. In March 1917, Tsar Nicholas ordered the military to forcibly suppress strikes in Petrograd but the troops refused to fire on the crowds. Revolutionaries set up the Petrograd Soviet and fearing a left-wing takeover, the State Duma forced Nicholas to abdicate and established the Russian Provisional Government, which confirmed Russia's willingness to continue the war. However, the Petrograd Soviet refused to disband, creating competing power centres and causing confusion and chaos, with frontline soldiers becoming increasingly demoralised.
Following the Tsar's abdication, Vladimir Lenin—with the help of the German government—was ushered from Switzerland into Russia on 16 April 1917. Discontent and the weaknesses of the Provisional Government led to a rise in the popularity of the Bolshevik Party, led by Lenin, which demanded an immediate end to the war. The Revolution of November was followed in December by an armistice and negotiations with Germany. At first, the Bolsheviks refused the German terms, but when German troops began marching across Ukraine unopposed, they acceded to the Treaty of Brest-Litovsk on 3 March 1918. The treaty ceded vast territories, including Finland, Estonia, Latvia, Lithuania, and parts of Poland and Ukraine to the Central Powers.
With the Russian Empire out of the war, Romania found itself alone on the Eastern Front and signed the Treaty of Bucharest with the Central Powers in May 1918. Under the terms of the treaty, Romania ceded territory to Austria-Hungary and Bulgaria and leased its oil reserves to Germany. However, the terms also included the Central Powers' recognition of the union of Bessarabia with Romania.


==== United States enters the war ====

The United States was a major supplier of war material to the Allies but remained neutral in 1914, in large part due to domestic opposition. The most significant factor in creating the support Wilson needed was the German submarine offensive, which not only cost American lives but paralysed trade as ships were reluctant to put to sea.
On 6 April 1917, Congress declared war on Germany as an ""Associated Power"" of the Allies. The US Navy sent a battleship group to Scapa Flow to join the Grand Fleet, and provided convoy escorts. In April 1917, the US Army had fewer than 300,000 men, including National Guard units, compared to British and French armies of 4.1 and 8.3 million respectively. The Selective Service Act of 1917 drafted 2.8 million men, though training and equipping such numbers was a huge logistical challenge. By June 1918, over 667,000 members of the American Expeditionary Forces (AEF) were transported to France, a figure which reached 2 million by the end of November.
Despite his conviction that Germany must be defeated, Wilson went to war to ensure the US played a leading role in shaping the peace, which meant preserving the AEF as a separate military force, rather than being absorbed into British or French units as his Allies wanted. He was strongly supported by AEF commander General John J. Pershing, a proponent of pre-1914 ""open warfare"" who considered the French and British emphasis on artillery misguided and incompatible with American ""offensive spirit"". Much to the frustration of his Allies, who had suffered heavy losses in 1917, he insisted on retaining control of American troops, and refused to commit them to the front line until able to operate as independent units. As a result, the first significant US involvement was the Meuse–Argonne offensive in late September 1918.


==== Nivelle Offensive (April–May 1917) ====

In December 1916, Robert Nivelle replaced Pétain as commander of French armies on the Western Front and began planning a spring attack in Champagne, part of a joint Franco-British operation. Poor security meant German intelligence was well informed on tactics and timetables, but despite this, when the attack began on 16 April the French made substantial gains, before being brought to a halt by the newly built and extremely strong defences of the Hindenburg Line. Nivelle persisted with frontal assaults and, by 25 April, the French had suffered nearly 135,000 casualties, including 30,000 dead, most incurred in the first two days.
Concurrent British attacks at Arras were more successful, though ultimately of little strategic value. Operating as a separate unit for the first time, the Canadian Corps' capture of Vimy Ridge is viewed by many Canadians as a defining moment in creating a sense of national identity. Though Nivelle continued the offensive, on 3 May the 21st Division, which had been involved in some of the heaviest fighting at Verdun, refused orders to go into battle, initiating the French Army mutinies; within days, ""collective indiscipline"" had spread to 54 divisions, while over 20,000 deserted.


==== Sinai and Palestine campaign (1917–1918) ====

In March and April 1917, at the First and Second Battles of Gaza, German and Ottoman forces stopped the advance of the Egyptian Expeditionary Force, which had begun in August 1916 at the Battle of Romani. At the end of October 1917, the Sinai and Palestine campaign resumed, when General Edmund Allenby's XXth Corps, XXI Corps and Desert Mounted Corps won the Battle of Beersheba. Two Ottoman armies were defeated a few weeks later at the Battle of Mughar Ridge and, early in December, Jerusalem had been captured following another Ottoman defeat at the Battle of Jerusalem. About this time, Friedrich Freiherr Kress von Kressenstein was relieved of his duties as the Eighth Army's commander, replaced by Djevad Pasha, and a few months later the commander of the Ottoman Army in Palestine, Erich von Falkenhayn, was replaced by Otto Liman von Sanders.
In early 1918, the front line was extended and the Jordan Valley was occupied, following the First Transjordan and the Second Transjordan attacks by British Empire forces in March and April 1918.


==== German offensive and Allied counter-offensive (March–November 1918) ====

In December 1917, the Central Powers signed an armistice with Russia, thus freeing large numbers of German troops for use in the West. With German reinforcements and new American troops pouring in, the outcome was to be decided on the Western Front. The Central Powers knew that they could not win a protracted war, but they held high hopes for success in a final quick offensive. Ludendorff drew up plans (Operation Michael) for the 1918 offensive on the Western Front. The operation commenced on 21 March 1918, with an attack on British forces near Saint-Quentin. German forces achieved an unprecedented advance of 60 kilometres (37 mi). The initial offensive was a success; after heavy fighting, however, the offensive was halted. Lacking tanks or motorised artillery, the Germans were unable to consolidate their gains. The problems of re-supply were also exacerbated by increasing distances that now stretched over terrain that was shell-torn and often impassable to traffic.
Germany launched Operation Georgette against the northern English Channel ports. The Allies halted the drive after limited territorial gains by Germany. The German Army to the south then conducted Operations Blücher and Yorck, pushing broadly towards Paris. Germany launched Operation Marne (Second Battle of the Marne) on 15 July, in an attempt to encircle Reims. The resulting counter-attack, which started the Hundred Days Offensive on 8 August, led to a marked collapse in German morale.


==== Allied advance to the Hindenburg Line ====

By September, the Germans had fallen back to the Hindenburg Line. The Allies had advanced to the Hindenburg Line in the north and centre. German forces launched numerous counterattacks, but positions and outposts of the Line continued falling, with the BEF alone taking 30,441 prisoners in the last week of September. On 24 September, the Supreme Army Command informed the leaders in Berlin that armistice talks were inevitable.
The final assault on the Hindenburg Line began with the Meuse-Argonne offensive, launched by American and French troops on 26 September. Two days later the Belgians, French and British attacked around Ypres, and the day after the British at St Quentin in the centre of the line. The following week, cooperating American and French units broke through in Champagne at the Battle of Blanc Mont Ridge (3–27 October), forcing the Germans off the commanding heights, and closing towards the Belgian frontier. On 8 October, the Hindenburg Line was pierced by British and Dominion troops of the First and Third British Armies at the Second Battle of Cambrai.


==== Breakthrough of Macedonian front (September 1918) ====

Allied forces started the Vardar offensive on 15 September at two key points: Dobro Pole and near Dojran Lake. In the Battle of Dobro Pole, the Serbian and French armies had success after a three-day-long battle with relatively small casualties, and subsequently made a breakthrough in the front, something which was rarely seen in World War I. After the front was broken, Allied forces started to liberate Serbia and reached Skopje at 29 September, after which Bulgaria signed an armistice with the Allies on 30 September.


=== Armistices and capitulations ===

The collapse of the Central Powers came swiftly. Bulgaria was the first to sign an armistice, the Armistice of Salonica on 29 September 1918. German Emperor Wilhelm II in a telegram to Bulgarian Tsar Ferdinand I described the situation thus: ""Disgraceful! 62,000 Serbs decided the war!"". On the same day, the German Supreme Army Command informed Kaiser Wilhelm II and the Imperial Chancellor Count Georg von Hertling, that the military situation facing Germany was hopeless.
On 24 October, the Italians began a push that rapidly recovered territory lost after the Battle of Caporetto. This culminated in the Battle of Vittorio Veneto, marking the end of the Austro-Hungarian Army as an effective fighting force. The offensive also triggered the disintegration of the Austro-Hungarian Empire. During the last week of October, declarations of independence were made in Budapest, Prague, and Zagreb. On 29 October, the imperial authorities asked Italy for an armistice, but the Italians continued advancing, reaching Trento, Udine, and Trieste. On 3 November, Austria-Hungary sent a flag of truce and accepted the Armistice of Villa Giusti, arranged with the Allied Authorities in Paris. Austria and Hungary signed separate armistices following the overthrow of the Habsburg monarchy. In the following days, the Italian Army occupied Innsbruck and all Tyrol, with over 20,000 soldiers.
On 30 October, the Ottoman Empire capitulated, and signed the Armistice of Mudros.


==== German government surrenders ====

With the military faltering and with widespread loss of confidence in the Kaiser leading to his abdication and fleeing of the country, Germany moved towards surrender. Prince Maximilian of Baden took charge on October 3 as Chancellor of Germany. Negotiations with President Wilson began immediately, in the hope that he would offer better terms than the British and French. Wilson demanded a constitutional monarchy and parliamentary control over the German military.
The German Revolution of 1918–1919 began at the end of October 1918. Units of the German Navy refused to set sail for a last, large-scale operation in a war they believed to be as good as lost. The sailors' revolt, which then ensued in the naval ports of Wilhelmshaven and Kiel, spread across the whole country within days and led to the proclamation of a republic on 9 November 1918, shortly thereafter to the abdication of Kaiser Wilhelm II, and German surrender.


== Aftermath ==

In the aftermath of the war, the German, Austro-Hungarian, Ottoman, and Russian empires disappeared. Numerous nations regained their former independence, and new ones were created. Four dynasties fell as a result of the war: the Romanovs, the Hohenzollerns, the Habsburgs, and the Ottomans. Belgium and Serbia were badly damaged, as was France, with 1.4 million soldiers dead, not counting other casualties. Germany and Russia were similarly affected.


=== Formal end of the war ===

A formal state of war between the two sides persisted for another seven months, until the signing of the Treaty of Versailles with Germany on 28 June 1919. The US Senate did not ratify the treaty despite public support for it, and did not formally end its involvement in the war until the Knox–Porter Resolution was signed on 2 July 1921 by President Warren G. Harding. For the British Empire, the state of war ceased under the provisions of the Termination of the Present War (Definition) Act 1918 concerning:

Germany on 10 January 1920.
Austria on 16 July 1920.
Bulgaria on 9 August 1920.
Hungary on 26 July 1921.
Turkey on 6 August 1924.

Some war memorials date the end of the war as being when the Versailles Treaty was signed in 1919, which was when many of the troops serving abroad finally returned home; by contrast, most commemorations of the war's end concentrate on the armistice of 11 November 1918.


=== Peace treaties and national boundaries ===

The Paris Peace Conference imposed a series of peace treaties on the Central Powers officially ending the war. The 1919 Treaty of Versailles dealt with Germany and, building on Wilson's 14th point, established the League of Nations on 28 June 1919.
The Central Powers had to acknowledge responsibility for ""all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by"" their aggression. In the Treaty of Versailles, this statement was Article 231. This article became known as the ""War Guilt Clause"", as the majority of Germans felt humiliated and resentful. The Germans felt they had been unjustly dealt with by what they called the ""diktat of Versailles"". German historian Hagen Schulze said the Treaty placed Germany ""under legal sanctions, deprived of military power, economically ruined, and politically humiliated."" Belgian historian Laurence Van Ypersele emphasises the central role played by memory of the war and the Versailles Treaty in German politics in the 1920s and 1930s:

Active denial of war guilt in Germany and German resentment at both reparations and continued Allied occupation of the Rhineland made widespread revision of the meaning and memory of the war problematic. The legend of the ""stab in the back"" and the wish to revise the ""Versailles diktat"", and the belief in an international threat aimed at the elimination of the German nation persisted at the heart of German politics. Even a man of peace such as [Gustav] Stresemann publicly rejected German guilt. As for the Nazis, they waved the banners of domestic treason and international conspiracy in an attempt to galvanise the German nation into a spirit of revenge. Like a Fascist Italy, Nazi Germany sought to redirect the memory of the war to the benefit of its policies.
Meanwhile, new nations liberated from German rule viewed the treaty as a recognition of wrongs committed against small nations by much larger aggressive neighbours.Austria-Hungary was partitioned into several successor states, largely but not entirely along ethnic lines. Apart from Austria and Hungary, Czechoslovakia, Italy, Poland, Romania and Yugoslavia received territories from the Dual Monarchy (the formerly separate and autonomous Kingdom of Croatia-Slavonia was incorporated into Yugoslavia). The details were contained in the treaties of Saint-Germain-en-Laye and Trianon. As a result, Hungary lost 64% of its total population, decreasing from 20.9 million to 7.6 million, and losing 31% (3.3 out of 10.7 million) of its ethnic Hungarians. According to the 1910 census, speakers of the Hungarian language included approximately 54% of the entire population of the Kingdom of Hungary. Within the country, numerous ethnic minorities were present: 16.1% Romanians, 10.5% Slovaks, 10.4% Germans, 2.5% Ruthenians, 2.5% Serbs and 8% others. Between 1920 and 1924, 354,000 Hungarians fled former Hungarian territories attached to Romania, Czechoslovakia, and Yugoslavia.
The Russian Empire lost much of its western frontier as the newly independent nations of Estonia, Finland, Latvia, Lithuania, and Poland were carved from it. Romania took control of Bessarabia in April 1918.


=== National identities ===

After 123 years, Poland re-emerged as an independent country. The Kingdom of Serbia and its dynasty, as a ""minor Entente nation"" and the country with the most casualties per capita, became the backbone of a new multinational state, the Kingdom of Serbs, Croats and Slovenes, later renamed Yugoslavia. Czechoslovakia, combining the Kingdom of Bohemia with parts of the Kingdom of Hungary, became a new nation. Romania would unite all Romanian-speaking people under a single state, leading to Greater Romania.
In Australia and New Zealand, the Battle of Gallipoli became known as those nations' ""Baptism of Fire"". It was the first major war in which the newly established countries fought, and it was one of the first times that Australian troops fought as Australians, not just subjects of the British Crown, and independent national identities for these nations took hold. Anzac Day, commemorating the Australian and New Zealand Army Corps (ANZAC), celebrates this defining moment.
In the aftermath of World War I, Greece fought against Turkish nationalists led by Mustafa Kemal, a war that eventually resulted in a massive population exchange between the two countries under the Treaty of Lausanne. According to various sources, several hundred thousand Greeks died during this period, which was tied in with the Greek genocide.


== Casualties ==

Of the 60 million European military personnel who were mobilised from 1914 to 1918, an estimated 8 million were killed, 7 million were permanently disabled, and 15 million were seriously injured. Germany lost 15.1% of its active male population, Austria-Hungary lost 17.1%, and France lost 10.5%. France mobilised 7.8 million men, of which 1.4 million died and 3.2 million were injured. Approximately 15,000 deployed men sustained gruesome facial injuries, causing social stigma and marginalisation; they were called the gueules cassées (broken faces). In Germany, civilian deaths were 474,000 higher than in peacetime, due in large part to food shortages and malnutrition that had weakened disease resistance. These excess deaths are estimated as 271,000 in 1918, plus another 71,000 in the first half of 1919 when the blockade was still in effect. Starvation caused by famine killed approximately 100,000 people in Lebanon.

Diseases flourished in the chaotic wartime conditions. In 1914 alone, louse-borne epidemic typhus killed 200,000 in Serbia. Starting in early 1918, a major influenza epidemic known as Spanish flu spread across the world, accelerated by the movement of large numbers of soldiers, often crammed together in camps and transport ships with poor sanitation. The Spanish flu killed at least 17 to 25 million people, including an estimated 2.64 million Europeans and as many as 675,000 Americans. Between 1915 and 1926, an epidemic of encephalitis lethargica affected nearly five million people worldwide.
Eight million equines mostly horses, donkeys and mules died, three-quarters of them from the extreme conditions they worked in.


=== War crimes ===


==== Chemical weapons in warfare ====

The German army was the first to successfully deploy chemical weapons during the Second Battle of Ypres (April–May 1915), after German scientists under the direction of Fritz Haber at the Kaiser Wilhelm Institute developed a method to weaponize chlorine. The use of chemical weapons had been sanctioned by the German High Command to force Allied soldiers out of their entrenched positions, complementing rather than supplanting more lethal conventional weapons. Chemical weapons were deployed by all major belligerents throughout the war, inflicting approximately 1.3 million casualties, of which about 90,000 were fatal. The use of chemical weapons in warfare was a direct violation of the 1899 Hague Declaration Concerning Asphyxiating Gases and the 1907 Hague Convention on Land Warfare, which prohibited their use.


==== Genocides by the Ottoman Empire ====

The ethnic cleansing of the Ottoman Empire's Armenian population, including mass deportations and executions, during the final years of the Ottoman Empire is considered genocide. The Ottomans carried out organised and systematic massacres of the Armenian population at the beginning of the war and manipulated acts of Armenian resistance by portraying them as rebellions to justify further extermination. In early 1915, several Armenians volunteered to join the Russian forces and the Ottoman government used this as a pretext to issue the Tehcir Law (Law on Deportation), which authorised the deportation of Armenians from the Empire's eastern provinces to Syria between 1915 and 1918. The Armenians were intentionally marched to death and a number were attacked by Ottoman brigands. While the exact number of deaths is unknown, the International Association of Genocide Scholars estimates around 1.5 million. The government of Turkey continues to deny the genocide to the present day, arguing that those who died were victims of inter-ethnic fighting, famine, or disease during World War I; these claims are rejected by most historians.
Other ethnic groups were similarly attacked by the Ottoman Empire during this period, including Assyrians and Greeks, and some scholars consider those events to be part of the same policy of extermination. At least 250,000 Assyrian Christians, about half of the population, and 350,000–750,000 Anatolian and Pontic Greeks were killed between 1915 and 1922.


=== Prisoners of war ===

About 8 million soldiers surrendered and were held in POW camps during the war. All nations pledged to follow the Hague Conventions on fair treatment of prisoners of war, and the survival rate for POWs was generally much higher than that of combatants at the front.
Around 25–31% of Russian losses (as a proportion of those captured, wounded, or killed) were to prisoner status; for Austria-Hungary 32%; for Italy 26%; for France 12%; for Germany 9%; for Britain 7%. Prisoners from the Allied armies totalled about 1.4 million (not including Russia, which lost 2.5–3.5 million soldiers as prisoners). From the Central Powers, about 3.3 million soldiers became prisoners; most of them surrendered to Russians.


== Soldiers' experiences ==
Allied personnel was around 42,928,000, while Central personnel was near 25,248,000. British soldiers of the war were initially volunteers but were increasingly conscripted. Surviving veterans returning home often found they could discuss their experiences only among themselves, so formed ""veterans' associations"" or ""Legions"".


=== Conscription ===

Conscription was common in most European countries. However, it was controversial in English-speaking countries, It was especially unpopular among minority ethnicities—especially the Irish Catholics in Ireland, Australia, and the French Catholics in Canada.
In the US, conscription began in 1917 and was generally well-received, with a few pockets of opposition in isolated rural areas. The administration decided to rely primarily on conscription, rather than voluntary enlistment, to raise military manpower after only 73,000 volunteers enlisted out of the initial 1 million target in the first six weeks of war.


=== Military attachés and war correspondents ===

Military and civilian observers from every major power closely followed the course of the war. Many were able to report on events from a perspective somewhat akin to modern ""embedded"" positions within the opposing land and naval forces.


== Economic effects ==

Macro- and micro-economic consequences devolved from the war. Families were altered by the departure of many men. With the death or absence of the primary wage earner, women were forced into the workforce in unprecedented numbers. At the same time, the industry needed to replace the lost labourers sent to war. This aided the struggle for voting rights for women.
In all nations, the government's share of GDP increased, surpassing 50% in both Germany and France and nearly reaching that level in Britain. To pay for purchases in the US, Britain cashed in its extensive investments in American railroads and then began borrowing heavily from Wall Street. President Wilson was on the verge of cutting off the loans in late 1916 but allowed a great increase in US government lending to the Allies. After 1919, the US demanded repayment of these loans. The repayments were, in part, funded by German reparations that, in turn, were supported by American loans to Germany. This circular system collapsed in 1931 and some loans were never repaid. Britain still owed the United States $4.4 billion of World War I debt in 1934; the last installment was finally paid in 2015.
Britain turned to her colonies for help in obtaining essential war materials whose supply from traditional sources had become difficult. Geologists such as Albert Kitson were called on to find new resources of precious minerals in the African colonies. Kitson discovered important new deposits of manganese, used in munitions production, in the Gold Coast.
Article 231 of the Treaty of Versailles (the so-called ""war guilt"" clause) stated Germany accepted responsibility for ""all the loss and damage to which the Allied and Associated Governments and their nationals have been subjected as a consequence of the war imposed upon them by the aggression of Germany and her allies."" It was worded as such to lay a legal basis for reparations, and a similar clause was inserted in the treaties with Austria and Hungary. However, neither of them interpreted it as an admission of war guilt. In 1921, the total reparation sum was placed at 132 billion gold marks. However, ""Allied experts knew that Germany could not pay"" this sum. The total sum was divided into three categories, with the third being ""deliberately designed to be chimerical"" and its ""primary function was to mislead public opinion ... into believing the 'total sum was being maintained.'"" Thus, 50 billion gold marks (12.5 billion dollars) ""represented the actual Allied assessment of German capacity to pay"" and ""therefore ... represented the total German reparations"" figure that had to be paid.
This figure could be paid in cash or in-kind (coal, timber, chemical dyes, etc.). Some of the territory lost—via the Treaty of Versailles—was credited towards the reparation figure as were other acts such as helping to restore the Library of Louvain. By 1929, the Great Depression caused political chaos throughout the world. In 1932 the payment of reparations was suspended by the international community, by which point Germany had paid only the equivalent of 20.598 billion gold marks. With the rise of Adolf Hitler, all bonds and loans that had been issued and taken out during the 1920s and early 1930s were cancelled. David Andelman notes ""Refusing to pay doesn't make an agreement null and void. The bonds, the agreement, still exist."" Thus, following the Second World War, at the London Conference in 1953, Germany agreed to resume payment on the money borrowed. On 3 October 2010, Germany made the final payment on these bonds.
The Australian prime minister, Billy Hughes, wrote to the British prime minister, David Lloyd George, ""You have assured us that you cannot get better terms. I much regret it, and hope even now that some way may be found of securing agreement for demanding reparation commensurate with the tremendous sacrifices made by the British Empire and her Allies."" Australia received £5,571,720 in war reparations, but the direct cost of the war to Australia had been £376,993,052, and, by the mid-1930s, repatriation pensions, war gratuities, interest and sinking fund charges were £831,280,947.


== Support and opposition for the war ==


=== Support ===

In the Balkans, Yugoslav nationalists such as the leader, Ante Trumbić, strongly supported the war, desiring the freedom of Yugoslavs from Austria-Hungary and other foreign powers and the creation of an independent Yugoslavia. The Yugoslav Committee, led by Trumbić, was formed in Paris on 30 April 1915 but shortly moved its office to London. In April 1918, the Rome Congress of Oppressed Nationalities met, including Czechoslovak, Italian, Polish, Transylvanian, and Yugoslav representatives who urged the Allies to support national self-determination for the peoples residing within Austria-Hungary.
In the Middle East, Arab nationalism soared in Ottoman territories in response to the rise of Turkish nationalism during the war, with Arab nationalist leaders advocating the creation of a pan-Arab state. In 1916, the Arab Revolt began in Ottoman-controlled territories of the Middle East to achieve independence.
In East Africa, Iyasu V of Ethiopia was supporting the Dervish state who were at war with the British in the Somaliland campaign. Von Syburg, the German envoy in Addis Ababa, said, ""now the time has come for Ethiopia to regain the coast of the Red Sea driving the Italians home, to restore the Empire to its ancient size."" The Ethiopian Empire was on the verge of entering World War I on the side of the Central Powers before Iyasu's overthrow at the Battle of Segale due to Allied pressure on the Ethiopian aristocracy.

Several socialist parties initially supported the war when it began in August 1914. But European socialists split on national lines, with the concept of class conflict held by radical socialists such as Marxists and syndicalists being overborne by their patriotic support for the war. Once the war began, Austrian, British, French, German, and Russian socialists followed the rising nationalist current by supporting their countries' intervention in the war.
Italian nationalism was stirred by the outbreak of the war and was initially strongly supported by a variety of political factions. One of the most prominent and popular Italian nationalist supporters of the war was Gabriele D'Annunzio, who promoted Italian irredentism and helped sway the Italian public to support intervention in the war. The Italian Liberal Party, under the leadership of Paolo Boselli, promoted intervention in the war on the side of the Allies and used the Dante Alighieri Society to promote Italian nationalism. Italian socialists were divided on whether to support the war or oppose it; some were militant supporters of the war, including Benito Mussolini and Leonida Bissolati. However, the Italian Socialist Party decided to oppose the war after anti-militarist protestors were killed, resulting in a general strike called Red Week. The Italian Socialist Party purged itself of pro-war nationalist members, including Mussolini. Mussolini formed the pro-interventionist Il Popolo d'Italia and the Fasci Rivoluzionario d'Azione Internazionalista (""Revolutionary Fasci for International Action"") in October 1914 that later developed into the Fasci Italiani di Combattimento in 1919, the origin of fascism. Mussolini's nationalism enabled him to raise funds from Ansaldo (an armaments firm) and other companies to create Il Popolo d'Italia to convince socialists and revolutionaries to support the war.


==== Patriotic funds ====
On both sides, there was large-scale fundraising for soldiers' welfare, their dependents and those injured. The Nail Men were a German example. Around the British Empire, there were many patriotic funds, including the Royal Patriotic Fund Corporation, Canadian Patriotic Fund, Queensland Patriotic Fund and, by 1919, there were 983 funds in New Zealand. At the start of the next world war the New Zealand funds were reformed, having been criticised as overlapping, wasteful and abused, but 11 were still functioning in 2002.


=== Opposition ===

Many countries jailed those who spoke out against the conflict. These included Eugene Debs in the US and Bertrand Russell in Britain. In the US, the Espionage Act of 1917 and Sedition Act of 1918 made it a federal crime to oppose military recruitment or make any statements deemed ""disloyal"". Publications at all critical of the government were removed from circulation by postal censors, and many served long prison sentences for statements of fact deemed unpatriotic.
Several nationalists opposed intervention, particularly within states that the nationalists were hostile to. Although the vast majority of Irish people consented to participate in the war in 1914 and 1915, a minority of advanced Irish nationalists had staunchly opposed taking part. The war began amid the Home Rule crisis in Ireland that had resurfaced in 1912, and by July 1914 there was a serious possibility of an outbreak of civil war in Ireland. Irish nationalists and Marxists attempted to pursue Irish independence, culminating in the Easter Rising of 1916, with Germany sending 20,000 rifles to Ireland to stir unrest in Britain. The British government placed Ireland under martial law in response to the Easter Rising, though once the immediate threat of revolution had dissipated, the authorities did try to make concessions to nationalist feeling. However, opposition to involvement in the war increased in Ireland, resulting in the Conscription Crisis of 1918.
Other opposition came from conscientious objectors—some socialist, some religious—who had refused to fight. In Britain, 16,000 people asked for conscientious objector status. Some of them, most notably prominent peace activist Stephen Hobhouse, refused both military and alternative service. Many suffered years of prison, including solitary confinement. Even after the war, in Britain, many job advertisements were marked ""No conscientious objectors need to apply"".
On 1–4 May 1917, about 100,000 workers and soldiers of Petrograd, and after them, the workers and soldiers of other Russian cities, led by the Bolsheviks, demonstrated under banners reading ""Down with the war!"" and ""all power to the Soviets!"". The mass demonstrations resulted in a crisis for the Russian Provisional Government. In Milan, in May 1917, Bolshevik revolutionaries organised and engaged in rioting calling for an end to the war, and managed to close down factories and stop public transportation. The Italian army was forced to enter Milan with tanks and machine guns to face Bolsheviks and anarchists, who fought violently until May 23 when the army gained control of the city. Almost 50 people (including three Italian soldiers) were killed and over 800 people were arrested.


== Technology ==

World War I began as a clash of 20th-century technology and 19th-century tactics, with the inevitably large ensuing casualties. By the end of 1917, however, the major armies had modernised and were making use of telephone, wireless communication, armoured cars, tanks (especially with the advent of the prototype tank, Little Willie), and aircraft.

Artillery also underwent a revolution. In 1914, cannons were positioned in the front line and fired directly at their targets. By 1917, indirect fire with guns (as well as mortars and even machine guns) was commonplace, using new techniques for spotting and ranging, notably, aircraft and the field telephone.
Fixed-wing aircraft were initially used for reconnaissance and ground attack. To shoot down enemy planes, anti-aircraft guns and fighter aircraft were developed. Strategic bombers were created, principally by the Germans and British, though the former used Zeppelins as well. Towards the end of the conflict, aircraft carriers were used for the first time, with HMS Furious launching Sopwith Camels in a raid to destroy the Zeppelin hangars at Tønder in 1918.


== Diplomacy ==

The non-military diplomatic and propaganda interactions among the nations were designed to build support for the cause or to undermine support for the enemy. For the most part, wartime diplomacy focused on five issues: propaganda campaigns; defining and redefining the war goals, which became harsher as the war went on; luring neutral nations (Italy, Ottoman Empire, Bulgaria, Romania) into the coalition by offering slices of enemy territory; and encouragement by the Allies of nationalistic minority movements inside the Central Powers, especially among Czechs, Poles, and Arabs. In addition, multiple peace proposals were coming from neutrals, or one side or the other; none of them progressed very far.


== Legacy and memory ==


=== Memorials ===

Memorials were built in thousands of villages and towns. Close to battlefields, those buried in improvised burial grounds were gradually moved to formal graveyards under the care of organisations such as the Commonwealth War Graves Commission, the American Battle Monuments Commission, the German War Graves Commission, and Le Souvenir français. Many of these graveyards also have monuments to the missing or unidentified dead, such as the Menin Gate Memorial to the Missing and the Thiepval Memorial to the Missing of the Somme.
In 1915, John McCrae, a Canadian army doctor, wrote the poem In Flanders Fields as a salute to those who perished in the war. It is still recited today, especially on Remembrance Day and Memorial Day.

National World War I Museum and Memorial in Kansas City, Missouri, is a memorial dedicated to all Americans who served in World War I. The Liberty Memorial was dedicated on 1 November 1921.
The British government budgeted substantial resources to the commemoration of the war during the period 2014 to 2018. The lead body is the Imperial War Museum. On 3 August 2014, French President François Hollande and German President Joachim Gauck together marked the centenary of Germany's declaration of war on France by laying the first stone of a memorial in Vieil Armand, known in German as Hartmannswillerkopf, for French and German soldiers killed in the war. As part of commemorations for the centenary of the 1918 Armistice, French President Emmanuel Macron and German Chancellor Angela Merkel visited the site of the signing of the Armistice of Compiègne and unveiled a plaque to reconciliation.


=== Historiography ===

 
The first efforts to comprehend the meaning and consequences of modern warfare began during the initial phases of the war and are still underway more than a century later. Teaching World War I has presented special challenges. When compared with World War II, the First World War is often thought to be ""a wrong war fought for the wrong reasons""; it lacks the metanarrative of good versus evil that characterizes retellings of the Second World War. Lacking recognizable heroes and villains, it is often taught thematically, invoking simplified tropes that obscure the complexity of the conflict.
Historian Heather Jones argues that the historiography has been reinvigorated by a cultural turn in the 21st century. Scholars have raised entirely new questions regarding military occupation, radicalisation of politics, race, medical science, gender and mental health. Among the major subjects that historians have long debated regarding the war include: Why the war began; why the Allies won; whether generals were responsible for high casualty rates; how soldiers endured the poor conditions of trench warfare; and to what extent the civilian home front accepted and endorsed the war effort.


=== Unexploded ordnance ===

As late as 2007, unexploded ordnance at battlefield sites like Verdun and Somme continued to pose a danger. In France and Belgium, locals who discover caches of unexploded munitions are assisted by weapons disposal units. In some places, plant life has still not recovered from the effects of the war.


== See also ==
Lists of World War I topics
List of military engagements of World War I
Outline of World War I
World war
World War II


== Footnotes ==


== References ==


== Bibliography ==


== External links ==

Links to other WWI Sites, worldwide links from Brigham Young U.
The World War One Document Archive, from Brigham Young U.
International Encyclopedia of the First World War
Records on the outbreak of World War I from the UK Parliamentary Collections
The Heritage of the Great War / First World War. Graphic colour photos, pictures and music
A multimedia history of World War I
European Newspapers from the start of the First World War and the end of the war
WWI Films on the European Film Gateway
The British Pathé WW1 Film Archive Archived 24 March 2019 at the Wayback Machine
World War I British press photograph collection – A sampling of images distributed by the British government during the war to diplomats overseas, from the UBC Library Digital Collections
Personal accounts of American World War I veterans, Veterans History Project, Library of Congress
Who is at fault for World War I? (YouTube Premium): Link


=== Library guides ===
National Library of New Zealand
State Library of New South Wales
US Library of Congress
Indiana University Bloomington Archived 5 June 2015 at the Wayback Machine
New York University Archived 5 April 2015 at the Wayback Machine
University of Alberta
California State Library, California History Room. Collection: California. State Council of Defense. California War History Committee. Records of Californians who served in World War I, 1918–1922."
The history of mathematics,"The history of mathematics deals with the origin of discoveries in mathematics and the mathematical methods and notation of the past. Before the modern age and the worldwide spread of knowledge, written examples of new mathematical developments have come to light only in a few locales. From 3000 BC the Mesopotamian states of Sumer, Akkad and Assyria, followed closely by Ancient Egypt and the Levantine state of Ebla began using arithmetic, algebra and geometry for purposes of taxation, commerce, trade and also in the field of astronomy to record time and formulate calendars.
The earliest mathematical texts available are from Mesopotamia and Egypt – Plimpton 322 (Babylonian c. 2000 – 1900 BC), the Rhind Mathematical Papyrus (Egyptian c. 1800 BC) and the Moscow Mathematical Papyrus (Egyptian c. 1890 BC). All of these texts mention the so-called Pythagorean triples, so, by inference, the Pythagorean theorem seems to be the most ancient and widespread mathematical development after basic arithmetic and geometry.
The study of mathematics as a ""demonstrative discipline"" began in the 6th century BC with the Pythagoreans, who coined the term ""mathematics"" from the ancient Greek μάθημα (mathema), meaning ""subject of instruction"". Greek mathematics greatly refined the methods (especially through the introduction of deductive reasoning and mathematical rigor in proofs) and expanded the subject matter of mathematics. Although they made virtually no contributions to theoretical mathematics, the ancient Romans used applied mathematics in surveying, structural engineering, mechanical engineering, bookkeeping, creation of lunar and solar calendars, and even arts and crafts. Chinese mathematics made early contributions, including a place value system and the first use of negative numbers. The Hindu–Arabic numeral system and the rules for the use of its operations, in use throughout the world today evolved over the course of the first millennium AD in India and were transmitted to the Western world via Islamic mathematics through the work of Muḥammad ibn Mūsā al-Khwārizmī. Islamic mathematics, in turn, developed and expanded the mathematics known to these civilizations. Contemporaneous with but independent of these traditions were the mathematics developed by the Maya civilization of Mexico and Central America, where the concept of zero was given a standard symbol in Maya numerals.
Many Greek and Arabic texts on mathematics were translated into Latin from the 12th century onward, leading to further development of mathematics in Medieval Europe. From ancient times through the Middle Ages, periods of mathematical discovery were often followed by centuries of stagnation. Beginning in Renaissance Italy in the 15th century, new mathematical developments, interacting with new scientific discoveries, were made at an increasing pace that continues through the present day. This includes the groundbreaking work of both Isaac Newton and Gottfried Wilhelm Leibniz in the development of infinitesimal calculus during the course of the 17th century.


== Prehistoric ==
The origins of mathematical thought lie in the concepts of number, patterns in nature, magnitude, and form. Modern studies of animal cognition have shown that these concepts are not unique to humans. Such concepts would have been part of everyday life in hunter-gatherer societies. The idea of the ""number"" concept evolving gradually over time is supported by the existence of languages which preserve the distinction between ""one"", ""two"", and ""many"", but not of numbers larger than two.
The Ishango bone, found near the headwaters of the Nile river (northeastern Congo), may be more than 20,000 years old and consists of a series of marks carved in three columns running the length of the bone. Common interpretations are that the Ishango bone shows either a tally of the earliest known demonstration of sequences of prime numbers or a six-month lunar calendar. Peter Rudman argues that the development of the concept of prime numbers could only have come about after the concept of division, which he dates to after 10,000 BC, with prime numbers probably not being understood until about 500 BC. He also writes that ""no attempt has been made to explain why a tally of something should exhibit multiples of two, prime numbers between 10 and 20, and some numbers that are almost multiples of 10."" The Ishango bone, according to scholar Alexander Marshack, may have influenced the later development of mathematics in Egypt as, like some entries on the Ishango bone, Egyptian arithmetic also made use of multiplication by 2; this however, is disputed.
Predynastic Egyptians of the 5th millennium BC pictorially represented geometric designs. It has been claimed that megalithic monuments in England and Scotland, dating from the 3rd millennium BC, incorporate geometric ideas such as circles, ellipses, and Pythagorean triples in their design. All of the above are disputed however, and the currently oldest undisputed mathematical documents are from Babylonian and dynastic Egyptian sources.


== Babylonian ==

Babylonian mathematics refers to any mathematics of the peoples of Mesopotamia (modern Iraq) from the days of the early Sumerians through the Hellenistic period almost to the dawn of Christianity. The majority of Babylonian mathematical work comes from two widely separated periods: The first few hundred years of the second millennium BC (Old Babylonian period), and the last few centuries of the first millennium BC (Seleucid period). It is named Babylonian mathematics due to the central role of Babylon as a place of study. Later under the Arab Empire, Mesopotamia, especially Baghdad, once again became an important center of study for Islamic mathematics.

In contrast to the sparsity of sources in Egyptian mathematics, knowledge of Babylonian mathematics is derived from more than 400 clay tablets unearthed since the 1850s. Written in Cuneiform script, tablets were inscribed whilst the clay was moist, and baked hard in an oven or by the heat of the sun. Some of these appear to be graded homework.
The earliest evidence of written mathematics dates back to the ancient Sumerians, who built the earliest civilization in Mesopotamia. They developed a complex system of metrology from 3000 BC that was chiefly concerned with administrative/financial counting, such as grain allotments, workers, weights of silver, or even liquids, among other things. From around 2500 BC onward, the Sumerians wrote multiplication tables on clay tablets and dealt with geometrical exercises and division problems. The earliest traces of the Babylonian numerals also date back to this period.

Babylonian mathematics were written using a sexagesimal (base-60) numeral system. From this derives the modern-day usage of 60 seconds in a minute, 60 minutes in an hour, and 360 (60 × 6) degrees in a circle, as well as the use of seconds and minutes of arc to denote fractions of a degree. It is thought the sexagesimal system was initially used by Sumerian scribes because 60 can be evenly divided by 2, 3, 4, 5, 6, 10, 12, 15, 20 and 30, and for scribes (doling out the aforementioned grain allotments, recording weights of silver, etc.) being able to easily calculate by hand was essential, and so a sexagesimal system is pragmatically easier to calculate by hand with; however, there is the possibility that using a sexagesimal system was an ethno-linguistic phenomenon (that might not ever be known), and not a mathematical/practical decision. Also, unlike the Egyptians, Greeks, and Romans, the Babylonians had a place-value system, where digits written in the left column represented larger values, much as in the decimal system. The power of the Babylonian notational system lay in that it could be used to represent fractions as easily as whole numbers; thus multiplying two numbers that contained fractions was no different from multiplying integers, similar to modern notation. The notational system of the Babylonians was the best of any civilization until the Renaissance, and its power allowed it to achieve remarkable computational accuracy; for example, the Babylonian tablet YBC 7289 gives an approximation of √2 accurate to five decimal places. The Babylonians lacked, however, an equivalent of the decimal point, and so the place value of a symbol often had to be inferred from the context. By the Seleucid period, the Babylonians had developed a zero symbol as a placeholder for empty positions; however it was only used for intermediate positions. This zero sign does not appear in terminal positions, thus the Babylonians came close but did not develop a true place value system.
Other topics covered by Babylonian mathematics include fractions, algebra, quadratic and cubic equations, and the calculation of regular numbers, and their reciprocal pairs. The tablets also include multiplication tables and methods for solving linear, quadratic equations and cubic equations, a remarkable achievement for the time. Tablets from the Old Babylonian period also contain the earliest known statement of the Pythagorean theorem. However, as with Egyptian mathematics, Babylonian mathematics shows no awareness of the difference between exact and approximate solutions, or the solvability of a problem, and most importantly, no explicit statement of the need for proofs or logical principles.


== Egyptian ==

Egyptian mathematics refers to mathematics written in the Egyptian language. From the Hellenistic period, Greek replaced Egyptian as the written language of Egyptian scholars. Mathematical study in Egypt later continued under the Arab Empire as part of Islamic mathematics, when Arabic became the written language of Egyptian scholars. Archaeological evidence has suggested that the Ancient Egyptian counting system had origins in Sub-Saharan Africa. Also, fractal geometry designs which are widespread among Sub-Saharan African cultures are also found in Egyptian architecture and cosmological signs.
The most extensive Egyptian mathematical text is the Rhind papyrus (sometimes also called the Ahmes Papyrus after its author), dated to c. 1650 BC but likely a copy of an older document from the Middle Kingdom of about 2000–1800 BC. It is an instruction manual for students in arithmetic and geometry. In addition to giving area formulas and methods for multiplication, division and working with unit fractions, it also contains evidence of other mathematical knowledge, including composite and prime numbers; arithmetic, geometric and harmonic means; and simplistic understandings of both the Sieve of Eratosthenes and perfect number theory (namely, that of the number 6). It also shows how to solve first order linear equations as well as arithmetic and geometric series.
Another significant Egyptian mathematical text is the Moscow papyrus, also from the Middle Kingdom period, dated to c. 1890 BC. It consists of what are today called word problems or story problems, which were apparently intended as entertainment. One problem is considered to be of particular importance because it gives a method for finding the volume of a frustum (truncated pyramid).
Finally, the Berlin Papyrus 6619 (c. 1800 BC) shows that ancient Egyptians could solve a second-order algebraic equation.


== Greek ==

Greek mathematics refers to the mathematics written in the Greek language from the time of Thales of Miletus (~600 BC) to the closure of the Academy of Athens in 529 AD. Greek mathematicians lived in cities spread over the entire Eastern Mediterranean, from Italy to North Africa, but were united by culture and language. Greek mathematics of the period following Alexander the Great is sometimes called Hellenistic mathematics.
Greek mathematics was much more sophisticated than the mathematics that had been developed by earlier cultures. All surviving records of pre-Greek mathematics show the use of inductive reasoning, that is, repeated observations used to establish rules of thumb. Greek mathematicians, by contrast, used deductive reasoning. The Greeks used logic to derive conclusions from definitions and axioms, and used mathematical rigor to prove them.
Greek mathematics is thought to have begun with Thales of Miletus (c. 624–c.546 BC) and Pythagoras of Samos (c. 582–c. 507 BC). Although the extent of the influence is disputed, they were probably inspired by Egyptian and Babylonian mathematics. According to legend, Pythagoras traveled to Egypt to learn mathematics, geometry, and astronomy from Egyptian priests.
Thales used geometry to solve problems such as calculating the height of pyramids and the distance of ships from the shore. He is credited with the first use of deductive reasoning applied to geometry, by deriving four corollaries to Thales' Theorem. As a result, he has been hailed as the first true mathematician and the first known individual to whom a mathematical discovery has been attributed. Pythagoras established the Pythagorean School, whose doctrine it was that mathematics ruled the universe and whose motto was ""All is number"". It was the Pythagoreans who coined the term ""mathematics"", and with whom the study of mathematics for its own sake begins. The Pythagoreans are credited with the first proof of the Pythagorean theorem, though the statement of the theorem has a long history, and with the proof of the existence of irrational numbers. Although he was preceded by the Babylonians, Indians and the Chinese, the Neopythagorean mathematician Nicomachus (60–120 AD) provided one of the earliest Greco-Roman multiplication tables, whereas the oldest extant Greek multiplication table is found on a wax tablet dated to the 1st century AD (now found in the British Museum). The association of the Neopythagoreans with the Western invention of the multiplication table is evident in its later Medieval name: the mensa Pythagorica.
Plato (428/427 BC – 348/347 BC) is important in the history of mathematics for inspiring and guiding others. His Platonic Academy, in Athens, became the mathematical center of the world in the 4th century BC, and it was from this school that the leading mathematicians of the day, such as Eudoxus of Cnidus (c. 390 - c. 340 BC), came. Plato also discussed the foundations of mathematics, clarified some of the definitions (e.g. that of a line as ""breadthless length""), and reorganized the assumptions. The analytic method is ascribed to Plato, while a formula for obtaining Pythagorean triples bears his name.
Eudoxus developed the method of exhaustion, a precursor of modern integration and a theory of ratios that avoided the problem of incommensurable magnitudes. The former allowed the calculations of areas and volumes of curvilinear figures, while the latter enabled subsequent geometers to make significant advances in geometry. Though he made no specific technical mathematical discoveries, Aristotle (384–c. 322 BC) contributed significantly to the development of mathematics by laying the foundations of logic.

In the 3rd century BC, the premier center of mathematical education and research was the Musaeum of Alexandria. It was there that Euclid (c. 300 BC) taught, and wrote the Elements, widely considered the most successful and influential textbook of all time. The Elements introduced mathematical rigor through the axiomatic method and is the earliest example of the format still used in mathematics today, that of definition, axiom, theorem, and proof. Although most of the contents of the Elements were already known, Euclid arranged them into a single, coherent logical framework. The Elements was known to all educated people in the West up through the middle of the 20th century and its contents are still taught in geometry classes today. In addition to the familiar theorems of Euclidean geometry, the Elements was meant as an introductory textbook to all mathematical subjects of the time, such as number theory, algebra and solid geometry, including proofs that the square root of two is irrational and that there are infinitely many prime numbers. Euclid also wrote extensively on other subjects, such as conic sections, optics, spherical geometry, and mechanics, but only half of his writings survive.

Archimedes (c. 287–212 BC) of Syracuse, widely considered the greatest mathematician of antiquity, used the method of exhaustion to calculate the area under the arc of a parabola with the summation of an infinite series, in a manner not too dissimilar from modern calculus. He also showed one could use the method of exhaustion to calculate the value of π with as much precision as desired, and obtained the most accurate value of π then known, 3+⁠10/71⁠ < π < 3+⁠10/70⁠. He also studied the spiral bearing his name, obtained formulas for the volumes of surfaces of revolution (paraboloid, ellipsoid, hyperboloid), and an ingenious method of exponentiation for expressing very large numbers. While he is also known for his contributions to physics and several advanced mechanical devices, Archimedes himself placed far greater value on the products of his thought and general mathematical principles. He regarded as his greatest achievement his finding of the surface area and volume of a sphere, which he obtained by proving these are 2/3 the surface area and volume of a cylinder circumscribing the sphere.

Apollonius of Perga (c. 262–190 BC) made significant advances to the study of conic sections, showing that one can obtain all three varieties of conic section by varying the angle of the plane that cuts a double-napped cone. He also coined the terminology in use today for conic sections, namely parabola (""place beside"" or ""comparison""), ""ellipse"" (""deficiency""), and ""hyperbola"" (""a throw beyond""). His work Conics is one of the best known and preserved mathematical works from antiquity, and in it he derives many theorems concerning conic sections that would prove invaluable to later mathematicians and astronomers studying planetary motion, such as Isaac Newton. While neither Apollonius nor any other Greek mathematicians made the leap to coordinate geometry, Apollonius' treatment of curves is in some ways similar to the modern treatment, and some of his work seems to anticipate the development of analytical geometry by Descartes some 1800 years later.
Around the same time, Eratosthenes of Cyrene (c. 276–194 BC) devised the Sieve of Eratosthenes for finding prime numbers. The 3rd century BC is generally regarded as the ""Golden Age"" of Greek mathematics, with advances in pure mathematics henceforth in relative decline. Nevertheless, in the centuries that followed significant advances were made in applied mathematics, most notably trigonometry, largely to address the needs of astronomers. Hipparchus of Nicaea (c. 190–120 BC) is considered the founder of trigonometry for compiling the first known trigonometric table, and to him is also due the systematic use of the 360 degree circle. Heron of Alexandria (c. 10–70 AD) is credited with Heron's formula for finding the area of a scalene triangle and with being the first to recognize the possibility of negative numbers possessing square roots. Menelaus of Alexandria (c. 100 AD) pioneered spherical trigonometry through Menelaus' theorem. The most complete and influential trigonometric work of antiquity is the Almagest of Ptolemy (c. AD 90–168), a landmark astronomical treatise whose trigonometric tables would be used by astronomers for the next thousand years. Ptolemy is also credited with Ptolemy's theorem for deriving trigonometric quantities, and the most accurate value of π outside of China until the medieval period, 3.1416.

Following a period of stagnation after Ptolemy, the period between 250 and 350 AD is sometimes referred to as the ""Silver Age"" of Greek mathematics. During this period, Diophantus made significant advances in algebra, particularly indeterminate analysis, which is also known as ""Diophantine analysis"". The study of Diophantine equations and Diophantine approximations is a significant area of research to this day. His main work was the Arithmetica, a collection of 150 algebraic problems dealing with exact solutions to determinate and indeterminate equations. The Arithmetica had a significant influence on later mathematicians, such as Pierre de Fermat, who arrived at his famous Last Theorem after trying to generalize a problem he had read in the Arithmetica (that of dividing a square into two squares). Diophantus also made significant advances in notation, the Arithmetica being the first instance of algebraic symbolism and syncopation.

Among the last great Greek mathematicians is Pappus of Alexandria (4th century AD). He is known for his hexagon theorem and centroid theorem, as well as the Pappus configuration and Pappus graph. His Collection is a major source of knowledge on Greek mathematics as most of it has survived. Pappus is considered the last major innovator in Greek mathematics, with subsequent work consisting mostly of commentaries on earlier work.
The first woman mathematician recorded by history was Hypatia of Alexandria (AD 350–415). She succeeded her father (Theon of Alexandria) as Librarian at the Great Library and wrote many works on applied mathematics. Because of a political dispute, the Christian community in Alexandria had her stripped publicly and executed. Her death is sometimes taken as the end of the era of the Alexandrian Greek mathematics, although work did continue in Athens for another century with figures such as Proclus, Simplicius and Eutocius. Although Proclus and Simplicius were more philosophers than mathematicians, their commentaries on earlier works are valuable sources on Greek mathematics. The closure of the neo-Platonic Academy of Athens by the emperor Justinian in 529 AD is traditionally held as marking the end of the era of Greek mathematics, although the Greek tradition continued unbroken in the Byzantine empire with mathematicians such as Anthemius of Tralles and Isidore of Miletus, the architects of the Hagia Sophia. Nevertheless, Byzantine mathematics consisted mostly of commentaries, with little in the way of innovation, and the centers of mathematical innovation were to be found elsewhere by this time.


== Roman ==

Although ethnic Greek mathematicians continued under the rule of the late Roman Republic and subsequent Roman Empire, there were no noteworthy native Latin mathematicians in comparison. Ancient Romans such as Cicero (106–43 BC), an influential Roman statesman who studied mathematics in Greece, believed that Roman surveyors and calculators were far more interested in applied mathematics than the theoretical mathematics and geometry that were prized by the Greeks. It is unclear if the Romans first derived their numerical system directly from the Greek precedent or from Etruscan numerals used by the Etruscan civilization centered in what is now Tuscany, central Italy.
Using calculation, Romans were adept at both instigating and detecting financial fraud, as well as managing taxes for the treasury. Siculus Flaccus, one of the Roman gromatici (i.e. land surveyor), wrote the Categories of Fields, which aided Roman surveyors in measuring the surface areas of allotted lands and territories. Aside from managing trade and taxes, the Romans also regularly applied mathematics to solve problems in engineering, including the erection of architecture such as bridges, road-building, and preparation for military campaigns. Arts and crafts such as Roman mosaics, inspired by previous Greek designs, created illusionist geometric patterns and rich, detailed scenes that required precise measurements for each tessera tile, the opus tessellatum pieces on average measuring eight millimeters square and the finer opus vermiculatum pieces having an average surface of four millimeters square.
The creation of the Roman calendar also necessitated basic mathematics. The first calendar allegedly dates back to 8th century BC during the Roman Kingdom and included 356 days plus a leap year every other year. In contrast, the lunar calendar of the Republican era contained 355 days, roughly ten-and-one-fourth days shorter than the solar year, a discrepancy that was solved by adding an extra month into the calendar after the 23rd of February. This calendar was supplanted by the Julian calendar, a solar calendar organized by Julius Caesar (100–44 BC) and devised by Sosigenes of Alexandria to include a leap day every four years in a 365-day cycle. This calendar, which contained an error of 11 minutes and 14 seconds, was later corrected by the Gregorian calendar organized by Pope Gregory XIII (r. 1572–1585), virtually the same solar calendar used in modern times as the international standard calendar.
At roughly the same time, the Han Chinese and the Romans both invented the wheeled odometer device for measuring distances traveled, the Roman model first described by the Roman civil engineer and architect Vitruvius (c. 80 BC – c. 15 BC). The device was used at least until the reign of emperor Commodus (r. 177 – 192 AD), but its design seems to have been lost until experiments were made during the 15th century in Western Europe. Perhaps relying on similar gear-work and technology found in the Antikythera mechanism, the odometer of Vitruvius featured chariot wheels measuring 4 feet (1.2 m) in diameter turning four-hundred times in one Roman mile (roughly 4590 ft/1400 m). With each revolution, a pin-and-axle device engaged a 400-tooth cogwheel that turned a second gear responsible for dropping pebbles into a box, each pebble representing one mile traversed.


== Chinese ==

An analysis of early Chinese mathematics has demonstrated its unique development compared to other parts of the world, leading scholars to assume an entirely independent development. The oldest extant mathematical text from China is the Zhoubi Suanjing (周髀算經), variously dated to between 1200 BC and 100 BC, though a date of about 300 BC during the Warring States Period appears reasonable. However, the Tsinghua Bamboo Slips, containing the earliest known decimal multiplication table (although ancient Babylonians had ones with a base of 60), is dated around 305 BC and is perhaps the oldest surviving mathematical text of China.

Of particular note is the use in Chinese mathematics of a decimal positional notation system, the so-called ""rod numerals"" in which distinct ciphers were used for numbers between 1 and 10, and additional ciphers for powers of ten. Thus, the number 123 would be written using the symbol for ""1"", followed by the symbol for ""100"", then the symbol for ""2"" followed by the symbol for ""10"", followed by the symbol for ""3"". This was the most advanced number system in the world at the time, apparently in use several centuries before the common era and well before the development of the Indian numeral system. Rod numerals allowed the representation of numbers as large as desired and allowed calculations to be carried out on the suan pan, or Chinese abacus. The date of the invention of the suan pan is not certain, but the earliest written mention dates from AD 190, in Xu Yue's Supplementary Notes on the Art of Figures.
The oldest extant work on geometry in China comes from the philosophical Mohist canon c. 330 BC, compiled by the followers of Mozi (470–390 BC). The Mo Jing described various aspects of many fields associated with physical science, and provided a small number of geometrical theorems as well. It also defined the concepts of circumference, diameter, radius, and volume.

In 212 BC, the Emperor Qin Shi Huang commanded all books in the Qin Empire other than officially sanctioned ones be burned. This decree was not universally obeyed, but as a consequence of this order little is known about ancient Chinese mathematics before this date. After the book burning of 212 BC, the Han dynasty (202 BC–220 AD) produced works of mathematics which presumably expanded on works that are now lost. The most important of these is The Nine Chapters on the Mathematical Art, the full title of which appeared by AD 179, but existed in part under other titles beforehand. It consists of 246 word problems involving agriculture, business, employment of geometry to figure height spans and dimension ratios for Chinese pagoda towers, engineering, surveying, and includes material on right triangles. It created mathematical proof for the Pythagorean theorem, and a mathematical formula for Gaussian elimination. The treatise also provides values of π, which Chinese mathematicians originally approximated as 3 until Liu Xin (d. 23 AD) provided a figure of 3.1457 and subsequently Zhang Heng (78–139) approximated pi as 3.1724, as well as 3.162 by taking the square root of 10. Liu Hui commented on the Nine Chapters in the 3rd century AD and gave a value of π accurate to 5 decimal places (i.e. 3.14159). Though more of a matter of computational stamina than theoretical insight, in the 5th century AD Zu Chongzhi computed the value of π to seven decimal places (between 3.1415926 and 3.1415927), which remained the most accurate value of π for almost the next 1000 years. He also established a method which would later be called Cavalieri's principle to find the volume of a sphere.
The high-water mark of Chinese mathematics occurred in the 13th century during the latter half of the Song dynasty (960–1279), with the development of Chinese algebra. The most important text from that period is the Precious Mirror of the Four Elements by Zhu Shijie (1249–1314), dealing with the solution of simultaneous higher order algebraic equations using a method similar to Horner's method. The Precious Mirror also contains a diagram of Pascal's triangle with coefficients of binomial expansions through the eighth power, though both appear in Chinese works as early as 1100. The Chinese also made use of the complex combinatorial diagram known as the magic square and magic circles, described in ancient times and perfected by Yang Hui (AD 1238–1298).
Even after European mathematics began to flourish during the Renaissance, European and Chinese mathematics were separate traditions, with significant Chinese mathematical output in decline from the 13th century onwards. Jesuit missionaries such as Matteo Ricci carried mathematical ideas back and forth between the two cultures from the 16th to 18th centuries, though at this point far more mathematical ideas were entering China than leaving.
Japanese mathematics, Korean mathematics, and Vietnamese mathematics are traditionally viewed as stemming from Chinese mathematics and belonging to the Confucian-based East Asian cultural sphere. Korean and Japanese mathematics were heavily influenced by the algebraic works produced during China's Song dynasty, whereas Vietnamese mathematics was heavily indebted to popular works of China's Ming dynasty (1368–1644). For instance, although Vietnamese mathematical treatises were written in either Chinese or the native Vietnamese Chữ Nôm script, all of them followed the Chinese format of presenting a collection of problems with algorithms for solving them, followed by numerical answers. Mathematics in Vietnam and Korea were mostly associated with the professional court bureaucracy of mathematicians and astronomers, whereas in Japan it was more prevalent in the realm of private schools.


== Japan ==

The mathematics that developed in Japan during the Edo period (1603-1887) is independent of Western mathematics; To this period belongs the mathematician Seki Takakazu, of great influence, for example, in the development of wasan (traditional Japanese mathematics), and whose discoveries (in areas such as integral calculus), are almost simultaneous with contemporary European mathematicians such as Gottfried Leibniz.
Japanese mathematics of this period is inspired by Chinese mathematics and is oriented towards essentially geometric problems. On wooden tablets called sangaku, ""geometric enigmas"" are proposed and solved; That's where, for example, Soddy's hexlet theorem comes from.


== Indian ==

The earliest civilization on the Indian subcontinent is the Indus Valley civilization (mature second phase: 2600 to 1900 BC) that flourished in the Indus river basin. Their cities were laid out with geometric regularity, but no known mathematical documents survive from this civilization.
The oldest extant mathematical records from India are the Sulba Sutras (dated variously between the 8th century BC and the 2nd century AD), appendices to religious texts which give simple rules for constructing altars of various shapes, such as squares, rectangles, parallelograms, and others. As with Egypt, the preoccupation with temple functions points to an origin of mathematics in religious ritual. The Sulba Sutras give methods for constructing a circle with approximately the same area as a given square, which imply several different approximations of the value of π. In addition, they compute the square root of 2 to several decimal places, list Pythagorean triples, and give a statement of the Pythagorean theorem. All of these results are present in Babylonian mathematics, indicating Mesopotamian influence. It is not known to what extent the Sulba Sutras influenced later Indian mathematicians. As in China, there is a lack of continuity in Indian mathematics; significant advances are separated by long periods of inactivity.
Pāṇini (c. 5th century BC) formulated the rules for Sanskrit grammar. His notation was similar to modern mathematical notation, and used metarules, transformations, and recursion. Pingala (roughly 3rd–1st centuries BC) in his treatise of prosody uses a device corresponding to a binary numeral system. His discussion of the combinatorics of meters corresponds to an elementary version of the binomial theorem. Pingala's work also contains the basic ideas of Fibonacci numbers (called mātrāmeru).
The next significant mathematical documents from India after the Sulba Sutras are the Siddhantas, astronomical treatises from the 4th and 5th centuries AD (Gupta period) showing strong Hellenistic influence. They are significant in that they contain the first instance of trigonometric relations based on the half-chord, as is the case in modern trigonometry, rather than the full chord, as was the case in Ptolemaic trigonometry. Through a series of translation errors, the words ""sine"" and ""cosine"" derive from the Sanskrit ""jiya"" and ""kojiya"".

Around 500 AD, Aryabhata wrote the Aryabhatiya, a slim volume, written in verse, intended to supplement the rules of calculation used in astronomy and mathematical mensuration, though with no feeling for logic or deductive methodology. It is in the Aryabhatiya that the decimal place-value system first appears. Several centuries later, the Muslim mathematician Abu Rayhan Biruni described the Aryabhatiya as a ""mix of common pebbles and costly crystals"".
In the 7th century, Brahmagupta identified the Brahmagupta theorem, Brahmagupta's identity and Brahmagupta's formula, and for the first time, in Brahma-sphuta-siddhanta, he lucidly explained the use of zero as both a placeholder and decimal digit, and explained the Hindu–Arabic numeral system. It was from a translation of this Indian text on mathematics (c. 770) that Islamic mathematicians were introduced to this numeral system, which they adapted as Arabic numerals. Islamic scholars carried knowledge of this number system to Europe by the 12th century, and it has now displaced all older number systems throughout the world. Various symbol sets are used to represent numbers in the Hindu–Arabic numeral system, all of which evolved from the Brahmi numerals. Each of the roughly dozen major scripts of India has its own numeral glyphs. In the 10th century, Halayudha's commentary on Pingala's work contains a study of the Fibonacci sequence and Pascal's triangle, and describes the formation of a matrix.
In the 12th century, Bhāskara II, who lived in southern India, wrote extensively on all then known branches of mathematics. His work contains mathematical objects equivalent or approximately equivalent to infinitesimals, the mean value theorem and the derivative of the sine function although he did not develop the notion of a derivative. In the 14th century, Narayana Pandita completed his Ganita Kaumudi.
Also in the 14th century, Madhava of Sangamagrama, the founder of the Kerala School of Mathematics, found the Madhava–Leibniz series and obtained from it a transformed series, whose first 21 terms he used to compute the value of π as 3.14159265359. Madhava also found the Madhava-Gregory series to determine the arctangent, the Madhava-Newton power series to determine sine and cosine and the Taylor approximation for sine and cosine functions. In the 16th century, Jyesthadeva consolidated many of the Kerala School's developments and theorems in the Yukti-bhāṣā. It has been argued that certain ideas of calculus like infinite series and taylor series of some trigonometry functions, were transmitted to Europe in the 16th century via Jesuit missionaries and traders who were active around the ancient port of Muziris at the time and, as a result, directly influenced later European developments in analysis and calculus. However, other scholars argue that the Kerala School did not formulate a systematic theory of differentiation and integration, and that there is not any direct evidence of their results being transmitted outside Kerala.


== Islamic empires ==

The Islamic Empire established across the Middle East, Central Asia, North Africa, Iberia, and in parts of India in the 8th century made significant contributions towards mathematics. Although most Islamic texts on mathematics were written in Arabic, they were not all written by Arabs, since much like the status of Greek in the Hellenistic world, Arabic was used as the written language of non-Arab scholars throughout the Islamic world at the time.
In the 9th century, the Persian mathematician Muḥammad ibn Mūsā al-Khwārizmī wrote an important book on the Hindu–Arabic numerals and one on methods for solving equations. His book On the Calculation with Hindu Numerals, written about 825, along with the work of Al-Kindi, were instrumental in spreading Indian mathematics and Indian numerals to the West. The word algorithm is derived from the Latinization of his name, Algoritmi, and the word algebra from the title of one of his works, Al-Kitāb al-mukhtaṣar fī hīsāb al-ğabr wa’l-muqābala (The Compendious Book on Calculation by Completion and Balancing). He gave an exhaustive explanation for the algebraic solution of quadratic equations with positive roots, and he was the first to teach algebra in an elementary form and for its own sake. He also discussed the fundamental method of ""reduction"" and ""balancing"", referring to the transposition of subtracted terms to the other side of an equation, that is, the cancellation of like terms on opposite sides of the equation. This is the operation which al-Khwārizmī originally described as al-jabr. His algebra was also no longer concerned ""with a series of problems to be resolved, but an exposition which starts with primitive terms in which the combinations must give all possible prototypes for equations, which henceforward explicitly constitute the true object of study."" He also studied an equation for its own sake and ""in a generic manner, insofar as it does not simply emerge in the course of solving a problem, but is specifically called on to define an infinite class of problems.""
In Egypt, Abu Kamil extended algebra to the set of irrational numbers, accepting square roots and fourth roots as solutions and coefficients to quadratic equations. He also developed techniques used to solve three non-linear simultaneous equations with three unknown variables. One unique feature of his works was trying to find all the possible solutions to some of his problems, including one where he found 2676 solutions. His works formed an important foundation for the development of algebra and influenced later mathematicians, such as al-Karaji and Fibonacci.
Further developments in algebra were made by Al-Karaji in his treatise al-Fakhri, where he extends the methodology to incorporate integer powers and integer roots of unknown quantities. Something close to a proof by mathematical induction appears in a book written by Al-Karaji around 1000 AD, who used it to prove the binomial theorem, Pascal's triangle, and the sum of integral cubes. The historian of mathematics, F. Woepcke, praised Al-Karaji for being ""the first who introduced the theory of algebraic calculus."" Also in the 10th century, Abul Wafa translated the works of Diophantus into Arabic. Ibn al-Haytham was the first mathematician to derive the formula for the sum of the fourth powers, using a method that is readily generalizable for determining the general formula for the sum of any integral powers. He performed an integration in order to find the volume of a paraboloid, and was able to generalize his result for the integrals of polynomials up to the fourth degree. He thus came close to finding a general formula for the integrals of polynomials, but he was not concerned with any polynomials higher than the fourth degree.
In the late 11th century, Omar Khayyam wrote Discussions of the Difficulties in Euclid, a book about what he perceived as flaws in Euclid's Elements, especially the parallel postulate. He was also the first to find the general geometric solution to cubic equations. He was also very influential in calendar reform.
In the 13th century, Nasir al-Din Tusi (Nasireddin) made advances in spherical trigonometry. He also wrote influential work on Euclid's parallel postulate. In the 15th century, Ghiyath al-Kashi computed the value of π to the 16th decimal place. Kashi also had an algorithm for calculating nth roots, which was a special case of the methods given many centuries later by Ruffini and Horner.
Other achievements of Muslim mathematicians during this period include the addition of the decimal point notation to the Arabic numerals, the discovery of all the modern trigonometric functions besides the sine, al-Kindi's introduction of cryptanalysis and frequency analysis, the development of analytic geometry by Ibn al-Haytham, the beginning of algebraic geometry by Omar Khayyam and the development of an algebraic notation by al-Qalasādī.
During the time of the Ottoman Empire and Safavid Empire from the 15th century, the development of Islamic mathematics became stagnant.


== Maya ==

In the Pre-Columbian Americas, the Maya civilization that flourished in Mexico and Central America during the 1st millennium AD developed a unique tradition of mathematics that, due to its geographic isolation, was entirely independent of existing European, Egyptian, and Asian mathematics. Maya numerals used a base of twenty, the vigesimal system, instead of a base of ten that forms the basis of the decimal system used by most modern cultures. The Maya used mathematics to create the Maya calendar as well as to predict astronomical phenomena in their native Maya astronomy. While the concept of zero had to be inferred in the mathematics of many contemporary cultures, the Maya developed a standard symbol for it.


== Medieval European ==

Medieval European interest in mathematics was driven by concerns quite different from those of modern mathematicians. One driving element was the belief that mathematics provided the key to understanding the created order of nature, frequently justified by Plato's Timaeus and the biblical passage (in the Book of Wisdom) that God had ordered all things in measure, and number, and weight.
Boethius provided a place for mathematics in the curriculum in the 6th century when he coined the term quadrivium to describe the study of arithmetic, geometry, astronomy, and music. He wrote De institutione arithmetica, a free translation from the Greek of Nicomachus's Introduction to Arithmetic; De institutione musica, also derived from Greek sources; and a series of excerpts from Euclid's Elements. His works were theoretical, rather than practical, and were the basis of mathematical study until the recovery of Greek and Arabic mathematical works.
In the 12th century, European scholars traveled to Spain and Sicily seeking scientific Arabic texts, including al-Khwārizmī's The Compendious Book on Calculation by Completion and Balancing, translated into Latin by Robert of Chester, and the complete text of Euclid's Elements, translated in various versions by Adelard of Bath, Herman of Carinthia, and Gerard of Cremona. These and other new sources sparked a renewal of mathematics.
Leonardo of Pisa, now known as Fibonacci, serendipitously learned about the Hindu–Arabic numerals on a trip to what is now Béjaïa, Algeria with his merchant father. (Europe was still using Roman numerals.) There, he observed a system of arithmetic (specifically algorism) which due to the positional notation of Hindu–Arabic numerals was much more efficient and greatly facilitated commerce. Leonardo wrote Liber Abaci in 1202 (updated in 1254) introducing the technique to Europe and beginning a long period of popularizing it. The book also brought to Europe what is now known as the Fibonacci sequence (known to Indian mathematicians for hundreds of years before that) which Fibonacci used as an unremarkable example.
The 14th century saw the development of new mathematical concepts to investigate a wide range of problems. One important contribution was development of mathematics of local motion.
Thomas Bradwardine proposed that speed (V) increases in arithmetic proportion as the ratio of force (F) to resistance (R) increases in geometric proportion. Bradwardine expressed this by a series of specific examples, but although the logarithm had not yet been conceived, we can express his conclusion anachronistically by writing:
V = log (F/R). Bradwardine's analysis is an example of transferring a mathematical technique used by al-Kindi and Arnald of Villanova to quantify the nature of compound medicines to a different physical problem.

One of the 14th-century Oxford Calculators, William Heytesbury, lacking differential calculus and the concept of limits, proposed to measure instantaneous speed ""by the path that would be described by [a body] if... it were moved uniformly at the same degree of speed with which it is moved in that given instant"".
Heytesbury and others mathematically determined the distance covered by a body undergoing uniformly accelerated motion (today solved by integration), stating that ""a moving body uniformly acquiring or losing that increment [of speed] will traverse in some given time a [distance] completely equal to that which it would traverse if it were moving continuously through the same time with the mean degree [of speed]"".
Nicole Oresme at the University of Paris and the Italian Giovanni di Casali independently provided graphical demonstrations of this relationship, asserting that the area under the line depicting the constant acceleration, represented the total distance traveled. In a later mathematical commentary on Euclid's Elements, Oresme made a more detailed general analysis in which he demonstrated that a body will acquire in each successive increment of time an increment of any quality that increases as the odd numbers. Since Euclid had demonstrated the sum of the odd numbers are the square numbers, the total quality acquired by the body increases as the square of the time.


== Renaissance ==

During the Renaissance, the development of mathematics and of accounting were intertwined. While there is no direct relationship between algebra and accounting, the teaching of the subjects and the books published often intended for the children of merchants who were sent to reckoning schools (in Flanders and Germany) or abacus schools (known as abbaco in Italy), where they learned the skills useful for trade and commerce. There is probably no need for algebra in performing bookkeeping operations, but for complex bartering operations or the calculation of compound interest, a basic knowledge of arithmetic was mandatory and knowledge of algebra was very useful.
Piero della Francesca (c. 1415–1492) wrote books on solid geometry and linear perspective, including De Prospectiva Pingendi (On Perspective for Painting), Trattato d’Abaco (Abacus Treatise), and De quinque corporibus regularibus (On the Five Regular Solids).

Luca Pacioli's Summa de Arithmetica, Geometria, Proportioni et Proportionalità (Italian: ""Review of Arithmetic, Geometry, Ratio and Proportion"") was first printed and published in Venice in 1494. It included a 27-page treatise on bookkeeping, ""Particularis de Computis et Scripturis"" (Italian: ""Details of Calculation and Recording""). It was written primarily for, and sold mainly to, merchants who used the book as a reference text, as a source of pleasure from the mathematical puzzles it contained, and to aid the education of their sons. In Summa Arithmetica, Pacioli introduced symbols for plus and minus for the first time in a printed book, symbols that became standard notation in Italian Renaissance mathematics. Summa Arithmetica was also the first known book printed in Italy to contain algebra. Pacioli obtained many of his ideas from Piero Della Francesca whom he plagiarized.
In Italy, during the first half of the 16th century, Scipione del Ferro and Niccolò Fontana Tartaglia discovered solutions for cubic equations. Gerolamo Cardano published them in his 1545 book Ars Magna, together with a solution for the quartic equations, discovered by his student Lodovico Ferrari. In 1572 Rafael Bombelli published his L'Algebra in which he showed how to deal with the imaginary quantities that could appear in Cardano's formula for solving cubic equations.
Simon Stevin's De Thiende ('the art of tenths'), first published in Dutch in 1585, contained the first systematic treatment of decimal notation in Europe, which influenced all later work on the real number system.
Driven by the demands of navigation and the growing need for accurate maps of large areas, trigonometry grew to be a major branch of mathematics. Bartholomaeus Pitiscus was the first to use the word, publishing his Trigonometria in 1595. Regiomontanus's table of sines and cosines was published in 1533.
During the Renaissance the desire of artists to represent the natural world realistically, together with the rediscovered philosophy of the Greeks, led artists to study mathematics. They were also the engineers and architects of that time, and so had need of mathematics in any case. The art of painting in perspective, and the developments in geometry that were involved, were studied intensely.


== Mathematics during the Scientific Revolution ==


=== 17th century ===

The 17th century saw an unprecedented increase of mathematical and scientific ideas across Europe. Galileo observed the moons of Jupiter in orbit about that planet, using a telescope based Hans Lipperhey's. Tycho Brahe had gathered a large quantity of mathematical data describing the positions of the planets in the sky. By his position as Brahe's assistant, Johannes Kepler was first exposed to and seriously interacted with the topic of planetary motion. Kepler's calculations were made simpler by the contemporaneous invention of logarithms by John Napier and Jost Bürgi. Kepler succeeded in formulating mathematical laws of planetary motion.
The analytic geometry developed by René Descartes (1596–1650) allowed those orbits to be plotted on a graph, in Cartesian coordinates.
Building on earlier work by many predecessors, Isaac Newton discovered the laws of physics that explain Kepler's Laws, and brought together the concepts now known as calculus. Independently, Gottfried Wilhelm Leibniz, developed calculus and much of the calculus notation still in use today. He also refined the binary number system, which is the foundation of nearly all digital (electronic, solid-state, discrete logic) computers, including the Von Neumann architecture, which is the standard design paradigm, or ""computer architecture"", followed from the second half of the 20th century, and into the 21st. Leibniz has been called the ""founder of computer science"".
Science and mathematics had become an international endeavor, which would soon spread over the entire world.
In addition to the application of mathematics to the studies of the heavens, applied mathematics began to expand into new areas, with the correspondence of Pierre de Fermat and Blaise Pascal. Pascal and Fermat set the groundwork for the investigations of probability theory and the corresponding rules of combinatorics in their discussions over a game of gambling. Pascal, with his wager, attempted to use the newly developing probability theory to argue for a life devoted to religion, on the grounds that even if the probability of success was small, the rewards were infinite. In some sense, this foreshadowed the development of utility theory in the 18th and 19th centuries.


=== 18th century ===

The most influential mathematician of the 18th century was arguably Leonhard Euler (1707–1783). His contributions range from founding the study of graph theory with the Seven Bridges of Königsberg problem to standardizing many modern mathematical terms and notations. For example, he named the square root of minus 1 with the symbol i, and he popularized the use of the Greek letter 
  
    
      
        π
      
    
    {\displaystyle \pi }
  
 to stand for the ratio of a circle's circumference to its diameter. He made numerous contributions to the study of topology, graph theory, calculus, combinatorics, and complex analysis, as evidenced by the multitude of theorems and notations named for him.
Other important European mathematicians of the 18th century included Joseph Louis Lagrange, who did pioneering work in number theory, algebra, differential calculus, and the calculus of variations, and Pierre-Simon Laplace, who, in the age of Napoleon, did important work on the foundations of celestial mechanics and on statistics.


== Modern ==


=== 19th century ===

Throughout the 19th century mathematics became increasingly abstract. Carl Friedrich Gauss (1777–1855) epitomizes this trend. He did revolutionary work on functions of complex variables, in geometry, and on the convergence of series, leaving aside his many contributions to science. He also gave the first satisfactory proofs of the fundamental theorem of algebra and of the quadratic reciprocity law.

This century saw the development of the two forms of non-Euclidean geometry, where the parallel postulate of Euclidean geometry no longer holds.
The Russian mathematician Nikolai Ivanovich Lobachevsky and his rival, the Hungarian mathematician János Bolyai, independently defined and studied hyperbolic geometry, where uniqueness of parallels no longer holds. In this geometry the sum of angles in a triangle add up to less than 180°. Elliptic geometry was developed later in the 19th century by the German mathematician Bernhard Riemann; here no parallel can be found and the angles in a triangle add up to more than 180°. Riemann also developed Riemannian geometry, which unifies and vastly generalizes the three types of geometry, and he defined the concept of a manifold, which generalizes the ideas of curves and surfaces, and set the mathematical foundations for the theory of general relativity.
The 19th century saw the beginning of a great deal of abstract algebra. Hermann Grassmann in Germany gave a first version of vector spaces, William Rowan Hamilton in Ireland developed noncommutative algebra. The British mathematician George Boole devised an algebra that soon evolved into what is now called Boolean algebra, in which the only numbers were 0 and 1. Boolean algebra is the starting point of mathematical logic and has important applications in electrical engineering and computer science. 
Augustin-Louis Cauchy, Bernhard Riemann, and Karl Weierstrass reformulated the calculus in a more rigorous fashion.
Also, for the first time, the limits of mathematics were explored. Niels Henrik Abel, a Norwegian, and Évariste Galois, a Frenchman, proved that there is no general algebraic method for solving polynomial equations of degree greater than four (Abel–Ruffini theorem). Other 19th-century mathematicians used this in their proofs that straight edge and compass alone are not sufficient to trisect an arbitrary angle, to construct the side of a cube twice the volume of a given cube, nor to construct a square equal in area to a given circle. Mathematicians had vainly attempted to solve all of these problems since the time of the ancient Greeks. On the other hand, the limitation of three dimensions in geometry was surpassed in the 19th century through considerations of parameter space and hypercomplex numbers.
Abel and Galois's investigations into the solutions of various polynomial equations laid the groundwork for further developments of group theory, and the associated fields of abstract algebra. In the 20th century physicists and other scientists have seen group theory as the ideal way to study symmetry.
In the later 19th century, Georg Cantor established the first foundations of set theory, which enabled the rigorous treatment of the notion of infinity and has become the common language of nearly all mathematics. Cantor's set theory, and the rise of mathematical logic in the hands of Peano, L.E.J. Brouwer, David Hilbert, Bertrand Russell, and A.N. Whitehead, initiated a long running debate on the foundations of mathematics.
The 19th century saw the founding of a number of national mathematical societies: the London Mathematical Society in 1865, the Société Mathématique de France in 1872, the Circolo Matematico di Palermo in 1884, the Edinburgh Mathematical Society in 1883, and the American Mathematical Society in 1888. The first international, special-interest society, the Quaternion Society, was formed in 1899, in the context of a vector controversy.
In 1897, Kurt Hensel introduced p-adic numbers.


=== 20th century ===
The 20th century saw mathematics become a major profession. By the end of the century, thousands of new Ph.D.s in mathematics were being awarded every year, and jobs were available in both teaching and industry. An effort to catalogue the areas and applications of mathematics was undertaken in Klein's encyclopedia.
In a 1900 speech to the International Congress of Mathematicians, David Hilbert set out a list of 23 unsolved problems in mathematics. These problems, spanning many areas of mathematics, formed a central focus for much of 20th-century mathematics. Today, 10 have been solved, 7 are partially solved, and 2 are still open. The remaining 4 are too loosely formulated to be stated as solved or not.

Notable historical conjectures were finally proven. In 1976, Wolfgang Haken and Kenneth Appel proved the four color theorem, controversial at the time for the use of a computer to do so. Andrew Wiles, building on the work of others, proved Fermat's Last Theorem in 1995. Paul Cohen and Kurt Gödel proved that the continuum hypothesis is independent of (could neither be proved nor disproved from) the standard axioms of set theory. In 1998, Thomas Callister Hales proved the Kepler conjecture, also using a computer.
Mathematical collaborations of unprecedented size and scope took place. An example is the classification of finite simple groups (also called the ""enormous theorem""), whose proof between 1955 and 2004 required 500-odd journal articles by about 100 authors, and filling tens of thousands of pages. A group of French mathematicians, including Jean Dieudonné and André Weil, publishing under the pseudonym ""Nicolas Bourbaki"", attempted to exposit all of known mathematics as a coherent rigorous whole. The resulting several dozen volumes has had a controversial influence on mathematical education.

Differential geometry came into its own when Albert Einstein used it in general relativity. Entirely new areas of mathematics such as mathematical logic, topology, and John von Neumann's game theory changed the kinds of questions that could be answered by mathematical methods. All kinds of structures were abstracted using axioms and given names like metric spaces, topological spaces etc. As mathematicians do, the concept of an abstract structure was itself abstracted and led to category theory. Grothendieck and Serre recast algebraic geometry using sheaf theory. Large advances were made in the qualitative study of dynamical systems that Poincaré had begun in the 1890s.
Measure theory was developed in the late 19th and early 20th centuries. Applications of measures include the Lebesgue integral, Kolmogorov's axiomatisation of probability theory, and ergodic theory. Knot theory greatly expanded. Quantum mechanics led to the development of functional analysis. Other new areas include Laurent Schwartz's distribution theory, fixed point theory, singularity theory and René Thom's catastrophe theory, model theory, and Mandelbrot's fractals. Lie theory with its Lie groups and Lie algebras became one of the major areas of study.
Non-standard analysis, introduced by Abraham Robinson, rehabilitated the infinitesimal approach to calculus, which had fallen into disrepute in favour of the theory of limits, by extending the field of real numbers to the Hyperreal numbers which include infinitesimal and infinite quantities. An even larger number system, the surreal numbers were discovered by John Horton Conway in connection with combinatorial games.
The development and continual improvement of computers, at first mechanical analog machines and then digital electronic machines, allowed industry to deal with larger and larger amounts of data to facilitate mass production and distribution and communication, and new areas of mathematics were developed to deal with this: Alan Turing's computability theory; complexity theory; Derrick Henry Lehmer's use of ENIAC to further number theory and the Lucas–Lehmer primality test; Rózsa Péter's recursive function theory; Claude Shannon's information theory; signal processing; data analysis; optimization and other areas of operations research. In the preceding centuries much mathematical focus was on calculus and continuous functions, but the rise of computing and communication networks led to an increasing importance of discrete concepts and the expansion of combinatorics including graph theory. The speed and data processing abilities of computers also enabled the handling of mathematical problems that were too time-consuming to deal with by pencil and paper calculations, leading to areas such as numerical analysis and symbolic computation. Some of the most important methods and algorithms of the 20th century are: the simplex algorithm, the fast Fourier transform, error-correcting codes, the Kalman filter from control theory and the RSA algorithm of public-key cryptography.
At the same time, deep insights were made about the limitations to mathematics. In 1929 and 1930, it was proved the truth or falsity of all statements formulated about the natural numbers plus either addition or multiplication (but not both), was decidable, i.e. could be determined by some algorithm. In 1931, Kurt Gödel found that this was not the case for the natural numbers plus both addition and multiplication; this system, known as Peano arithmetic, was in fact incomplete. (Peano arithmetic is adequate for a good deal of number theory, including the notion of prime number.) A consequence of Gödel's two incompleteness theorems is that in any mathematical system that includes Peano arithmetic (including all of analysis and geometry), truth necessarily outruns proof, i.e. there are true statements that cannot be proved within the system. Hence mathematics cannot be reduced to mathematical logic, and David Hilbert's dream of making all of mathematics complete and consistent needed to be reformulated.

One of the more colorful figures in 20th-century mathematics was Srinivasa Aiyangar Ramanujan (1887–1920), an Indian autodidact who conjectured or proved over 3000 theorems, including properties of highly composite numbers, the partition function and its asymptotics, and mock theta functions. He also made major investigations in the areas of gamma functions, modular forms, divergent series, hypergeometric series and prime number theory.
Paul Erdős published more papers than any other mathematician in history, working with hundreds of collaborators. Mathematicians have a game equivalent to the Kevin Bacon Game, which leads to the Erdős number of a mathematician. This describes the ""collaborative distance"" between a person and Erdős, as measured by joint authorship of mathematical papers.
Emmy Noether has been described by many as the most important woman in the history of mathematics. She studied the theories of rings, fields, and algebras.
As in most areas of study, the explosion of knowledge in the scientific age has led to specialization: by the end of the century, there were hundreds of specialized areas in mathematics, and the Mathematics Subject Classification was dozens of pages long. More and more mathematical journals were published and, by the end of the century, the development of the World Wide Web led to online publishing.


=== 21st century ===

In 2000, the Clay Mathematics Institute announced the seven Millennium Prize Problems. In 2003 the Poincaré conjecture was solved by Grigori Perelman (who declined to accept an award, as he was critical of the mathematics establishment).
Most mathematical journals now have online versions as well as print versions, and many online-only journals are launched. There is an increasing drive toward open access publishing, first made popular by arXiv.


== Future ==

There are many observable trends in mathematics, the most notable being that the subject is growing ever larger as computers are ever more important and powerful; the volume of data being produced by science and industry, facilitated by computers, continues expanding exponentially. As a result, there is a corresponding growth in the demand for mathematics to help process and understand this big data. Math science careers are also expected to continue to grow, with the US Bureau of Labor Statistics estimating (in 2018) that ""employment of mathematical science occupations is projected to grow 27.9 percent from 2016 to 2026.""


== See also ==


== Notes ==


== References ==
de Crespigny, Rafe (2007), A Biographical Dictionary of Later Han to the Three Kingdoms (23–220 AD), Leiden: Koninklijke Brill, ISBN 978-90-04-15605-0.
Berggren, Lennart; Borwein, Jonathan M.; Borwein, Peter B. (2004), Pi: A Source Book, New York: Springer, ISBN 978-0-387-20571-7
Boyer, C.B. (1991) [1989], A History of Mathematics (2nd ed.), New York: Wiley, ISBN 978-0-471-54397-8
Cuomo, Serafina (2001), Ancient Mathematics, London: Routledge, ISBN 978-0-415-16495-5
Goodman, Michael, K.J. (2016), An introduction of the Early Development of Mathematics, Hoboken: Wiley, ISBN 978-1-119-10497-1{{citation}}:  CS1 maint: multiple names: authors list (link)
Gullberg, Jan (1997), Mathematics: From the Birth of Numbers, New York: W.W. Norton and Company, ISBN 978-0-393-04002-9
Joyce, Hetty (July 1979), ""Form, Function and Technique in the Pavements of Delos and Pompeii"", American Journal of Archaeology, 83 (3): 253–63, doi:10.2307/505056, JSTOR 505056, S2CID 191394716.
Katz, Victor J. (1998), A History of Mathematics: An Introduction (2nd ed.), Addison-Wesley, ISBN 978-0-321-01618-8
Katz, Victor J. (2007), The Mathematics of Egypt, Mesopotamia, China, India, and Islam: A Sourcebook, Princeton, NJ: Princeton University Press, ISBN 978-0-691-11485-9
Needham, Joseph; Wang, Ling (1995) [1959], Science and Civilization in China: Mathematics and the Sciences of the Heavens and the Earth, vol. 3, Cambridge: Cambridge University Press, ISBN 978-0-521-05801-8
Needham, Joseph; Wang, Ling (2000) [1965], Science and Civilization in China: Physics and Physical Technology: Mechanical Engineering, vol. 4 (reprint ed.), Cambridge: Cambridge University Press, ISBN 978-0-521-05803-2
Sleeswyk, Andre (October 1981), ""Vitruvius' odometer"", Scientific American, 252 (4): 188–200, Bibcode:1981SciAm.245d.188S, doi:10.1038/scientificamerican1081-188.
Straffin, Philip D. (1998), ""Liu Hui and the First Golden Age of Chinese Mathematics"", Mathematics Magazine, 71 (3): 163–81, doi:10.1080/0025570X.1998.11996627
Tang, Birgit (2005), Delos, Carthage, Ampurias: the Housing of Three Mediterranean Trading Centres, Rome: L'Erma di Bretschneider (Accademia di Danimarca), ISBN 978-88-8265-305-7.
Volkov, Alexei (2009), ""Mathematics and Mathematics Education in Traditional Vietnam"", in Robson, Eleanor; Stedall, Jacqueline (eds.), The Oxford Handbook of the History of Mathematics, Oxford: Oxford University Press, pp. 153–76, ISBN 978-0-19-921312-2


== Further reading ==


=== General ===
Aaboe, Asger (1964). Episodes from the Early History of Mathematics. New York: Random House.
Bell, E. T. (1937). Men of Mathematics. Simon and Schuster.
Burton, David M. (1997). The History of Mathematics: An Introduction. McGraw Hill.
Grattan-Guinness, Ivor (2003). Companion Encyclopedia of the History and Philosophy of the Mathematical Sciences. The Johns Hopkins University Press. ISBN 978-0-8018-7397-3.
Kline, Morris. Mathematical Thought from Ancient to Modern Times.
Struik, D. J. (1987). A Concise History of Mathematics, fourth revised edition. Dover Publications, New York.


=== Books on a specific period ===
Gillings, Richard J. (1972). Mathematics in the Time of the Pharaohs. Cambridge, MA: MIT Press.
Heath, Thomas Little (1921). A History of Greek Mathematics. Oxford, Claredon Press.
van der Waerden, B. L. (1983). Geometry and Algebra in Ancient Civilizations, Springer, ISBN 0-387-12159-5.


=== Books on a specific topic ===
Corry, Leo (2015), A Brief History of Numbers, Oxford University Press, ISBN 978-0198702597
Hoffman, Paul (1998). The Man Who Loved Only Numbers: The Story of Paul Erdős and the Search for Mathematical Truth. Hyperion. ISBN 0-7868-6362-5.
Menninger, Karl W. (1969). Number Words and Number Symbols: A Cultural History of Numbers. MIT Press. ISBN 978-0-262-13040-0.
Stigler, Stephen M. (1990). The History of Statistics: The Measurement of Uncertainty before 1900. Belknap Press. ISBN 978-0-674-40341-3.


== External links ==


=== Documentaries ===
BBC (2008). The Story of Maths.
Renaissance Mathematics, BBC Radio 4 discussion with Robert Kaplan, Jim Bennett & Jackie Stedall (In Our Time, Jun 2, 2005)


=== Educational material ===
MacTutor History of Mathematics archive (John J. O'Connor and Edmund F. Robertson; University of St Andrews, Scotland). An award-winning website containing detailed biographies on many historical and contemporary mathematicians, as well as information on notable curves and various topics in the history of mathematics.
History of Mathematics Home Page (David E. Joyce; Clark University). Articles on various topics in the history of mathematics with an extensive bibliography.
The History of Mathematics (David R. Wilkins; Trinity College, Dublin). Collections of material on the mathematics between the 17th and 19th century.
Earliest Known Uses of Some of the Words of Mathematics (Jeff Miller). Contains information on the earliest known uses of terms used in mathematics.
Earliest Uses of Various Mathematical Symbols (Jeff Miller). Contains information on the history of mathematical notations.
Mathematical Words: Origins and Sources (John Aldrich, University of Southampton) Discusses the origins of the modern mathematical word stock.
Biographies of Women Mathematicians (Larry Riddle; Agnes Scott College).
Mathematicians of the African Diaspora (Scott W. Williams; University at Buffalo).
Notes for MAA minicourse: teaching a course in the history of mathematics. (2009) (V. Frederick Rickey & Victor J. Katz).
Ancient Rome: The Odometer Of Vitruv. Pictorial (moving) re-construction of Vitusius' Roman ododmeter.


=== Bibliographies ===
A Bibliography of Collected Works and Correspondence of Mathematicians archive dated 2007/3/17 (Steven W. Rockey; Cornell University Library).


=== Organizations ===
International Commission for the History of Mathematics


=== Journals ===
Historia Mathematica
Convergence Archived 2020-09-08 at the Wayback Machine, the Mathematical Association of America's online Math History Magazine
History of Mathematics Archived 2006-10-04 at the Wayback Machine Math Archives (University of Tennessee, Knoxville)
History/Biography The Math Forum (Drexel University)
History of Mathematics (Courtright Memorial Library).
History of Mathematics Web Sites Archived 2009-05-25 at the Wayback Machine (David Calvis; Baldwin-Wallace College)
Historia de las Matemáticas (Universidad de La La guna)
História da Matemática (Universidade de Coimbra)
Using History in Math Class
Mathematical Resources: History of Mathematics (Bruno Kevius)
History of Mathematics (Roberta Tucci)"
Interest,"In finance and economics, interest is payment from a debtor or deposit-taking financial institution to a lender or depositor of an amount above repayment of the principal sum (that is, the amount borrowed), at a particular rate. It is distinct from a fee which the borrower may pay to the lender or some third party. It is also distinct from dividend which is paid by a company to its shareholders (owners) from its profit or reserve, but not at a particular rate decided beforehand, rather on a pro rata basis as a share in the reward gained by risk taking entrepreneurs when the revenue earned exceeds the total costs.
For example, a customer would usually pay interest to borrow from a bank, so they pay the bank an amount which is more than the amount they borrowed; or a customer may earn interest on their savings, and so they may withdraw more than they originally deposited. In the case of savings, the customer is the lender, and the bank plays the role of the borrower.
Interest differs from profit, in that interest is received by a lender, whereas profit is received by the owner of an asset, investment or enterprise. (Interest may be part or the whole of the profit on an investment, but the two concepts are distinct from each other from an accounting perspective.)
The rate of interest is equal to the interest amount paid or received over a particular period divided by the principal sum borrowed or lent (usually expressed as a percentage).
Compound interest means that interest is earned on prior interest in addition to the principal. Due to compounding, the total amount of debt grows exponentially, and its mathematical study led to the discovery of the number e. In practice, interest is most often calculated on a daily, monthly, or yearly basis, and its impact is influenced greatly by its compounding rate.


== History ==

Credit is thought to have preceded the existence of coinage by several thousands of years. The first recorded instance of credit is a collection of old Sumerian documents from 3000 BC that show systematic use of credit to loan both grain and metals. The rise of interest as a concept is unknown, though its use in Sumeria argue that it was well established as a concept by 3000BC if not earlier, with historians believing that the concept in its modern sense may have arisen from the lease of animal or seeds for productive purposes. The argument that acquired seeds and animals could reproduce themselves was used to justify interest, but ancient Jewish religious prohibitions against usury (נשך NeSheKh) represented a ""different view"".
The first written evidence of compound interest dates roughly 2400 BC. The annual interest rate was roughly 20%. Compound interest was necessary for the development of agriculture and important for urbanization. 
While the traditional Middle Eastern views on interest were the result of the urbanized, economically developed character of the societies that produced them, the new Jewish prohibition on interest showed a pastoral, tribal influence. In the early 2nd millennium BC, since silver used in exchange for livestock or grain could not multiply of its own, the Laws of Eshnunna instituted a legal interest rate, specifically on deposits of dowry. Early Muslims called this riba, translated today as the charging of interest.
The First Council of Nicaea, in 325, forbade clergy from engaging in usury which was defined as lending on interest above 1 percent per month (12.7% AER). Ninth-century ecumenical councils applied this regulation to the laity. Catholic Church opposition to interest hardened in the era of the Scholastics, when even defending it was considered a heresy. St. Thomas Aquinas, the leading theologian of the Catholic Church, argued that the charging of interest is wrong because it amounts to ""double charging"", charging for both the thing and the use of the thing.
In the medieval economy, loans were entirely a consequence of necessity (bad harvests, fire in a workplace) and, under those conditions, it was considered morally reproachable to charge interest. It was also considered morally dubious, since no goods were produced through the lending of money, and thus it should not be compensated, unlike other activities with direct physical output such as blacksmithing or farming. For the same reason, interest has often been looked down upon in Islamic civilization, with almost all scholars agreeing that the Qur'an explicitly forbids charging interest.
Medieval jurists developed several financial instruments to encourage responsible lending and circumvent prohibitions on usury, such as the Contractum trinius.

In the Renaissance era, greater mobility of people facilitated an increase in commerce and the appearance of appropriate conditions for entrepreneurs to start new, lucrative businesses. Given that borrowed money was no longer strictly for consumption but for production as well, interest was no longer viewed in the same manner.
The first attempt to control interest rates through manipulation of the money supply was made by the Banque de France in 1847.


=== Islamic finance ===

The latter half of the 20th century saw the rise of interest-free Islamic banking and finance, a movement that applies Islamic law to financial institutions and the economy. Some countries, including Iran, Sudan, and Pakistan, have taken steps to eradicate interest from their financial systems. Rather than charging interest, the interest-free lender shares the risk by investing as a partner in profit loss sharing scheme, because predetermined loan repayment as interest is prohibited, as well as making money out of money is unacceptable. All financial transactions must be asset-backed and must not charge any interest or fee for the service of lending.


=== In the history of mathematics ===
It is thought that Jacob Bernoulli discovered the mathematical constant e by studying a question about compound interest. He realized that if an account that starts with $1.00 and pays say 100% interest per year, at the end of the year, the value is $2.00; but if the interest is computed and added twice in the year, the $1 is multiplied by 1.5 twice, yielding $1.00×1.52 = $2.25. Compounding quarterly yields $1.00×1.254 = $2.4414..., and so on.
Bernoulli noticed that if the frequency of compounding is increased without limit, this sequence can be modeled as follows:

  
    
      
        
          lim
          
            n
            →
            ∞
          
        
        
          
            (
            
              1
              +
              
                
                  
                    1
                    n
                  
                
              
            
            )
          
          
            n
          
        
        =
        e
        ,
      
    
    {\displaystyle \lim _{n\rightarrow \infty }\left(1+{\dfrac {1}{n}}\right)^{n}=e,}
  

where n is the number of times the interest is to be compounded in a year.


== Economics ==
In economics, the rate of interest is the price of credit, and it plays the role of the cost of capital. In a free market economy, interest rates are subject to the law of supply and demand of the money supply, and one explanation of the tendency of interest rates to be generally greater than zero is the scarcity of loanable funds.
Over centuries, various schools of thought have developed explanations of interest and interest rates. The School of Salamanca justified paying interest in terms of the benefit to the borrower, and interest received by the lender in terms of a premium for the risk of default. In the sixteenth century, Martín de Azpilcueta applied a time preference argument: it is preferable to receive a given good now rather than in the future. Accordingly, interest is compensation for the time the lender forgoes the benefit of spending the money.
On the question of why interest rates are normally greater than zero, in 1770, French economist Anne-Robert-Jacques Turgot, Baron de Laune proposed the theory of fructification. By applying an opportunity cost argument, comparing the loan rate with the rate of return on agricultural land, and a mathematical argument, applying the formula for the value of a perpetuity to a plantation, he argued that the land value would rise without limit, as the interest rate approached zero. For the land value to remain positive and finite keeps the interest rate above zero.
Adam Smith, Carl Menger, and Frédéric Bastiat also propounded theories of interest rates. In the late 19th century, Swedish economist Knut Wicksell in his 1898 Interest and Prices elaborated a comprehensive theory of economic crises based upon a distinction between natural and nominal interest rates. In the 1930s, Wicksell's approach was refined by Bertil Ohlin and Dennis Robertson and became known as the loanable funds theory. Other notable interest rate theories of the period are those of Irving Fisher and John Maynard Keynes.


== Calculation ==


=== Simple interest ===
Simple interest is calculated only on the principal amount, or on that portion of the principal amount that remains. It excludes the effect of compounding. Simple interest can be applied over a time period other than a year, for example, every month.
Simple interest is calculated according to the following formula:

  
    
      
        
          
            
              r
              ⋅
              B
              ⋅
              m
            
            n
          
        
      
    
    {\displaystyle {\frac {r\cdot B\cdot m}{n}}}
  

where 

r is the simple annual interest rate
B is the initial balance
m is the number of time periods elapsed and
n is the frequency of applying interest.
For example, imagine that a credit card holder has an outstanding balance of $2500 and that the simple annual interest rate is 12.99% per annum, applied monthly, so the frequency of applying interest is 12 per year. Over one month,

  
    
      
        
          
            
              0.1299
              ×
              $
              2500
            
            12
          
        
        =
        $
        27.06
      
    
    {\displaystyle {\frac {0.1299\times \$2500}{12}}=\$27.06}
  

interest is due (rounded to the nearest cent).
Simple interest applied over 3 months would be

  
    
      
        
          
            
              0.1299
              ×
              $
              2500
              ×
              3
            
            12
          
        
        =
        $
        81.19
      
    
    {\displaystyle {\frac {0.1299\times \$2500\times 3}{12}}=\$81.19}
  

If the card holder pays off only interest at the end of each of the 3 months, the total amount of interest paid would be

  
    
      
        
          
            
              0.1299
              ×
              $
              2500
            
            12
          
        
        ×
        3
        =
        $
        27.06
        
           per month
        
        ×
        3
        
           months
        
        =
        $
        81.18
      
    
    {\displaystyle {\frac {0.1299\times \$2500}{12}}\times 3=\$27.06{\text{ per month}}\times 3{\text{ months}}=\$81.18}
  

which is the simple interest applied over 3 months, as calculated above. (The one cent difference arises due to rounding to the nearest cent.)


=== Compound interest ===

Compound interest includes interest earned on the interest that was previously accumulated.
Compare, for example, a bond paying 6 percent semiannually (that is, coupons of 3 percent twice a year) with a certificate of deposit (GIC) that pays 6 percent interest once a year. The total interest payment is $6 per $100 par value in both cases, but the holder of the semiannual bond receives half the $6 per year after only 6 months (time preference), and so has the opportunity to reinvest the first $3 coupon payment after the first 6 months, and earn additional interest.
For example, suppose an investor buys $10,000 par value of a US dollar bond, which pays coupons twice a year, and that the bond's simple annual coupon rate is 6 percent per year. This means that every 6 months, the issuer pays the holder of the bond a coupon of 3 dollars per 100 dollars par value. At the end of 6 months, the issuer pays the holder:

  
    
      
        
          
            
              r
              ⋅
              B
              ⋅
              m
            
            n
          
        
        =
        
          
            
              6
              %
              ×
              $
              10
              
              000
              ×
              1
            
            2
          
        
        =
        $
        300
      
    
    {\displaystyle {\frac {r\cdot B\cdot m}{n}}={\frac {6\%\times \$10\,000\times 1}{2}}=\$300}
  

Assuming the market price of the bond is 100, so it is trading at par value, suppose further that the holder immediately reinvests the coupon by spending it on another $300 par value of the bond. In total, the investor therefore now holds:

  
    
      
        $
        10
        
        000
        +
        $
        300
        =
        
          (
          
            1
            +
            
              
                r
                n
              
            
          
          )
        
        ⋅
        B
        =
        
          (
          
            1
            +
            
              
                
                  6
                  %
                
                2
              
            
          
          )
        
        ×
        $
        10
        
        000
      
    
    {\displaystyle \$10\,000+\$300=\left(1+{\frac {r}{n}}\right)\cdot B=\left(1+{\frac {6\%}{2}}\right)\times \$10\,000}
  

and so earns a coupon at the end of the next 6 months of:

  
    
      
        
          
            
              
                
                  
                    
                      r
                      ⋅
                      B
                      ⋅
                      m
                    
                    n
                  
                
              
              
                
                =
                
                  
                    
                      6
                      %
                      ×
                      
                        (
                        
                          $
                          10
                          
                          000
                          +
                          $
                          300
                        
                        )
                      
                    
                    2
                  
                
              
            
            
              
              
                
                =
                
                  
                    
                      6
                      %
                      ×
                      
                        (
                        
                          1
                          +
                          
                            
                              
                                6
                                %
                              
                              2
                            
                          
                        
                        )
                      
                      ×
                      $
                      10
                      
                      000
                    
                    2
                  
                
              
            
            
              
              
                
                =
                $
                309
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}{\frac {r\cdot B\cdot m}{n}}&={\frac {6\%\times \left(\$10\,000+\$300\right)}{2}}\\&={\frac {6\%\times \left(1+{\frac {6\%}{2}}\right)\times \$10\,000}{2}}\\&=\$309\end{aligned}}}
  

Assuming the bond remains priced at par, the investor accumulates at the end of a full 12 months a total value of:

  
    
      
        
          
            
              
                $
                10
                ,
                000
                +
                $
                300
                +
                $
                309
              
              
                
                =
                $
                10
                
                000
                +
                
                  
                    
                      6
                      %
                      ×
                      $
                      10
                      ,
                      000
                    
                    2
                  
                
                +
                
                  
                    
                      6
                      %
                      ×
                      
                        (
                        
                          1
                          +
                          
                            
                              
                                6
                                %
                              
                              2
                            
                          
                        
                        )
                      
                      ×
                      $
                      10
                      
                      000
                    
                    2
                  
                
              
            
            
              
              
                
                =
                $
                10
                
                000
                ×
                
                  
                    (
                    
                      1
                      +
                      
                        
                          
                            6
                            %
                          
                          2
                        
                      
                    
                    )
                  
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\$10,000+\$300+\$309&=\$10\,000+{\frac {6\%\times \$10,000}{2}}+{\frac {6\%\times \left(1+{\frac {6\%}{2}}\right)\times \$10\,000}{2}}\\&=\$10\,000\times \left(1+{\frac {6\%}{2}}\right)^{2}\end{aligned}}}
  

and the investor earned in total:

  
    
      
        
          
            
              
                $
                10
                
                000
                ×
                
                  
                    (
                    
                      1
                      +
                      
                        
                          
                            6
                            %
                          
                          2
                        
                      
                    
                    )
                  
                  
                    2
                  
                
                −
                $
                10
                
                000
              
            
            
              
                =
                $
                10
                
                000
                ×
                
                  (
                  
                    
                      
                        (
                        
                          1
                          +
                          
                            
                              
                                6
                                %
                              
                              2
                            
                          
                        
                        )
                      
                      
                        2
                      
                    
                    −
                    1
                  
                  )
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\$10\,000\times \left(1+{\frac {6\%}{2}}\right)^{2}-\$10\,000\\=\$10\,000\times \left(\left(1+{\frac {6\%}{2}}\right)^{2}-1\right)\end{aligned}}}
  

The formula for the annual equivalent compound interest rate is:

  
    
      
        
          
            (
            
              1
              +
              
                
                  r
                  n
                
              
            
            )
          
          
            n
          
        
        −
        1
      
    
    {\displaystyle \left(1+{\frac {r}{n}}\right)^{n}-1}
  

where

r is the simple annual rate of interest
n is the frequency of applying interest
For example, in the case of a 6% simple annual rate, the annual equivalent compound rate is:

  
    
      
        
          
            (
            
              1
              +
              
                
                  
                    6
                    %
                  
                  2
                
              
            
            )
          
          
            2
          
        
        −
        1
        =
        
          1.03
          
            2
          
        
        −
        1
        =
        6.09
        %
      
    
    {\displaystyle \left(1+{\frac {6\%}{2}}\right)^{2}-1=1.03^{2}-1=6.09\%}
  


=== Other formulations ===
The outstanding balance Bn of a loan after n regular payments increases each period by a growth factor according to the periodic interest, and then decreases by the amount paid p at the end of each period:

  
    
      
        
          B
          
            n
          
        
        =
        
          
            (
          
        
        1
        +
        r
        
          
            )
          
        
        
          B
          
            n
            −
            1
          
        
        −
        p
        ,
      
    
    {\displaystyle B_{n}={\big (}1+r{\big )}B_{n-1}-p,}
  

where

i = simple annual loan rate in decimal form (for example, 10% = 0.10. The loan rate is the rate used to compute payments and balances.)
r = period interest rate (for example, i/12 for monthly payments) [2]
B0 = initial balance, which equals the principal sum
By repeated substitution, one obtains expressions for Bn, which are linearly proportional to B0 and p, and use of the formula for the partial sum of a geometric series results in

  
    
      
        
          B
          
            n
          
        
        =
        (
        1
        +
        r
        
          )
          
            n
          
        
        
          B
          
            0
          
        
        −
        
          
            
              (
              1
              +
              r
              
                )
                
                  n
                
              
              −
              1
            
            r
          
        
        p
      
    
    {\displaystyle B_{n}=(1+r)^{n}B_{0}-{\frac {(1+r)^{n}-1}{r}}p}
  

A solution of this expression for p in terms of B0 and Bn reduces to

  
    
      
        p
        =
        r
        
          [
          
            
              
                (
                1
                +
                r
                
                  )
                  
                    n
                  
                
                
                  B
                  
                    0
                  
                
                −
                
                  B
                  
                    n
                  
                
              
              
                (
                1
                +
                r
                
                  )
                  
                    n
                  
                
                −
                1
              
            
          
          ]
        
      
    
    {\displaystyle p=r\left[{\frac {(1+r)^{n}B_{0}-B_{n}}{(1+r)^{n}-1}}\right]}
  

To find the payment if the loan is to be finished in n payments, one sets Bn = 0.
The PMT function found in spreadsheet programs can be used to calculate the monthly payment of a loan:

  
    
      
        p
        =
        
          P
          M
          T
        
        (
        
          rate
        
        ,
        
          num
        
        ,
        
          PV
        
        ,
        
          FV
        
        ,
        )
        =
        
          P
          M
          T
        
        (
        r
        ,
        n
        ,
        −
        
          B
          
            0
          
        
        ,
        
          B
          
            n
          
        
        ,
        )
      
    
    {\displaystyle p=\mathrm {PMT} ({\text{rate}},{\text{num}},{\text{PV}},{\text{FV}},)=\mathrm {PMT} (r,n,-B_{0},B_{n},)}
  

An interest-only payment on the current balance would be

  
    
      
        
          p
          
            I
          
        
        =
        r
        B
        .
      
    
    {\displaystyle p_{I}=rB.}
  

The total interest, IT, paid on the loan is

  
    
      
        
          I
          
            T
          
        
        =
        n
        p
        −
        
          B
          
            0
          
        
        .
      
    
    {\displaystyle I_{T}=np-B_{0}.}
  

The formulas for a regular savings program are similar, but the payments are added to the balances instead of being subtracted, and the formula for the payment is the negative of the one above. These formulas are only approximate since actual loan balances are affected by rounding. To avoid an underpayment at the end of the loan, the payment must be rounded up to the next cent.
Consider a similar loan but with a new period equal to k periods of the problem above. If rk and pk are the new rate and payment, we now have

  
    
      
        
          B
          
            k
          
        
        =
        
          B
          
            0
          
          ′
        
        =
        (
        1
        +
        
          r
          
            k
          
        
        )
        
          B
          
            0
          
        
        −
        
          p
          
            k
          
        
        .
      
    
    {\displaystyle B_{k}=B'_{0}=(1+r_{k})B_{0}-p_{k}.}
  

Comparing this with the expression for Bk above, we note that

  
    
      
        
          r
          
            k
          
        
        =
        (
        1
        +
        r
        
          )
          
            k
          
        
        −
        1
      
    
    {\displaystyle r_{k}=(1+r)^{k}-1}
  

and

  
    
      
        
          p
          
            k
          
        
        =
        
          
            p
            r
          
        
        
          r
          
            k
          
        
        .
      
    
    {\displaystyle p_{k}={\frac {p}{r}}r_{k}.}
  

The last equation allows us to define a constant that is the same for both problems:

  
    
      
        
          B
          
            ∗
          
        
        =
        
          
            p
            r
          
        
        =
        
          
            
              p
              
                k
              
            
            
              r
              
                k
              
            
          
        
      
    
    {\displaystyle B^{*}={\frac {p}{r}}={\frac {p_{k}}{r_{k}}}}
  

and Bk can be written as

  
    
      
        
          B
          
            k
          
        
        =
        (
        1
        +
        
          r
          
            k
          
        
        )
        
          B
          
            0
          
        
        −
        
          r
          
            k
          
        
        
          B
          
            ∗
          
        
        .
      
    
    {\displaystyle B_{k}=(1+r_{k})B_{0}-r_{k}B^{*}.}
  

Solving for rk, we find a formula for rk involving known quantities and Bk, the balance after k periods:

  
    
      
        
          r
          
            k
          
        
        =
        
          
            
              
                B
                
                  0
                
              
              −
              
                B
                
                  k
                
              
            
            
              
                B
                
                  ∗
                
              
              −
              
                B
                
                  0
                
              
            
          
        
      
    
    {\displaystyle r_{k}={\frac {B_{0}-B_{k}}{B^{*}-B_{0}}}}
  
.
Since B0 could be any balance in the loan, the formula works for any two balances separate by k periods and can be used to compute a value for the annual interest rate.
B* is a scale invariant, since it does not change with changes in the length of the period.
Rearranging the equation for B*, one obtains a transformation coefficient (scale factor):

  
    
      
        
          λ
          
            k
          
        
        =
        
          
            
              p
              
                k
              
            
            p
          
        
        =
        
          
            
              r
              
                k
              
            
            r
          
        
        =
        
          
            
              (
              1
              +
              r
              
                )
                
                  k
                
              
              −
              1
            
            r
          
        
        =
        k
        
          [
          
            1
            +
            
              
                
                  (
                  k
                  −
                  1
                  )
                  r
                
                2
              
            
            +
            ⋯
          
          ]
        
      
    
    {\displaystyle \lambda _{k}={\frac {p_{k}}{p}}={\frac {r_{k}}{r}}={\frac {(1+r)^{k}-1}{r}}=k\left[1+{\frac {(k-1)r}{2}}+\cdots \right]}
  
 (see binomial theorem)
and we see that r and p transform in the same manner:

  
    
      
        
          r
          
            k
          
        
        =
        
          λ
          
            k
          
        
        r
      
    
    {\displaystyle r_{k}=\lambda _{k}r}
  

  
    
      
        
          p
          
            k
          
        
        =
        
          λ
          
            k
          
        
        p
      
    
    {\displaystyle p_{k}=\lambda _{k}p}
  
.
The change in the balance transforms likewise:

  
    
      
        Δ
        
          B
          
            k
          
        
        =
        
          B
          ′
        
        −
        B
        =
        (
        
          λ
          
            k
          
        
        r
        B
        −
        
          λ
          
            k
          
        
        p
        )
        =
        
          λ
          
            k
          
        
        
        Δ
        B
      
    
    {\displaystyle \Delta B_{k}=B'-B=(\lambda _{k}rB-\lambda _{k}p)=\lambda _{k}\,\Delta B}
  
,
which gives an insight into the meaning of some of the coefficients found in the formulas above. The annual rate, r12, assumes only one payment per year and is not an ""effective"" rate for monthly payments. With monthly payments, the monthly interest is paid out of each payment and so should not be compounded, and an annual rate of 12·r would make more sense. If one just made interest-only payments, the amount paid for the year would be 12·r·B0.
Substituting pk = rk B* into the equation for the Bk, we obtain

  
    
      
        
          B
          
            k
          
        
        =
        
          B
          
            0
          
        
        −
        
          r
          
            k
          
        
        (
        
          B
          
            ∗
          
        
        −
        
          B
          
            0
          
        
        )
      
    
    {\displaystyle B_{k}=B_{0}-r_{k}(B^{*}-B_{0})}
  
.
Since Bn = 0, we can solve for B*: 

  
    
      
        
          B
          
            ∗
          
        
        =
        
          B
          
            0
          
        
        
          (
          
            
              
                1
                
                  r
                  
                    n
                  
                
              
            
            +
            1
          
          )
        
        .
      
    
    {\displaystyle B^{*}=B_{0}\left({\frac {1}{r_{n}}}+1\right).}
  

Substituting back into the formula for the Bk shows that they are a linear function of the rk and therefore the λk:

  
    
      
        
          B
          
            k
          
        
        =
        
          B
          
            0
          
        
        
          (
          
            1
            −
            
              
                
                  r
                  
                    k
                  
                
                
                  r
                  
                    n
                  
                
              
            
          
          )
        
        =
        
          B
          
            0
          
        
        
          (
          
            1
            −
            
              
                
                  λ
                  
                    k
                  
                
                
                  λ
                  
                    n
                  
                
              
            
          
          )
        
      
    
    {\displaystyle B_{k}=B_{0}\left(1-{\frac {r_{k}}{r_{n}}}\right)=B_{0}\left(1-{\frac {\lambda _{k}}{\lambda _{n}}}\right)}
  
.
This is the easiest way of estimating the balances if the λk are known. Substituting into the first formula for Bk above and solving for λk+1, we obtain

  
    
      
        
          λ
          
            k
            +
            1
          
        
        =
        1
        +
        (
        1
        +
        r
        )
        
          λ
          
            k
          
        
      
    
    {\displaystyle \lambda _{k+1}=1+(1+r)\lambda _{k}}
  
.
λ0 and λn can be found using the formula for λk above or computing the λk recursively from λ0 = 0 to λn.
Since p = rB*, the formula for the payment reduces to

  
    
      
        p
        =
        
          (
          
            r
            +
            
              
                1
                
                  λ
                  
                    n
                  
                
              
            
          
          )
        
        
          B
          
            0
          
        
      
    
    {\displaystyle p=\left(r+{\frac {1}{\lambda _{n}}}\right)B_{0}}
  

and the average interest rate over the period of the loan is

  
    
      
        
          r
          
            loan
          
        
        =
        
          
            
              I
              
                T
              
            
            
              n
              
                B
                
                  0
                
              
            
          
        
        =
        r
        +
        
          
            1
            
              λ
              
                n
              
            
          
        
        −
        
          
            1
            n
          
        
        ,
      
    
    {\displaystyle r_{\text{loan}}={\frac {I_{T}}{nB_{0}}}=r+{\frac {1}{\lambda _{n}}}-{\frac {1}{n}},}
  

which is less than r if n > 1.


== Discount instruments ==
US and Canadian T-Bills (short term Government debt) have a different calculation for interest. Their interest is calculated as (100 − P)/P where P is the price paid. Instead of normalizing it to a year, the interest is prorated by the number of days t: (365/t)·100. (See also: Day count convention). The total calculation is ((100 − P)/P)·((365/t)·100). This is equivalent to calculating the price by a process called discounting at a simple interest rate.


== Rules of thumb ==


=== Rule of 78s ===

In the age before electronic computers were widely available, flat rate consumer loans in the United States of America would be priced using the Rule of 78s, or ""sum of digits"" method. (The sum of the integers from 1 to 12 is 78.) The technique required only a simple calculation.
Payments remain constant over the life of the loan; however, payments are allocated to interest in progressively smaller amounts. In a one-year loan, in the first month, 12/78 of all interest owed over the life of the loan is due; in the second month, 11/78; progressing to the twelfth month where only 1/78 of all interest is due. The practical effect of the Rule of 78s is to make early pay-offs of term loans more expensive. For a one-year loan, approximately 3/4 of all interest due is collected by the sixth month, and pay-off of the principal then will cause the effective interest rate to be much higher than the APR used to calculate the payments.
In 1992, the United States outlawed the use of ""Rule of 78s"" interest in connection with mortgage refinancing and other consumer loans over five years in term. Certain other jurisdictions have outlawed application of the Rule of 78s in certain types of loans, particularly consumer loans.


=== Rule of 72 ===

To approximate how long it takes for money to double at a given interest rate, that is, for accumulated compound interest to reach or exceed the initial deposit, divide 72 by the percentage interest rate. For example, compounding at an annual interest rate of 6 percent, it will take 72/6 = 12 years for the money to double.
The rule provides a good indication for interest rates up to 10%.
In the case of an interest rate of 18 percent, the rule of 72 predicts that money will double after 72/18 = 4 years.

  
    
      
        
          1.18
          
            4
          
        
        =
        1.9388
        
           (4 d.p.)
        
      
    
    {\displaystyle 1.18^{4}=1.9388{\text{ (4 d.p.)}}}
  

In the case of an interest rate of 24 percent, the rule predicts that money will double after 72/24 = 3 years.

  
    
      
        
          1.24
          
            3
          
        
        =
        1.9066
        
           (4 d.p.)
        
      
    
    {\displaystyle 1.24^{3}=1.9066{\text{ (4 d.p.)}}}
  


== Market interest rates ==

There are markets for investments (which include the money market, bond market, as well as retail financial institutions like banks) that set interest rates. Each specific debt takes into account the following factors in determining its interest rate:


=== Opportunity cost and deferred consumption ===
Opportunity cost encompasses any other use to which the money could be put, including lending to others, investing elsewhere, holding cash, or spending the funds.
Charging interest equal to inflation preserves the lender's purchasing power, but does not compensate for the time value of money in real terms. The lender may prefer to invest in another product rather than consume. The return they might obtain from competing investments is a factor in determining the interest rate they demand.


=== Inflation ===
Since the lender is deferring consumption, they will wish, as a bare minimum, to recover enough to pay the increased cost of goods due to inflation. Because future inflation is unknown, there are three ways this might be achieved:

Charge X% interest ""plus inflation"" Many governments issue ""real-return"" or ""inflation indexed"" bonds. The principal amount or the interest payments are continually increased by the rate of inflation. See the discussion at real interest rate.
Decide on the ""expected"" inflation rate. This still leaves the lender exposed to the risk of ""unexpected"" inflation.
Allow the interest rate to be periodically changed. While a ""fixed interest rate"" remains the same throughout the life of the debt, ""variable"" or ""floating"" rates can be reset. There are derivative products that allow for hedging and swaps between the two.
However interest rates are set by the market, and it happens frequently that they are insufficient to compensate for inflation: for example at times of high inflation during, for example, the oil crisis; and during 2011 when real yields on many inflation-linked government stocks are negative.


=== Default ===
There is always the risk the borrower will become bankrupt, abscond or otherwise default on the loan. The risk premium attempts to measure the integrity of the borrower, the risk of his enterprise succeeding and the security of any collateral pledged. For example, loans to developing countries have higher risk premiums than those to the US government due to the difference in creditworthiness. An operating line of credit to a business will have a higher rate than a mortgage loan.
The creditworthiness of businesses is measured by bond rating services and individual's credit scores by credit bureaus. The risks of an individual debt may have a large standard deviation of possibilities. The lender may want to cover his maximum risk, but lenders with portfolios of debt can lower the risk premium to cover just the most probable outcome.


=== Composition of interest rates ===
In economics, interest is considered the price of credit, therefore, it is also subject to distortions due to inflation. The nominal interest rate, which refers to the price before adjustment to inflation, is the one visible to the consumer (that is, the interest tagged in a loan contract, credit card statement, etc.). Nominal interest is composed of the real interest rate plus inflation, among other factors. An approximate formula for the nominal interest is:

  
    
      
        i
        =
        r
        +
        π
      
    
    {\displaystyle i=r+\pi }
  

Where

i is the nominal interest rate
r is the real interest rate
and π is inflation.

However, not all borrowers and lenders have access to the same interest rate, even if they are subject to the same inflation. Furthermore, expectations of future inflation vary, so a forward-looking interest rate cannot depend on a single real interest rate plus a single expected rate of inflation.
Interest rates also depend on credit quality or risk of default. Governments are normally highly reliable debtors, and the interest rate on government securities is normally lower than the interest rate available to other borrowers.
The equation:

  
    
      
        i
        =
        r
        +
        π
        +
        c
      
    
    {\displaystyle i=r+\pi +c}
  

relates expectations of inflation and credit risk to nominal and expected real interest rates, over the life of a loan, where

i is the nominal interest applied
r is the real interest expected
π is the inflation expected and
c is yield spread according to the perceived credit risk.


=== Default interest ===
Default interest is the rate of interest that a borrower must pay after material breach of a loan covenant.
The default interest is usually much higher than the original interest rate since it is reflecting the aggravation in the financial risk of the borrower. Default interest compensates the lender for the added risk.
From the borrower's perspective, this means failure to make their regular payment for one or two payment periods or failure to pay taxes or insurance premiums for the loan collateral will lead to substantially higher interest for the entire remaining term of the loan.
Banks tend to add default interest to the loan agreements in order to separate between different scenarios.
In some jurisdictions, default interest clauses are unenforceable as against public policy.


=== Term ===
Shorter terms often have less risk of default and exposure to inflation because the near future is easier to predict. In these circumstances, short-term interest rates are lower than longer-term interest rates (an upward sloping yield curve).


=== Government intervention ===
Interest rates are generally determined by the market, but government intervention - usually by a central bank - may strongly influence short-term interest rates, and is one of the main tools of monetary policy. The central bank offers to borrow (or lend) large quantities of money at a rate which they determine (sometimes this is money that they have created ex nihilo, that is, printed) which has a major influence on supply and demand and hence on market interest rates.


=== Open market operations in the United States ===

The Federal Reserve (Fed) implements monetary policy largely by targeting the federal funds rate. This is the rate that banks charge each other for overnight loans of federal funds. Federal funds are the reserves held by banks at the Fed.
Open market operations are one tool within monetary policy implemented by the Federal Reserve to steer short-term interest rates. Using the power to buy and sell treasury securities, the Open Market Desk at the Federal Reserve Bank of New York can supply the market with dollars by purchasing U.S. Treasury notes, hence increasing the nation's money supply. By increasing the money supply or Aggregate Supply of Funding (ASF), interest rates will fall due to the excess of dollars banks will end up with in their reserves. Excess reserves may be lent in the Fed funds market to other banks, thus driving down rates.


=== Interest rates and credit risk ===
It is increasingly recognized that during the business cycle, interest rates and credit risk are tightly interrelated. The Jarrow-Turnbull model was the first model of credit risk that explicitly had random interest rates at its core. Lando (2004), Darrell Duffie and Singleton (2003), and van Deventer and Imai (2003) discuss interest rates when the issuer of the interest-bearing instrument can default.


=== Money and inflation ===
Loans and bonds have some of the characteristics of money and are included in the broad money supply.
National governments (provided, of course, that the country has retained its own currency) can influence interest rates and thus the supply and demand for such loans, thus altering the total of loans and bonds issued. Generally speaking, a higher real interest rate reduces the broad money supply.
Through the quantity theory of money, increases in the money supply lead to inflation. This means that interest rates can affect inflation in the future.


=== Liquidity ===
Liquidity is the ability to quickly re-sell an asset for fair or near-fair value. All else equal, an investor will want a higher return on an illiquid asset than a liquid one, to compensate for the loss of the option to sell it at any time. U.S. Treasury bonds are highly liquid with an active secondary market, while some other debts are less liquid. In the mortgage market, the lowest rates are often issued on loans that can be re-sold as securitized loans. Highly non-traditional loans such as seller financing often carry higher interest rates due to a lack of liquidity.


== Theories of interest ==


=== Aristotle's view of interest ===
Aristotle and the Scholastics held that it was unjust to claim payment except in compensation for one's own efforts and sacrifices, and that since money is by its nature sterile, there is no loss in being temporarily separated from it. Compensation for risk or for the trouble of setting up a loan was not necessarily impermissible on these grounds.


=== Development of the theory of interest during the seventeenth and eighteenth centuries ===
Nicholas Barbon (c.1640–c.1698) described as a ""mistake"" the view that interest is a monetary value, arguing that because money is typically borrowed to buy assets (goods and stock), the interest that is charged on a loan is a type of rent – ""a payment for the use of goods"". According to Schumpeter, Barbon's theories were forgotten until similar views were put forward by Joseph Massie in 1750.
In 1752 David Hume published his essay ""Of money"" which relates interest to the ""demand for borrowing"", the ""riches available to supply that demand"" and the ""profits arising from commerce"". Schumpeter considered Hume's theory superior to that of Ricardo and Mill, but the reference to profits concentrates to a surprising degree on 'commerce' rather than on industry.
Turgot brought the theory of interest close to its classical form. Industrialists...

... share their profits with capitalists who supply the funds (Réflexions, LXXI). The share that goes to the latter is determined like all other prices (LXXV) by the play of supply and demand amongst borrowers and lenders, so that the analysis is from the outset firmly planted in the general theory of prices.


=== The classical theory of the interest rate ===
The classical theory was the work of a number of authors, including Turgot, Ricardo, Mountifort Longfield, J. S. Mill, and Irving Fisher. It was strongly criticised by Keynes whose remarks nonetheless made a positive contribution to it.
Mill's theory is set out the chapter ""Of the rate of interest"" in his ""Principles of political economy"". He says that the interest rate adjusts to maintain equilibrium between the demands for lending and borrowing. Individuals lend in order to defer consumption or for the sake of the greater quantity they will be able to consume at a later date owing to interest earned. They borrow in order to anticipate consumption (whose relative desirability is reflected by the time value of money), but entrepreneurs also borrow to fund investment and governments borrow for their own reasons. The three sources of demand compete for loans.
For entrepreneurial borrowing to be in equilibrium with lending:

The interest for money... is... regulated... by the rate of profits which can be made by the employment of capital...
Ricardo's and Mill's 'profit' is made more precise by the concept of the marginal efficiency of capital (the expression, though not the concept, is due to Keynes), which may be defined as the annual revenue which will be yielded by an extra increment of capital as a proportion of its cost. So the interest rate r in equilibrium will be equal to the marginal efficiency of capital r'. Rather than work with r and r' as separate variables, we can assume that they are equal and let the single variable r denote their common value.

The investment schedule i (r) shows how much investment is possible with a return of at least r. In a stationary economy it is likely to resemble the blue curve in the diagram, with a step shape arising from the assumption that opportunities to invest with yields greater than r̂ have been largely exhausted while there is untapped scope to invest with a lower return.
Saving is the excess of deferred over anticipated consumption, and its dependence on income is much as described by Keynes (see The General Theory), but in classical theory definitely an increasing function of r. (The dependence of s on income y was not relevant to classical concerns prior to the development of theories of unemployment.) The rate of interest is given by the intersection of the solid red saving curve with the blue investment schedule. But so long as the investment schedule is almost vertical, a change in income (leading in extreme cases to the broken red saving curve) will make little difference to the interest rate.
In some cases the analysis will be less simple. The introduction of a new technique, leading to demand for new forms of capital, will shift the step to the right and reduce its steepness. Or a sudden increase in the desire to anticipate consumption (perhaps through military spending in time of war) will absorb most available loans; the interest rate will increase and investment will be reduced to the amount whose return exceeds it. This is illustrated by the dotted red saving curve.


==== Keynes's criticisms ====
In the case of extraordinary spending in time of war the government may wish to borrow more than the public would be willing to lend at a normal interest rate. If the dotted red curve started negative and showed no tendency to increase with r, then the government would be trying to buy what the public was unwilling to sell at any price. Keynes mentions this possibility as a point ""which might, perhaps, have warned the classical school that something was wrong"" (p. 182).
He also remarks (on the same page) that the classical theory does not explain the usual supposition that ""an increase in the quantity of money has a tendency to reduce the rate of interest, at any rate in the first instance"".
Keynes's diagram of the investment schecule lacks the step shape which can be seen as part of the classical theory. He objects that

the functions used by classical theory... do not furnish material for a theory of the rate of interest; but they could be used to tell us... what the rate of interest will have to be, if the level of employment [which determines income] is maintained at a given figure.
Later (p. 184) Keynes claims that ""it involves a circular argument"" to construct a theory of interest from the investment schedule since 

the 'marginal efficiency of capital' partly depends on the scale of current investment, and we must already know the rate of interest before we can calculate what this scale will be.


=== Theories of exploitation, productivity and abstinence ===
The classical theory of interest explains it as the capitalist's share of business profits, but the pre-marginalist authors were unable to reconcile these profits with the labor theory of value (excluding Longfield, who was essentially a marginalist). Their responses often had a moral tone: Ricardo and Marx viewed profits as exploitation, and McCulloch's productivity theory justified profits by portraying capital equipment as an embodiment of accumulated labor. The theory that interest is a payment for abstinence is attributed to Nassau Senior, and according to Schumpeter was intended neutrally, but it can easily be understood as making a moral claim and was sharply criticised by Marx and Lassalle.


=== Wicksell's theory ===
Knut Wicksell published his ""Interest and Prices"" in 1898, elaborating a comprehensive theory of economic crises based upon a distinction between natural and nominal interest rates. 

Wicksell's contribution, in fact, was twofold. First he separated the monetary rate of interest from the hypothetical ""natural"" rate that would have resulted from equilibrium of capital supply and demand in a barter economy, and he assumed that as a result of the presence of money alone, the effective market rate could fail to correspond to this ideal rate in actuality. Next he supposed that through the mechanism of credit, the rate of interest had an influence on prices; that a rise of the monetary rate above the ""natural"" level produced a fall, and a decline below that level a rise, in prices. But Wicksell went on to conclude that if the natural rate coincided with the monetary rate, stability of prices would follow.
In the 1930s Wicksell's approach was refined by Bertil Ohlin and Dennis Robertson and became known as the loanable funds theory.


=== Austrian theories ===
Eugen Böhm von Bawerk and other members of the Austrian School also put forward notable theories of the interest rate.

The doyen of the Austrian school, Murray N. Rothbard, sees the emphasis on the loan market which makes up the general analysis on interest as a mistaken view to take. As he explains in his primary economic work, Man, Economy, and State, the market rate of interest is but a manifestation of the natural phenomenon of time preference, which is to prefer present goods to future goods. To Rothbard, Too many writers consider the rate of interest as only the price of loans on the loan market. In reality...the rate of interest pervades all time markets, and the productive loan market is a strictly subsidiary time market of only derivative importance.
Interest is explainable by the rate of time preference among the people. To point to the loan market is insufficient at best. Rather, the rate of interest is what would be observed between the ""stages of production"", indeed a time market itself, where capital goods which are used to make consumers' goods are ordered out further in time away from the final consumers' goods stage of the economy where consumption takes place. It is this spread (between these various stages which will tend toward uniformity), with consumers' goods representing present goods and producers' goods representing future goods, that the real rate of interest is observed. Rothbard has said that Interest rate is equal to the rate of price spread in the various stages. Rothbard has furthermore criticized the Keynesian conception of interest, saying One grave and fundamental Keynesian error is to persist in regarding the interest rate as a contract rate on loans, instead of the price spreads between stages of production.


=== Pareto's indifference ===
Pareto held that

The interest rate, being one of the many elements of the general system of equilibrium, was, of course, simultaneously determined with all of them so that there was no point at all in looking for any particular element that 'caused' interest.


=== Keynes's theory of the interest rate ===
Interest is one of the main components of the economic theories developed in Keynes's 1936 General theory of employment, interest, and money. In his initial account of liquidity preference (the demand for money) in Chapter 13, this demand is solely a function of the interest rate; and since the supply is given and equilibrium is assumed, the interest rate is determined by the money supply.
In his later account (Chapter 15), interest cannot be separated from other economic variables and needs to be analysed together with them. See The General Theory for details.


== In religious contexts ==


=== Judaism ===

Jews are forbidden from usury in dealing with fellow Jews, and this lending is to be considered tzedakah, or charity. However, there are permissions to charge interest on loans to non-Jews. This is outlined in the Jewish scriptures of the Torah, which Christians hold as part of the Old Testament, and other books of the Tanakh. From the Jewish Publication Society's 1917 Tanakh, with Christian verse numbers, where different, in parentheses:

If thou lend money to any of My people, even to the poor with thee, thou shalt not be to him as a creditor; neither shall ye lay upon him interest.
Take thou no interest of him or increase; but fear thy God; that thy brother may live with thee.
Thou shalt not give him thy money upon interest, nor give him thy victuals for increase.
Thou shalt not lend upon interest to thy brother: interest of money, interest of victuals, interest of any thing that is lent upon interest.
Unto a foreigner thou mayest lend upon interest; but unto thy brother thou shalt not lend upon interest; that the LORD thy God may bless thee in all that thou puttest thy hand unto, in the land whither thou goest in to possess it.
... that hath withdrawn his hand from the poor, that hath not received interest nor increase, hath executed Mine ordinances, hath walked in My statutes; he shall not die for the iniquity of his father, he shall surely live.
He that putteth not out his money on interest, nor taketh a bribe against the innocent. He that doeth these things shall never be moved.
Several historical rulings in Jewish law have mitigated the allowances for usury toward non-Jews. For instance, the 15th-century commentator Rabbi Isaac Abrabanel specified that the rubric for allowing interest does not apply to Christians or Muslims, because their faith systems have a common ethical basis originating from Judaism. The medieval commentator Rabbi David Kimchi extended this principle to non-Jews who show consideration for Jews, saying they should be treated with the same consideration when they borrow.


=== Islam ===

The following quotations are English translations from the Qur'an:

Those who charge usury are in the same position as those controlled by the devil's influence. This is because they claim that usury is the same as commerce. However, God permits commerce, and prohibits usury. Thus, whoever heeds this commandment from his Lord, and refrains from usury, he may keep his past earnings, and his judgment rests with God. As for those who persist in usury, they incur Hell, wherein they abide forever.
God condemns usury, and blesses charities. God dislikes every sinning disbeliever. Those who believe and do good works and establish worship and pay the poor-due, their reward is with their Lord and there shall no fear come upon them neither shall they grieve. O you who believe, you shall observe God and refrain from all kinds of usury, if you are believers. If you do not, then expect a war from God and His messenger. But if you repent, you may keep your capitals, without inflicting injustice, or incurring injustice. If the debtor is unable to pay, wait for a better time. If you give up the loan as a charity, it would be better for you, if you only knew.
O you who believe, you shall not take usury, compounded over and over. Observe God, that you may succeed.
And for practicing usury, which was forbidden, and for consuming the people's money illicitly. We have prepared for the disbelievers among them painful retribution.
The usury that is practiced to increase some people's wealth, does not gain anything at God. But if people give to charity, seeking God's pleasure, these are the ones who receive their reward many fold.
The attitude of Muhammad to usury is articulated in his Last Sermon:

O People, just as you regard this month, this day, this city as Sacred, so regard the life and property of every Muslim as a sacred trust. Return the goods entrusted to you to their rightful owners. Hurt no one so that no one may hurt you. Remember that you will indeed meet your Lord, and that He will indeed reckon your deeds. Allah has forbidden you to take usury, therefore all usurious obligation shall henceforth be waived. Your capital, however, is yours to keep. You will neither inflict nor suffer any inequity. Allah has Judged that there shall be no usury and that all the usury due to Abbas ibn 'Abd'al Muttalib (Prophet's uncle) shall henceforth be waived ...


=== Christianity ===

The Old Testament ""condemns the practice of charging interest because a loan should be an act of compassion and taking care of one's neighbor""; it teaches that ""making a profit off a loan is exploiting that person and dishonoring God's covenant (Exodus 22:25–27)"".
The first of the scholastic Christian theologians, Saint Anselm of Canterbury, led the shift in thought that labeled charging interest the same as theft. Previously usury had been seen as a lack of charity.
St. Thomas Aquinas, the leading scholastic theologian of the Roman Catholic Church, argued charging of interest is wrong because it amounts to ""double charging"", charging for both the thing and the use of the thing. Aquinas said this would be morally wrong in the same way as if one sold a bottle of wine, charged for the bottle of wine, and then charged for the person using the wine to actually drink it. Similarly, one cannot charge for a piece of cake and for the eating of the piece of cake. Yet this, said Aquinas, is what usury does. Money is a medium of exchange, and is used up when it is spent. To charge for the money and for its use (by spending) is therefore to charge for the money twice. It is also to sell time since the usurer charges, in effect, for the time that the money is in the hands of the borrower. Time, however, is not a commodity that anyone can charge. In condemning usury Aquinas was much influenced by the recently rediscovered philosophical writings of Aristotle and his desire to assimilate Greek philosophy with Christian theology. Aquinas argued that in the case of usury, as in other aspects of Christian revelation, Christian doctrine is reinforced by Aristotelian natural law rationalism. Aristotle's argument is that interest is unnatural, since money, as a sterile element, cannot naturally reproduce itself. Thus, usury conflicts with natural law just as it offends Christian revelation: see Thought of Thomas Aquinas. As such, Aquinas taught that interest is inherently unjust and one who charges interest sins.
Outlawing usury did not prevent investment, but stipulated that in order for the investor to share in the profit he must share the risk. In short he must be a joint-venturer. Simply to invest the money and expect it to be returned regardless of the success of the venture was to make money simply by having money and not by taking any risk or by doing any work or by any effort or sacrifice at all, which is usury. St Thomas quotes Aristotle as saying that ""to live by usury is exceedingly unnatural"". Islam likewise condemns usury but allowed commerce (Al-Baqarah 2:275) – an alternative that suggests investment and sharing of profit and loss instead of sharing only profit through interests. Judaism condemns usury towards Jews, but allows it towards non-Jews (Deut. 23:19–20). St Thomas allows, however, charges for actual services provided. Thus a banker or credit-lender could charge for such actual work or effort as he did carry out, for example, any fair administrative charges. The Catholic Church, in a decree of the Fifth Council of the Lateran, expressly allowed such charges in respect of credit-unions run for the benefit of the poor known as ""montes pietatis"".
In the 13th century Cardinal Hostiensis enumerated thirteen situations in which charging interest was not immoral. The most important of these was lucrum cessans (profits given up) which allowed for the lender to charge interest ""to compensate him for profit foregone in investing the money himself"". This idea is very similar to opportunity cost. Many scholastic thinkers who argued for a ban on interest charges also argued for the legitimacy of lucrum cessans profits (for example, Pierre Jean Olivi and St. Bernardino of Siena). However, Hostiensis' exceptions, including for lucrum cessans, were never accepted as official by the Roman Catholic Church.
The Westminster Confession of Faith, a confession of faith upheld by the Reformed Churches, teaches that usury — defined as charging interest at any rate — is a sin prohibited by the eighth commandment.
The Roman Catholic Church has always condemned usury, but in modern times, with the rise of capitalism and the disestablishment of the Catholic Church in majority Catholic countries, this prohibition on usury has not been enforced.
Pope Benedict XIV's encyclical Vix Pervenit gives the reasons why usury is sinful:

The nature of the sin called usury has its proper place and origin in a loan contract ... [which] demands, by its very nature, that one return to another only as much as he has received. The sin rests on the fact that sometimes the creditor desires more than he has given ..., but any gain which exceeds the amount he gave is illicit and usurious.
One cannot condone the sin of usury by arguing that the gain is not great or excessive, but rather moderate or small; neither can it be condoned by arguing that the borrower is rich; nor even by arguing that the money borrowed is not left idle, but is spent usefully ...
The Congregation of the Missionary Sons of the Immaculate Heart of Mary, a Catholic Christian religious order, thus teaches that:

It might initially seem like little is at stake when it comes to interest, but this is an issue of human dignity. A person is made in God's own image and therefore may never be treated as a thing. Interest can diminish the human person to a thing to be manipulated for money. In an article for The Catholic Worker, Dorothy Day articulated this well: ""Can I talk about the people living off usury . . . not knowing the way that their infertile money has bred more money by wise investment in God knows what devilish nerve gas, drugs, napalm, missiles, or vanities, when housing and employment . . . for the poor were needed, and money could have been invested there?"" Her thoughts were a precursor to what Pope Francis now calls an ""economy that kills."" To sin is to say ""no"" to God and God's presence by harming others, ourselves, or all of creation. Charging interest is indeed sinful when doing so takes advantage of a person in need as well as when it means investing in corporations involved in the harming of God's creatures.


== See also ==
Actuarial notation
Credit card interest
Credit rating agency
DIRTI 5
Discount
Fisher equation
Hire purchase
Interest expense
Leasing
Promissory note
Risk-free interest rate


== Notes ==


== References ==
Duffie, Darrell and Kenneth J. Singleton (2003). Credit Risk: Pricing, Measurement, and Management. Princeton University Press. ISBN 978-0-691-09046-7.
Kellison, Stephen G. (1970). The Theory of Interest. Richard D. Irwin, Inc. Library of Congress Catalog Card No. 79-98251.
Lando, David (2004). Credit Risk Modeling: Theory and Applications. Princeton University Press. ISBN 978-0-691-08929-4.
van Deventer, Donald R. and Kenji Imai (2003). Credit Risk Models and the Basel Accords. John Wiley & Sons. ISBN 978-0-470-82091-9.
Rothbard, Murray N. (2001). Man, economy, and state : a treatise on economic principles (Rev ed.). Auburn, Alabama: Mises Institute. ISBN 0945466323. OCLC 47279566.
Schumpeter, Joseph (1954). History of Economic Analysis. Allen & Unwin.


== External links ==

White Paper: More than Math, The Lost Art of Interest calculation
Mortgages made clear Financial Services Authority (UK)
List of current interest rates:
World Interest Rates
Forex Motion
""Which way to pay"" Archived 2011-04-19 at the Wayback Machine
Deposit Rates in European Countries"
history of medicine,"The history of medicine is both a study of medicine throughout history as well as a multidisciplinary field of study that seeks to explore and understand medical practices, both past and present, throughout human societies.
The history of medicine is the study and documentation of the evolution of medical treatments, practices, and knowledge over time. Medical historians often draw from other humanities fields of study including economics, health sciences, sociology, and politics to better understand the institutions, practices, people, professions, and social systems that have shaped medicine. When a period which predates or lacks written sources regarding medicine, information is instead drawn from archaeological sources. This field tracks the evolution of human societies' approach to health, illness, and injury ranging from prehistory to the modern day, the events that shape these approaches, and their impact on populations.
Early medical traditions include those of Babylon, China, Egypt and India. Invention of the microscope was a consequence of improved understanding, during the Renaissance. Prior to the 19th century, humorism (also known as humoralism) was thought to explain the cause of disease but it was gradually replaced by the germ theory of disease, leading to effective treatments and even cures for many infectious diseases. Military doctors advanced the methods of trauma treatment and surgery. Public health measures were developed especially in the 19th century as the rapid growth of cities required systematic sanitary measures. Advanced research centers opened in the early 20th century, often connected with major hospitals. The mid-20th century was characterized by new biological treatments, such as antibiotics. These advancements, along with developments in chemistry, genetics, and radiography led to modern medicine. Medicine was heavily professionalized in the 20th century, and new careers opened to women as nurses (from the 1870s) and as physicians (especially after 1970).


== Prehistoric medicine ==

Prehistoric medicine is a field of study focused on understanding the use of medicinal plants, healing practices, illnesses, and wellness of humans before written records existed. Although styled prehistoric ""medicine"", prehistoric healthcare practices were vastly different from what we understand medicine to be in the present era and more accurately refers to studies and exploration of early healing practices.
This period extends across the first use of stone tools by early humans c. 3.3 million years ago to the beginning of writing systems and subsequent recorded history c. 5000 years ago.
As human populations were once scattered across the world, forming isolated communities and cultures that sporadically interacted, a range of archaeological periods have been developed to account for the differing contexts of technology, sociocultural developments, and uptake of writing systems throughout early human societies. Prehistoric medicine is then highly contextual to the location and people in question, creating an ununiform period of study to reflect various degrees of societal development.
Without written records, insights into prehistoric medicine comes indirectly from interpreting evidence left behind by prehistoric humans. One branch of this includes the archaeology of medicine; a discipline that uses a range of archaeological techniques from observing illness in human remains, plant fossils, to excavations to uncover medical practices. There is evidence of healing practices within Neanderthals and other early human species. Prehistoric evidence of human engagement with medicine include the discovery of psychoactive plant sources such as psilocybin mushrooms in c. 6000 BCE Sahara to primitive dental care in c. 10,900 BCE (13,000 BP) Riparo Fredian (present-day Italy) and c. 7000 BCE Mehrgarh (present-day Pakistan).
Anthropology is another academic branch that contributes to understanding prehistoric medicine in uncovering the sociocultural relationships, meaning, and interpretation of prehistoric evidence. The overlap of medicine as both a root to healing the body as well as the spiritual throughout prehistoric periods highlights the multiple purposes that healing practices and plants could potentially have. From proto-religions to developed spiritual systems, relationships of humans and supernatural entities, from Gods to shamans, have played an interwoven part in prehistoric medicine.


== Ancient medicine ==

Ancient history covers time between c. 3000 BCE to c. 500 CE, starting from evidenced development of writing systems to the end of the classical era and beginning of the post-classical period. This periodisation presents history as if it were the same everywhere, however it is important to note that socioculture and technological developments could differ locally from settlement to settlement as well as globally from one society to the next.
Ancient medicine covers a similar period of time and presented a range of similar healing theories from across the world connecting nature, religion, and humans within ideas of circulating fluids and energy. Although prominent scholars and texts detailed well-defined medical insights, their real-world applications were marred by knowledge destruction and loss, poor communication, localised reinterpretations, and subsequent inconsistent applications.


=== Ancient Mesopotamian medicine ===
The Mesopotamian region, covering much of present-day Iraq, Kuwait, Syria, Iran, and Turkey, was dominated by a series of civilisations including Sumer, the earliest known civilisation in the Fertile Crescent region, alongside the Akkadians (including Assyrians and Babylonians). Overlapping ideas of what we now understand as medicine, science, magic, and religion characterised early Mesopotamian healing practices as a hybrid naturalistic and supernatural belief system.
The Sumerians, having developed one of the earliest known writing systems in the 3rd millennium BCE, created numerous cuneiform clay tablets regarding their civilisation included detailed accounts of drug prescriptions, operations, to exorcisms. These were administered and carried out by highly defined professionals including bârû (seers), âs[h]ipu (exorcists), and asû (physician-priests). An example of an early, prescription-like medication appeared in Sumerian during the Third Dynasty of Ur (c. 2112 BCE – c. 2004 BCE).
Following the conquest of the Sumerian civilisation by the Akkadian Empire and the empire's eventual collapse from a number of social and environmental factors, the Babylonian civilisation began to dominate the region. Examples of Babylonian Medicine include the extensive Babylonian medical text, the Diagnostic Handbook, written by the ummânū, or chief scholar, Esagil-kin-apli of Borsippa,: 99  in the middle of the 11th century BCE during the reign of the Babylonian king Adad-apla-iddina (1069–1046 BCE).

This medical treatise presented great attention to the practice of diagnosis, prognosis, physical examination, and remedies. The text contains a list of medical symptoms and often detailed empirical observations along with logical rules used in combining observed symptoms on the body of a patient with its diagnosis and prognosis.: 97–98  Here, clearly developed rationales were developed to understand the causes of disease and injury, supported by agreed upon theories at-the-time of elements we might now understand as natural causes, supernatural magic and religious explanations.Most known and recovered artefacts from the ancient Mesopotamian civilisations centre on the neo-Assyrian (c. 900 – 600 BCE) and neo-Babylonian (c. 600 – 500 BCE) periods, as the last empires ruled by native Mesopotamian rulers. These discoveries include a huge array of medical clay tablets from this period, although damage to the clay documents creates large gaps in our understanding of medical practices.
Throughout the civilisations of Mesopotamia there is a wide range of medical innovations including evidenced practices of prophylaxis, taking measures to prevent the spread of disease, accounts of stroke, to an awareness of mental illnesses.


=== Ancient Egyptian medicine ===

Ancient Egypt, a civilisation spanning across the river Nile (throughout parts of present-day Egypt, Sudan, and South Sudan), existed from its unification in c. 3150 BCE to its collapse via Persian conquest in 525 BCE and ultimate downfall from the conquest of Alexander the Great in 332 BCE.
Throughout unique dynasties, golden eras, and intermediate periods of instability, ancient Egyptians developed a complex, experimental, and communicative medical tradition that has been uncovered through surviving documents, most made of papyrus, such as the Kahun Gynaecological Papyrus, the Edwin Smith Papyrus, the Ebers Papyrus, the London Medical Papyrus, to the Greek Magical Papyri.
Herodotus described the Egyptians as ""the healthiest of all men, next to the Libyans"", because of the dry climate and the notable public health system that they possessed. According to him, ""the practice of medicine is so specialized among them that each physician is a healer of one disease and no more."" Although Egyptian medicine, to a considerable extent, dealt with the supernatural, it eventually developed a practical use in the fields of anatomy, public health, and clinical diagnostics.

Medical information in the Edwin Smith Papyrus may date to a time as early as 3000 BCE. Imhotep in the 3rd dynasty is sometimes credited with being the founder of ancient Egyptian medicine and with being the original author of the Edwin Smith Papyrus, detailing cures, ailments and anatomical observations. The Edwin Smith Papyrus is regarded as a copy of several earlier works and was written c. 1600 BCE. It is an ancient textbook on surgery almost completely devoid of magical thinking and describes in exquisite detail the examination, diagnosis, treatment, and prognosis of numerous ailments.
The Kahun Gynaecological Papyrus treats women's complaints, including problems with conception. Thirty four cases detailing diagnosis and treatment survive, some of them fragmentarily. Dating to 1800 BCE, it is the oldest surviving medical text of any kind.
Medical institutions, referred to as Houses of Life are known to have been established in ancient Egypt as early as 2200 BCE.
The Ebers Papyrus is the oldest written text mentioning enemas. Many medications were administered by enemas and one of the many types of medical specialists was an Iri, the Shepherd of the Anus.
The earliest known physician is also credited to ancient Egypt: Hesy-Ra, ""Chief of Dentists and Physicians"" for King Djoser in the 27th century BCE. Also, the earliest known woman physician, Peseshet, practiced in Ancient Egypt at the time of the 4th dynasty. Her title was ""Lady Overseer of the Lady Physicians.""


=== Ancient Chinese medicine ===

Medical and healing practices in early Chinese dynasties were heavily shaped by the practice of traditional Chinese medicine (TCM). Starting around the Zhou dynasty, parts of this system were being developed and are demonstrated in early writings on herbs in Classic of Changes (Yi Jing) and Classic of Poetry (Shi Jing).
China also developed a large body of traditional medicine. Much of the philosophy of traditional Chinese medicine derived from empirical observations of disease and illness by Taoist physicians and reflects the classical Chinese belief that individual human experiences express causative principles effective in the environment at all scales. These causative principles, whether material, essential, or mystical, correlate as the expression of the natural order of the universe.
The foundational text of Chinese medicine is the Huangdi Neijing, (or Yellow Emperor's Inner Canon), written 5th century to 3rd century BCE. Near the end of the 2nd century CE, during the Han dynasty, Zhang Zhongjing, wrote a Treatise on Cold Damage, which contains the earliest known reference to the Neijing Suwen. The Jin dynasty practitioner and advocate of acupuncture and moxibustion, Huangfu Mi (215–282), also quotes the Yellow Emperor in his Jiayi jing, c. 265. During the Tang dynasty, the Suwen was expanded and revised and is now the best extant representation of the foundational roots of traditional Chinese medicine. Traditional Chinese medicine that is based on the use of herbal medicine, acupuncture, massage and other forms of therapy has been practiced in China for thousands of years.
Critics say that TCM theory and practice have no basis in modern science, and TCM practitioners do not agree on what diagnosis and treatments should be used for any given person. A 2007 editorial in the journal Nature wrote that TCM ""remains poorly researched and supported, and most of its treatments have no logical mechanism of action."" It also described TCM as ""fraught with pseudoscience"".  A review of the literature in 2008 found that scientists are ""still unable to find a shred of evidence"" according to standards of science-based medicine for traditional Chinese concepts such as qi, meridians, and acupuncture points, and that the traditional principles of acupuncture are deeply flawed. There are concerns over a number of potentially toxic plants, animal parts, and mineral Chinese compounds, as well as the facilitation of disease. Trafficked and farm-raised animals used in TCM are a source of several fatal zoonotic diseases. There are additional concerns over the illegal trade and transport of endangered species including rhinoceroses and tigers, and the welfare of specially farmed animals, including bears.


=== Ancient Indian medicine ===

The Atharvaveda, a sacred text of Hinduism dating from the middle Vedic age (c. 1200–900 BCE), is one of the first Indian texts dealing with medicine. It is a text filled with magical charms, spells, and incantations used for various purposes, such as protection against demons, rekindling love, ensuring childbirth, and achieving success in battle, trade, and even gambling. It also includes numerous charms aimed at curing diseases and several remedies from medicinal herbs, overall making it a key source of medical knowledge during the Vedic period. The use of herbs to treat ailments would later form a large part of Ayurveda. 
Ayurveda, meaning the ""complete knowledge for long life"" is another medical system of India. Its two most famous texts(samhitas) belong to the schools of Charaka and Sushruta. The Samhitas represent later revised versions(recensions) of their original works. The earliest foundations of Ayurveda were built on a synthesis of traditional herbal practices together with a massive addition of theoretical conceptualizations, new nosologies and new therapies dating from about 600 BCE onwards, and coming out of the communities of thinkers which included the Buddha and others.
According to the compendium of Charaka, the Charakasamhitā, health and disease are not predetermined and life may be prolonged by human effort. The compendium of Suśruta, the Suśrutasamhitā defines the purpose of medicine to cure the diseases of the sick, protect the healthy, and to prolong life. Both these ancient compendia include details of the examination, diagnosis, treatment, and prognosis of numerous ailments. The Suśrutasamhitā is notable for describing procedures on various forms of surgery, including rhinoplasty, the repair of torn ear lobes, perineal lithotomy, cataract surgery, and several other excisions and other surgical procedures. Most remarkable was Susruta's surgery specially the rhinoplasty for which he is called father of plastic surgery. Susruta also described more than 125 surgical instruments in detail. Also remarkable is Sushruta's penchant for scientific classification:
His medical treatise consists of 184 chapters, 1,120 conditions are listed, including injuries and illnesses relating to aging and mental illness.
The Ayurvedic classics mention eight branches of medicine: kāyācikitsā (internal medicine), śalyacikitsā (surgery including anatomy), śālākyacikitsā (eye, ear, nose, and throat diseases), kaumārabhṛtya (pediatrics with obstetrics and gynaecology), bhūtavidyā (spirit and psychiatric medicine), agada tantra (toxicology with treatments of stings and bites), rasāyana (science of rejuvenation), and vājīkaraṇa (aphrodisiac and fertility). Apart from learning these, the student of Āyurveda was expected to know ten arts that were indispensable in the preparation and application of his medicines: distillation, operative skills, cooking, horticulture, metallurgy, sugar manufacture, pharmacy, analysis and separation of minerals, compounding of metals, and preparation of alkalis. The teaching of various subjects was done during the instruction of relevant clinical subjects. For example, the teaching of anatomy was a part of the teaching of surgery, embryology was a part of training in pediatrics and obstetrics, and the knowledge of physiology and pathology was interwoven in the teaching of all the clinical disciplines.
Even today Ayurvedic treatment is practiced, but it is considered pseudoscientific because its premises are not based on science, some ayurvedic medicines have been found to contain toxic substances. Both the lack of scientific soundness in the theoretical foundations of ayurveda and the quality of research have been criticized.


=== Ancient Greek medicine ===

Humors
The theory of humors was derived from ancient medical works, dominated Western medicine until the 19th century, and is credited to Greek philosopher and surgeon Galen of Pergamon (129 – c. 216 CE). In Greek medicine, there are thought to be four humors, or bodily fluids that are linked to illness: blood, phlegm, yellow bile, and black bile. Early scientists believed that food is digested into blood, muscle, and bones, while the humors that were not blood were then formed by indigestible materials that are left over. An excess or shortage of any one of the four humors is theorized to cause an imbalance that results in sickness; the aforementioned statement was hypothesized by sources before Hippocrates. Hippocrates (c. 400 BCE) deduced that the four seasons of the year and four ages of man that affect the body in relation to the humors. The four ages of man are childhood, youth, prime age, and old age. Black bile is associated with autumn, phlegm with winter, blood with spring, and yellow bile with summer.
In De temperamentis, Galen linked what he called temperaments, or personality characteristics, to a person's natural mixture of humors. He also said that the best place to check the balance of temperaments was in the palm of the hand. A person that is considered to be phlegmatic is said to be an introvert, even-tempered, calm, and peaceful. This person would have an excess of phlegm, which is described as a viscous substance or mucous. Similarly, a melancholic temperament related to being moody, anxious, depressed, introverted, and pessimistic. A melancholic temperament is caused by an excess of black bile, which is sedimentary and dark in colour. Being extroverted, talkative, easygoing, carefree, and sociable coincides with a sanguine temperament, which is linked to too much blood. Finally, a choleric temperament is related to too much yellow bile, which is actually red in colour and has the texture of foam; it is associated with being aggressive, excitable, impulsive, and also extroverted.
There are numerous ways to treat a disproportion of the humors. For example, if someone was suspected to have too much blood, then the physician would perform bloodletting as a treatment. Likewise, if a person believed to have too much phlegm should feel better after expectorating, and someone with too much yellow bile would purge. Another factor to be considered in the balance of humors is the quality of air where one resides, such as the climate and elevation. Also, the standard of food and drink, balance of sleeping and waking, exercise and rest, retention and evacuation are important. Moods such as anger, sadness, joy, and love can affect the balance. During that time, the importance of balance was demonstrated by the fact that women lose blood monthly during menstruation, and have a lesser occurrence of gout, arthritis, and epilepsy then men do. Galen also hypothesized that there are three faculties. The natural faculty affects growth and reproduction and is produced in the liver. Animal or vital faculty controls respiration and emotion, coming from the heart. In the brain, the psychic faculty commands the senses and thoughts. The structure of bodily functions is related to the humors as well. Greek physicians understood that food was cooked in the stomach; this is where the nutrients are extracted. The best, most potent and pure nutrients from food are reserved for blood, which is produced in the liver and carried through veins to organs. Blood enhanced with pneuma, which means wind or breath, is carried by the arteries. The path that blood take is as follows: venous blood passes through the vena cava and is moved into the right ventricle of the heart; then, the pulmonary artery takes it to the lungs. Later, the pulmonary vein then mixes air from the lungs with blood to form arterial blood, which has different observable characteristics. After leaving the liver, half of the yellow bile that is produced travels to the blood, while the other half travels to the gallbladder. Similarly, half of the black bile produced gets mixed in with blood, and the other half is used by the spleen.
People
Around 800 BCE Homer in the Iliad gives descriptions of wound treatment by the two sons of Asklepios, the admirable physicians Podaleirius and Machaon and one acting doctor, Patroclus. Because Machaon is wounded and Podaleirius is in combat Eurypylus asks Patroclus to ""cut out the arrow-head, and wash the dark blood from my thigh with warm water, and sprinkle soothing herbs with power to heal on my wound"". Asklepios, like Imhotep, came to be associated as a god of healing over time.

Temples dedicated to the healer-god Asclepius, known as Asclepieia (Ancient Greek: Ἀσκληπιεῖα, sing. Ἀσκληπιεῖον, Asclepieion), functioned as centers of medical advice, prognosis, and healing. At these shrines, patients would enter a dream-like state of induced sleep known as enkoimesis (ἐγκοίμησις) not unlike anesthesia, in which they either received guidance from the deity in a dream or were cured by surgery. Asclepeia provided carefully controlled spaces conducive to healing and fulfilled several of the requirements of institutions created for healing. In the Asclepeion of Epidaurus, three large marble boards dated to 350 BCE preserve the names, case histories, complaints, and cures of about 70 patients who came to the temple with a problem and shed it there. Some of the surgical cures listed, such as the opening of an abdominal abscess or the removal of traumatic foreign material, are realistic enough to have taken place, but with the patient in a state of enkoimesis induced with the help of soporific substances such as opium. Alcmaeon of Croton wrote on medicine between 500 and 450 BCE. He argued that channels linked the sensory organs to the brain, and it is possible that he discovered one type of channel, the optic nerves, by dissection.
Hippocrates of Kos (c. 460 – c. 370 BCE), considered the ""father of modern medicine."" The Hippocratic Corpus is a collection of around seventy early medical works from ancient Greece strongly associated with Hippocrates and his students. Most famously, the Hippocratics invented the Hippocratic Oath for physicians. Contemporary physicians swear an oath of office which includes aspects found in early editions of the Hippocratic Oath.
Hippocrates and his followers were first to describe many diseases and medical conditions. Though humorism (humoralism) as a medical system predates 5th-century Greek medicine, Hippocrates and his students systematized the thinking that illness can be explained by an imbalance of blood, phlegm, black bile, and yellow bile. Hippocrates is given credit for the first description of clubbing of the fingers, an important diagnostic sign in chronic suppurative lung disease, lung cancer and cyanotic heart disease. For this reason, clubbed fingers are sometimes referred to as ""Hippocratic fingers"". Hippocrates was also the first physician to describe the Hippocratic face in Prognosis. Shakespeare famously alludes to this description when writing of Falstaff's death in Act II, Scene iii. of Henry V. Hippocrates began to categorize illnesses as acute, chronic, endemic and epidemic, and use terms such as, ""exacerbation, relapse, resolution, crisis, paroxysm, peak, and convalescence.""

The Greek Galen (c. 129–216 CE) was one of the greatest physicians of the ancient world, as his theories dominated all medical studies for nearly 1500 years. His theories and experimentation laid the foundation for modern medicine surrounding the heart and blood. Galen's influence and innovations in medicine can be attributed to the experiments he conducted, which were unlike any other medical experiments of his time. Galen strongly believed that medical dissection was one of the essential procedures in truly understanding medicine. He began to dissect different animals that were anatomically similar to humans, which allowed him to learn more about the internal organs and extrapolate the surgical studies to the human body. In addition, he performed many audacious operations—including brain and eye surgeries—that were not tried again for almost two millennia. Through the dissections and surgical procedures, Galen concluded that blood is able to circulate throughout the human body, and the heart is most similar to the human soul. In Ars medica (""Arts of Medicine""), he further explains the mental properties in terms of specific mixtures of the bodily organs. While much of his work surrounded the physical anatomy, he also worked heavily in humoral physiology.
Galen's medical work was regarded as authoritative until well into the Middle Ages. He left a physiological model of the human body that became the mainstay of the medieval physician's university anatomy curriculum. Although he attempted to extrapolate the animal dissections towards the model of the human body, some of Galen's theories were incorrect. This caused his model to suffer greatly from stasis and intellectual stagnation. Greek and Roman taboos caused dissection of the human body to usually be banned in ancient times, but in the Middle Ages it changed.
In 1523 Galen's On the Natural Faculties was published in London. In the 1530s Belgian anatomist and physician Andreas Vesalius launched a project to translate many of Galen's Greek texts into Latin. Vesalius's most famous work, De humani corporis fabrica was greatly influenced by Galenic writing and form.

		
			
			
		
		
			
			
		

Herophilus and Erasistratus

Two great Alexandrians laid the foundations for the scientific study of anatomy and physiology, Herophilus of Chalcedon and Erasistratus of Ceos. Other Alexandrian surgeons gave us ligature (hemostasis), lithotomy, hernia operations, ophthalmic surgery, plastic surgery, methods of reduction of dislocations and fractures, tracheotomy, and mandrake as an anaesthetic. Some of what we know of them comes from Celsus and Galen of Pergamum.
Herophilus of Chalcedon, the renowned Alexandrian physician, was one of the pioneers of human anatomy. Though his knowledge of the anatomical structure of the human body was vast, he specialized in the aspects of neural anatomy. Thus, his experimentation was centered around the anatomical composition of the blood-vascular system and the pulsations that can be analyzed from the system. Furthermore, the surgical experimentation he administered caused him to become very prominent throughout the field of medicine, as he was one of the first physicians to initiate the exploration and dissection of the human body.
The banned practice of human dissection was lifted during his time within the scholastic community. This brief moment in the history of Greek medicine allowed him to further study the brain, which he believed was the core of the nervous system. He also distinguished between veins and arteries, noting that the latter pulse and the former do not. Thus, while working at the medical school of Alexandria, Herophilus placed intelligence in the brain based on his surgical exploration of the body, and he connected the nervous system to motion and sensation. In addition, he and his contemporary, Erasistratus of Chios, continued to research the role of veins and nerves. After conducting extensive research, the two Alexandrians mapped out the course of the veins and nerves across the human body. Erasistratus connected the increased complexity of the surface of the human brain compared to other animals to its superior intelligence. He sometimes employed experiments to further his research, at one time repeatedly weighing a caged bird, and noting its weight loss between feeding times. In Erasistratus' physiology, air enters the body, is then drawn by the lungs into the heart, where it is transformed into vital spirit, and is then pumped by the arteries throughout the body. Some of this vital spirit reaches the brain, where it is transformed into animal spirit, which is then distributed by the nerves.


=== Ancient Roman medicine ===

The Romans invented numerous surgical instruments, including the first instruments unique to women, as well as the surgical uses of forceps, scalpels, cautery, cross-bladed scissors, the surgical needle, the sound, and speculas. Romans also performed cataract surgery.
The Roman army physician Dioscorides (c. 40–90 CE), was a Greek botanist and pharmacologist. He wrote the encyclopedia De Materia Medica describing over 600 herbal cures, forming an influential pharmacopoeia which was used extensively for the following 1,500 years.
Early Christians in the Roman Empire incorporated medicine into their theology, ritual practices, and metaphors.


== Post-classical medicine ==


=== Middle East ===


==== Places ====
Byzantine medicine
Byzantine medicine encompasses the common medical practices of the Byzantine Empire from about 400 CE to 1453 CE. Byzantine medicine was notable for building upon the knowledge base developed by its Greco-Roman predecessors. In preserving medical practices from antiquity, Byzantine medicine influenced Islamic medicine as well as fostering the Western rebirth of medicine during the Renaissance.
Byzantine physicians often compiled and standardized medical knowledge into textbooks. Their records tended to include both diagnostic explanations and technical drawings. The Medical Compendium in Seven Books, written by the leading physician Paul of Aegina, survived as a particularly thorough source of medical knowledge. This compendium, written in the late seventh century, remained in use as a standard textbook for the following 800 years.
Late antiquity ushered in a revolution in medical science, and historical records often mention civilian hospitals (although battlefield medicine and wartime triage were recorded well before Imperial Rome). Constantinople stood out as a center of medicine during the Middle Ages, which was aided by its crossroads location, wealth, and accumulated knowledge.
The first ever known example of separating conjoined twins occurred in the Byzantine Empire in the 10th century. The next example of separating conjoined twins would be recorded many centuries later in Germany in 1689.
The Byzantine Empire's neighbors, the Persian Sassanid Empire, also made their noteworthy contributions mainly with the establishment of the Academy of Gondeshapur, which was ""the most important medical center of the ancient world during the 6th and 7th centuries."" In addition, Cyril Elgood, British physician and a historian of medicine in Persia, commented that thanks to medical centers like the Academy of Gondeshapur, ""to a very large extent, the credit for the whole hospital system must be given to Persia.""

Islamic medicine

The Islamic civilization rose to primacy in medical science as its physicians contributed significantly to the field of medicine, including anatomy, ophthalmology, pharmacology, pharmacy, physiology, and surgery. Islamic civilization's contribution to these fields within medicine was a gradual process that took hundreds of years. During the time of the first great Muslim dynasty, the Umayyad Caliphate (661–750 CE), these fields that were in their very early stages of development, and not much progress was made. One reason for the limited advancement in medicine during the Umayyad Caliphate was the Caliphate's focus on expansion after the death of Muhammad (632 CE). The focus on expansionism redirected resources from other fields, such as medicine. The priority on these factors led a large percentage of the population to believe that God will provide cures for their illnesses and diseases because of the attention on spirituality.

There were also many other areas of interest during that time before there was a rising interest in the field of medicine. Abd al-Malik ibn Marwan, the fifth caliph of the Umayyad, developed governmental administration, adopted Arabic as the main language, and focused on many other areas. However, this rising interest in Islamic medicine grew significantly when the Abbasid Caliphate (750–1258 CE) overthrew the Umayyad Caliphate in 750 CE. This change in dynasty from the Umayyad Caliphate to the Abbasid Caliphate served as a turning point towards scientific and medical developments. A large contributor to this was that under Abbasid rule there much of the Greek legacy was transmitted into Arabic which by then, was the main language of Islamic nations. Because of this, many Islamic physicians were heavily influenced by the works of Greek scholars of Alexandria and Egypt and were able to further expand on those texts to produce new medical pieces of knowledge. This period of time is also known as the Islamic Golden Age where there was a period of development for development and flourishments of technology, commerce, and sciences including medicine. Additionally, during this time the creation of the first Islamic Hospital in 805 CE by the Abbasid caliph Harun al-Rashid in Baghdad was recounted as a glorious event of the Golden Age. This hospital in Baghdad contributed immensely to Baghdad's success and also provided educational opportunities for Islamic physicians. During the Islamic Golden Age, there were many famous Islamic physicians that paved the way for medical advancements and understandings. However, this would not be possible without the influence from many different areas of the world that influenced the Arabs.
Muslims were influenced by ancient Indian, Persian, Greek, Roman and Byzantine medical practices, and helped them develop further.Galen & Hippocrates were pre-eminent authorities. The translation of 129 of Galen's works into Arabic by the Nestorian Christian Hunayn ibn Ishaq and his assistants, and in particular Galen's insistence on a rational systematic approach to medicine, set the template for Islamic medicine, which rapidly spread throughout the Arab Empire. Its most famous physicians included the Persian polymaths Muhammad ibn Zakarīya al-Rāzi and Avicenna, who wrote more than 40 works on health, medicine, and well-being. Taking leads from Greece and Rome, Islamic scholars kept both the art and science of medicine alive and moving forward. Persian polymath Avicenna has also been called the ""father of medicine"". He wrote The Canon of Medicine which became a standard medical text at many medieval European universities, considered one of the most famous books in the history of medicine. The Canon of Medicine presents an overview of the contemporary medical knowledge of the medieval Islamic world, which had been influenced by earlier traditions including Greco-Roman medicine (particularly Galen), Persian medicine, Chinese medicine and Indian medicine. Persian physician al-Rāzi was one of the first to question the Greek theory of humorism, which nevertheless remained influential in both medieval Western and medieval Islamic medicine. Some volumes of al-Rāzi's work Al-Mansuri, namely ""On Surgery"" and ""A General Book on Therapy"", became part of the medical curriculum in European universities. Additionally, he has been described as a doctor's doctor, the father of pediatrics, and a pioneer of ophthalmology. For example, he was the first to recognize the reaction of the eye's pupil to light.
In addition to contributions to humanity's understanding of human anatomy, Islamicate scientists and scholars, physicians specifically, played an invaluable role in the development of the modern hospital system, creating the foundations on which more contemporary medical professionals would build models of public health systems in Europe and elsewhere. During the time of the Safavid empire (16th–18th centuries) in Iran and the Mughal empire (16th–19th centuries) in India, Muslim scholars radically transformed the institution of the hospital, creating an environment in which rapidly developing medical knowledge of the time could be passed among students and teachers from a wide range of cultures. There were two main schools of thought with patient care at the time. These included humoral physiology from the Persians and Ayurvedic practice. After these theories were translated from Sanskrit to Persian and vice-versa, hospitals could have a mix of culture and techniques. This allowed for a sense of collaborative medicine. Hospitals became increasingly common during this period as wealthy patrons commonly founded them. Many features that are still in use today, such as an emphasis on hygiene, a staff fully dedicated to the care of patients, and separation of individual patients from each other were developed in Islamicate hospitals long before they came into practice in Europe. At the time, the patient care aspects of hospitals in Europe had not taken effect. European hospitals were places of religion rather than institutions of science. As was the case with much of the scientific work done by Islamicate scholars, many of these novel developments in medical practice were transmitted to European cultures hundreds of years after they had long been used throughout the Islamicate world. Although Islamicate scientists were responsible for discovering much of the knowledge that allows the hospital system to function safely today, European scholars who built on this work still receive the majority of the credit historically.
Before the development of scientific medical practices in the Islamicate empires, medical care was mainly performed by religious figures such as priests. Without a profound understanding of how infectious diseases worked and why sickness spread from person to person, these early attempts at caring for the ill and injured often did more harm than good. Contrarily, with the development of new and safer practices by scholars and physicians in hospitals of the Islamic world, ideas vital for the effective care of patients were developed, learned, and transmitted widely. Hospitals developed novel ""concepts and structures"" which are still in use today: separate wards for male and female patients, pharmacies, medical record-keeping, and personal and institutional sanitation and hygiene. Much of this knowledge was recorded and passed on through Islamicate medical texts, many of which were carried to Europe and translated for the use of European medical workers. The Tasrif, written by surgeon Abu Al-Qasim Al-Zahrawi, was translated into Latin; it became one of the most important medical texts in European universities during the Middle Ages and contained useful information on surgical techniques and spread of bacterial infection.
The hospital was a typical institution included in the majority of Muslim cities, and although they were often physically attached to religious institutions, they were not themselves places of religious practice. Rather, they served as facilities in which education and scientific innovation could flourish. If they had places of worship, they were secondary to the medical side of the hospital. Islamicate hospitals, along with observatories used for astronomical science, were some of the most important points of exchange for the spread of scientific knowledge. Undoubtedly, the hospital system developed in the Islamicate world played an invaluable role in the creation and evolution of the hospitals we as a society know and depend on today.


=== Europe ===
After 400 CE, the study and practice of medicine in the Western Roman Empire went into deep decline. Medical services were provided, especially for the poor, in the thousands of monastic hospitals that sprang up across Europe, but the care was rudimentary and mainly palliative. Most of the writings of Galen and Hippocrates were lost to the West, with the summaries and compendia of St. Isidore of Seville being the primary channel for transmitting Greek medical ideas. The Carolingian Renaissance brought increased contact with Byzantium and a greater awareness of ancient medicine, but only with the Renaissance of the 12th century and the new translations coming from Muslim and Jewish sources in Spain, and the fifteenth-century flood of resources after the fall of Constantinople did the West fully recover its acquaintance with classical antiquity.
Greek and Roman taboos had meant that dissection was usually banned in ancient times, but in the Middle Ages it changed: medical teachers and students at Bologna began to open human bodies, and Mondino de Luzzi (c. 1275–1326) produced the first known anatomy textbook based on human dissection.
Wallis identifies a prestige hierarchy with university educated physicians on top, followed by learned surgeons; craft-trained surgeons; barber surgeons; itinerant specialists such as dentist and oculists; empirics; and midwives.


==== Institutions ====
The first medical schools were opened in the 9th century, most notably the Schola Medica Salernitana at Salerno in southern Italy. The cosmopolitan influences from Greek, Latin, Arabic, and Hebrew sources gave it an international reputation as the Hippocratic City. Students from wealthy families came for three years of preliminary studies and five of medical studies. The medicine, following the laws of Federico II, that he founded in 1224 the university and improved the Schola Salernitana, in the period between 1200 and 1400, it had in Sicily (so-called Sicilian Middle Ages) a particular development so much to create a true school of Jewish medicine.
As a result of which, after a legal examination, was conferred to a Jewish Sicilian woman, Virdimura, wife of another physician Pasquale of Catania, the historical record of before woman officially trained to exercise of the medical profession.
At the University of Bologna the training of physicians began in 1219. The Italian city attracted students from across Europe. Taddeo Alderotti built a tradition of medical education that established the characteristic features of Italian learned medicine and was copied by medical schools elsewhere.Turisanus (d. 1320) was his student.
The University of Padua was founded about 1220 by walkouts from the University of Bologna, and began teaching medicine in 1222. It played a leading role in the identification and treatment of diseases and ailments, specializing in autopsies and the inner workings of the body. Starting in 1595, Padua's famous anatomical theatre drew artists and scientists studying the human body during public dissections. The intensive study of Galen led to critiques of Galen modeled on his own writing, as in the first book of Vesalius's De humani corporis fabrica. Andreas Vesalius held the chair of Surgery and Anatomy (explicator chirurgiae) and in 1543 published his anatomical discoveries in De Humani Corporis Fabrica. He portrayed the human body as an interdependent system of organ groupings. The book triggered great public interest in dissections and caused many other European cities to establish anatomical theatres.
By the thirteenth century, the medical school at Montpellier began to eclipse the Salernitan school. In the 12th century, universities were founded in Italy, France, and England, which soon developed schools of medicine. The University of Montpellier in France and Italy's University of Padua and University of Bologna were leading schools. Nearly all the learning was from lectures and readings in Hippocrates, Galen, Avicenna, and Aristotle. In later centuries, the importance of universities founded in the late Middle Ages gradually increased, e.g. Charles University in Prague (established in 1348), Jagiellonian University in Kraków (1364), University of Vienna (1365), Heidelberg University (1386) and University of Greifswald (1456).

		
			
			
		
		
			
			
		
		
			
			
		


==== People ====


== Early modern medicine ==


=== Places ===
England
In England, there were but three small hospitals after 1550. Pelling and Webster estimate that in London in the 1580 to 1600 period, out of a population of nearly 200,000 people, there were about 500 medical practitioners. Nurses and midwives are not included. There were about 50 physicians, 100 licensed surgeons, 100 apothecaries, and 250 additional unlicensed practitioners. In the last category about 25% were women. All across England—and indeed all of the world—the vast majority of the people in city, town or countryside depended for medical care on local amateurs with no professional training but with a reputation as wise healers who could diagnose problems and advise sick people what to do—and perhaps set broken bones, pull a tooth, give some traditional herbs or brews or perform a little magic to cure what ailed them.


=== People ===

		
			
			
		
		
			
			
		
		
			
			
		


=== Europe ===
The Renaissance brought an intense focus on scholarship to Christian Europe. A major effort to translate the Arabic and Greek scientific works into Latin emerged. Europeans gradually became experts not only in the ancient writings of the Romans and Greeks, but in the contemporary writings of Islamic scientists. During the later centuries of the Renaissance came an increase in experimental investigation, particularly in the field of dissection and body examination, thus advancing our knowledge of human anatomy.


==== Ideas ====
Animalcules: In 1677 Antonie van Leeuwenhoek identified ""animalcules"", which we now know as microorganisms, within their paper ""letter on the protozoa"".
Blood circulation: In 1628 the English physician William Harvey made a ground-breaking discovery when he correctly described the circulation of the blood in his Exercitatio Anatomica de Motu Cordis et Sanguinis in Animalibus. Before this time the most useful manual in medicine used both by students and expert physicians was Dioscorides' De Materia Medica, a pharmacopoeia.


==== Inventions ====
Microscopes: Bacteria and protists were first observed with a microscope by Antonie van Leeuwenhoek in 1676, initiating the scientific field of microbiology.


==== Institutions ====
At the University of Bologna the curriculum was revised and strengthened in 1560–1590. A representative professor was Julius Caesar Aranzi (Arantius) (1530–1589). He became Professor of Anatomy and Surgery at the University of Bologna in 1556, where he established anatomy as a major branch of medicine for the first time. Aranzi combined anatomy with a description of pathological processes, based largely on his own research, Galen, and the work of his contemporary Italians. Aranzi discovered the 'Nodules of Aranzio' in the semilunar valves of the heart and wrote the first description of the superior levator palpebral and the coracobrachialis muscles. His books (in Latin) covered surgical techniques for many conditions, including hydrocephalus, nasal polyp, goitre and tumours to phimosis, ascites, haemorrhoids, anal abscess and fistulae.


==== People ====
Women
Catholic women played large roles in health and healing in medieval and early modern Europe. A life as a nun was a prestigious role; wealthy families provided dowries for their daughters, and these funded the convents, while the nuns provided free nursing care for the poor.
The Catholic elites provided hospital services because of their theology of salvation that good works were the route to heaven. The Protestant reformers rejected the notion that rich men could gain God's grace through good works—and thereby escape purgatory—by providing cash endowments to charitable institutions. They also rejected the Catholic idea that the poor patients earned grace and salvation through their suffering. Protestants generally closed all the convents and most of the hospitals, sending women home to become housewives, often against their will. On the other hand, local officials recognized the public value of hospitals, and some were continued in Protestant lands, but without monks or nuns and in the control of local governments.
In London, the crown allowed two hospitals to continue their charitable work, under nonreligious control of city officials. The convents were all shut down but Harkness finds that women—some of them former nuns—were part of a new system that delivered essential medical services to people outside their family. They were employed by parishes and hospitals, as well as by private families, and provided nursing care as well as some medical, pharmaceutical, and surgical services.
Meanwhile, in Catholic lands such as France, rich families continued to fund convents and monasteries, and enrolled their daughters as nuns who provided free health services to the poor. Nursing was a religious role for the nurse, and there was little call for science.


=== Asia ===


==== China ====
In the 18th century, during the Qing dynasty, there was a proliferation of popular books as well as more advanced encyclopedias on traditional medicine. Jesuit missionaries introduced Western science and medicine to the royal court, although the Chinese physicians ignored them.


==== India ====
Unani medicine, based on Avicenna's Canon of Medicine (ca. 1025), was developed in India throughout the Medieval and Early Modern periods. Its use continued, especially in Muslim communities, during the Indian Sultanate and Mughal periods. Unani medicine is in some respects close to Ayurveda and to Early Modern European medicine. All share a theory of the presence of the elements (in Unani, as in Europe, they are considered to be fire, water, earth, and air) and humors in the human body. According to Unani physicians, these elements are present in different humoral fluids and their balance leads to health and their imbalance leads to illness.
Sanskrit medical literature of the Early Modern period included innovative works such as the Compendium of Śārṅgadhara (Skt. Śārṅgadharasaṃhitā, ca. 1350) and especially The Illumination of Bhāva (Bhāvaprakāśa, by Bhāvamiśra, ca. 1550). The latter work also contained an extensive dictionary of materia medica, and became a standard textbook used widely by ayurvedic practitioners in north India up to the present day (2024). Medical innovations of this period included pulse diagnosis, urine diagnosis, the use of mercury and china root to treat syphilis, and the increasing use of metallic ingredients in drugs.
By the 18th century CE, Ayurvedic medical therapy was still widely used among most of the population. Muslim rulers built large hospitals in 1595 in Hyderabad, and in Delhi in 1719, and numerous commentaries on ancient texts were written.

		
			
			
		
		
			
			
		


=== Europe ===


==== Events ====
European Age of Enlightenment
During the Age of Enlightenment, the 18th century, science was held in high esteem and physicians upgraded their social status by becoming more scientific. The health field was crowded with self-trained barber-surgeons, apothecaries, midwives, drug peddlers, and charlatans.
Across Europe medical schools relied primarily on lectures and readings. The final year student would have limited clinical experience by trailing the professor through the wards. Laboratory work was uncommon, and dissections were rarely done because of legal restrictions on cadavers. Most schools were small, and only Edinburgh Medical School, Scotland, with 11,000 alumni, produced large numbers of graduates.


==== Places ====
Spain and the Spanish Empire
In the Spanish Empire, the viceregal capital of Mexico City was a site of medical training for physicians and the creation of hospitals. Epidemic disease had decimated indigenous populations starting with the early sixteenth-century Spanish conquest of the Aztec empire, when a black auxiliary in the armed forces of conqueror Hernán Cortés, with an active case of smallpox, set off a virgin land epidemic among indigenous peoples, Spanish allies and enemies alike. Aztec emperor Cuitlahuac died of smallpox. Disease was a significant factor in the Spanish conquest elsewhere as well.

Medical education instituted at the Royal and Pontifical University of Mexico chiefly served the needs of urban elites. Male and female curanderos or lay practitioners, attended to the ills of the popular classes. The Spanish crown began regulating the medical profession just a few years after the conquest, setting up the Royal Tribunal of the Protomedicato, a board for licensing medical personnel in 1527. Licensing became more systematic after 1646 with physicians, druggists, surgeons, and bleeders requiring a license before they could publicly practice. Crown regulation of medical practice became more general in the Spanish empire.
Elites and the popular classes alike called on divine intervention in personal and society-wide health crises, such as the epidemic of 1737. The intervention of the Virgin of Guadalupe was depicted in a scene of dead and dying Indians, with elites on their knees praying for her aid. In the late eighteenth century, the crown began implementing secularizing policies on the Iberian peninsula and its overseas empire to control disease more systematically and scientifically.
Spanish Quest for Medicinal Spices
Botanical medicines also became popular during the 16th, 17th, and 18th Centuries. Spanish pharmaceutical books during this time contain medicinal recipes consisting of spices, herbs, and other botanical products. For example, nutmeg oil was documented for curing stomach ailments and cardamom oil was believed to relieve intestinal ailments. During the rise of the global trade market, spices and herbs, along with many other goods, that were indigenous to different territories began to appear in different locations across the globe. Herbs and spices were especially popular for their utility in cooking and medicines. As a result of this popularity and increased demand for spices, some areas in Asia, like China and Indonesia, became hubs for spice cultivation and trade. The Spanish Empire also wanted to benefit from the international spice trade, so they looked towards their American colonies.
The Spanish American colonies became an area where the Spanish searched to discover new spices and indigenous American medicinal recipes. The Florentine Codex, a 16th-century ethnographic research study in Mesoamerica by the Spanish Franciscan friar Bernardino de Sahagún, is a major contribution to the history of Nahua medicine. The Spanish did discover many spices and herbs new to them, some of which were reportedly similar to Asian spices. A Spanish physician by the name of Nicolás Monardes studied many of the American spices coming into Spain. He documented many of the new American spices and their medicinal properties in his survey Historia medicinal de las cosas que se traen de nuestras Indias Occidentales. For example, Monardes describes the ""Long Pepper"" (Pimienta luenga), found along the coasts of the countries that are now known Panama and Colombia, as a pepper that was more flavorful, healthy, and spicy in comparison to the Eastern black pepper. The Spanish interest in American spices can first be seen in the commissioning of the Libellus de Medicinalibus Indorum Herbis, which was a Spanish-American codex describing indigenous American spices and herbs and describing the ways that these were used in natural Aztec medicines. The codex was commissioned in the year 1552 by Francisco de Mendoza, the son of Antonio de Mendoza, who was the first Viceroy of New Spain. Francisco de Mendoza was interested in studying the properties of these herbs and spices, so that he would be able to profit from the trade of these herbs and the medicines that could be produced by them.
Francisco de Mendoza recruited the help of Monardez in studying the traditional medicines of the indigenous people living in what was then the Spanish colonies. Monardez researched these medicines and performed experiments to discover the possibilities of spice cultivation and medicine creation in the Spanish colonies. The Spanish transplanted some herbs from Asia, but only a few foreign crops were successfully grown in the Spanish Colonies. One notable crop brought from Asia and successfully grown in the Spanish colonies was ginger, as it was considered Hispaniola's number 1 crop at the end of the 16th Century. The Spanish Empire did profit from cultivating herbs and spices, but they also introduced pre-Columbian American medicinal knowledge to Europe. Other Europeans were inspired by the actions of Spain and decided to try to establish a botanical transplant system in colonies that they controlled, however, these subsequent attempts were not successful.

United Kingdom and the British Empire
The London Dispensary opened in 1696, the first clinic in the British Empire to dispense medicines to poor sick people. The innovation was slow to catch on, but new dispensaries were open in the 1770s. In the colonies, small hospitals opened in Philadelphia in 1752, New York in 1771, and Boston (Massachusetts General Hospital) in 1811.

Guy's Hospital, the first great British hospital with a modern foundation, opened in 1721 in London, with funding from businessman Thomas Guy. It had been preceded by St Bartholomew's Hospital and St Thomas's Hospital, both medieval foundations. In 1821 a bequest of £200,000 by William Hunt in 1829 funded expansion for an additional hundred beds at Guy's. Samuel Sharp (1709–78), a surgeon at Guy's Hospital from 1733 to 1757, was internationally famous; his A Treatise on the Operations of Surgery (1st ed., 1739), was the first British study focused exclusively on operative technique.
English physician Thomas Percival (1740–1804) wrote a comprehensive system of medical conduct, Medical Ethics; or, a Code of Institutes and Precepts, Adapted to the Professional Conduct of Physicians and Surgeons (1803) that set the standard for many textbooks.


== Late modern medicine ==


=== Germ theory and bacteriology ===
In the 1830s in Italy, Agostino Bassi traced the silkworm disease muscardine to microorganisms. Meanwhile, in Germany, Theodor Schwann led research on alcoholic fermentation by yeast, proposing that living microorganisms were responsible.
Leading chemists, such as Justus von Liebig, seeking solely physicochemical explanations, derided this claim and alleged that Schwann was regressing to vitalism.
In 1847 in Vienna, Ignaz Semmelweis (1818–1865), dramatically reduced the death rate of new mothers (due to childbed fever) by requiring physicians to clean their hands before attending childbirth, yet his principles were marginalized and attacked by professional peers. At that time most people still believed that infections were caused by foul odors called miasmas.

French scientist Louis Pasteur confirmed Schwann's fermentation experiments in 1857 and afterwards supported the hypothesis that yeast were microorganisms. Moreover, he suggested that such a process might also explain contagious disease. In 1860, Pasteur's report on bacterial fermentation of butyric acid motivated fellow Frenchman Casimir Davaine to identify a similar species (which he called bacteridia) as the pathogen of the deadly disease anthrax. Others dismissed ""bacteridia"" as a mere byproduct of the disease. British surgeon Joseph Lister, however, took these findings seriously and subsequently introduced antisepsis to wound treatment in 1865.
German physician Robert Koch, noting fellow German Ferdinand Cohn's report of a spore stage of a certain bacterial species, traced the life cycle of Davaine's bacteridia, identified spores, inoculated laboratory animals with them, and reproduced anthrax—a breakthrough for experimental pathology and germ theory of disease. Pasteur's group added ecological investigations confirming spores' role in the natural setting, while Koch published a landmark treatise in 1878 on the bacterial pathology of wounds. In 1881, Koch reported discovery of the ""tubercle bacillus"", cementing germ theory and Koch's acclaim.
Upon the outbreak of a cholera epidemic in Alexandria, Egypt, two medical missions went to investigate and attend the sick, one was sent out by Pasteur and the other led by Koch. Koch's group returned in 1883, having successfully discovered the cholera pathogen. In Germany, however, Koch's bacteriologists had to vie against Max von Pettenkofer, Germany's leading proponent of miasmatic theory. Pettenkofer conceded bacteria's casual involvement, but maintained that other, environmental factors were required to turn it pathogenic, and opposed water treatment as a misdirected effort amid more important ways to improve public health. The massive cholera epidemic in Hamburg in 1892 devastated Pettenkoffer's position, and yielded German public health to ""Koch's bacteriology"".
On losing the 1883 rivalry in Alexandria, Pasteur switched research direction, and introduced his third vaccine—rabies vaccine—the first vaccine for humans since Jenner's for smallpox. From across the globe, donations poured in, funding the founding of Pasteur Institute, the globe's first biomedical institute, which opened in 1888. Along with Koch's bacteriologists, Pasteur's group—which preferred the term microbiology—led medicine into the new era of ""scientific medicine"" upon bacteriology and germ theory. Accepted from Jakob Henle, Koch's steps to confirm a species' pathogenicity became famed as ""Koch's postulates"". Although his proposed tuberculosis treatment, tuberculin, seemingly failed, it soon was used to test for infection with the involved species. In 1905, Koch was awarded the Nobel Prize in Physiology or Medicine, and remains renowned as the founder of medical microbiology.


=== Nursing ===
The breakthrough to professionalization based on knowledge of advanced medicine was led by Florence Nightingale in England. She resolved to provide more advanced training than she saw on the Continent. At Kaiserswerth, where the first German nursing schools were founded in 1836 by Theodor Fliedner, she said, ""The nursing was nil and the hygiene horrible."") Britain's male doctors preferred the old system, but Nightingale won out and her Nightingale Training School opened in 1860 and became a model. The Nightingale solution depended on the patronage of upper-class women, and they proved eager to serve. Royalty became involved. In 1902 the wife of the British king took control of the nursing unit of the British army, became its president, and renamed it after herself as the Queen Alexandra's Royal Army Nursing Corps; when she died the next queen became president. Today its Colonel in Chief is Sophie, Countess of Wessex, the daughter-in-law of Queen Elizabeth II. In the United States, upper-middle-class women who already supported hospitals promoted nursing. The new profession proved highly attractive to women of all backgrounds, and schools of nursing opened in the late 19th century. Nurses were soon a part of large hospitals, where they provided a steady stream of low-paid idealistic workers. The International Red Cross began operations in numerous countries in the late 19th century, promoting nursing as an ideal profession for middle-class women.


=== Statistical methods ===

A major breakthrough in epidemiology came with the introduction of statistical maps and graphs. They allowed careful analysis of seasonality issues in disease incidents, and the maps allowed public health officials to identify critical loci for the dissemination of disease. John Snow in London developed the methods. In 1849, he observed that the symptoms of cholera, which had already claimed around 500 lives within a month, were vomiting and diarrhoea. He concluded that the source of contamination must be through ingestion, rather than inhalation as was previously thought. It was this insight that resulted in the removal of The Pump On Broad Street, after which deaths from cholera plummeted. English nurse Florence Nightingale pioneered analysis of large amounts of statistical data, using graphs and tables, regarding the condition of thousands of patients in the Crimean War to evaluate the efficacy of hospital services. Her methods proved convincing and led to reforms in military and civilian hospitals, usually with the full support of the government.
By the late 19th and early 20th century English statisticians led by Francis Galton, Karl Pearson and Ronald Fisher developed the mathematical tools such as correlations and hypothesis tests that made possible much more sophisticated analysis of statistical data.
During the U.S. Civil War the Sanitary Commission collected enormous amounts of statistical data, and opened up the problems of storing information for fast access and mechanically searching for data patterns. The pioneer was John Shaw Billings (1838–1913). A senior surgeon in the war, Billings built the Library of the Surgeon General's Office (now the National Library of Medicine), the centerpiece of modern medical information systems. Billings figured out how to mechanically analyze medical and demographic data by turning facts into numbers and punching the numbers onto cardboard cards that could be sorted and counted by machine. The applications were developed by his assistant Herman Hollerith; Hollerith invented the punch card and counter-sorter system that dominated statistical data manipulation until the 1970s. Hollerith's company became International Business Machines (IBM) in 1911.


=== Psychiatry ===

Until the nineteenth century, the care of the insane was largely a communal and family responsibility rather than a medical one. The vast majority of the mentally ill were treated in domestic contexts with only the most unmanageable or burdensome likely to be institutionally confined. This situation was transformed radically from the late eighteenth century as, amid changing cultural conceptions of madness, a new-found optimism in the curability of insanity within the asylum setting emerged. Increasingly, lunacy was perceived less as a physiological condition than as a mental and moral one to which the correct response was persuasion, aimed at inculcating internal restraint, rather than external coercion. This new therapeutic sensibility, referred to as moral treatment, was epitomised in French physician Philippe Pinel's quasi-mythological unchaining of the lunatics of the Bicêtre Hospital in Paris and realised in an institutional setting with the foundation in 1796 of the Quaker-run York Retreat in England.

From the early nineteenth century, as lay-led lunacy reform movements gained in influence, ever more state governments in the West extended their authority and responsibility over the mentally ill. Small-scale asylums, conceived as instruments to reshape both the mind and behaviour of the disturbed, proliferated across these regions. By the 1830s, moral treatment, together with the asylum itself, became increasingly medicalised and asylum doctors began to establish a distinct medical identity with the establishment in the 1840s of associations for their members in France, Germany, the United Kingdom and America, together with the founding of medico-psychological journals. Medical optimism in the capacity of the asylum to cure insanity soured by the close of the nineteenth century as the growth of the asylum population far outstripped that of the general population. Processes of long-term institutional segregation, allowing for the psychiatric conceptualisation of the natural course of mental illness, supported the perspective that the insane were a distinct population, subject to mental pathologies stemming from specific medical causes. As degeneration theory grew in influence from the mid-nineteenth century, heredity was seen as the central causal element in chronic mental illness, and, with national asylum systems overcrowded and insanity apparently undergoing an inexorable rise, the focus of psychiatric therapeutics shifted from a concern with treating the individual to maintaining the racial and biological health of national populations.

Emil Kraepelin (1856–1926) introduced new medical categories of mental illness, which eventually came into psychiatric usage despite their basis in behavior rather than pathology or underlying cause. Shell shock among frontline soldiers exposed to heavy artillery bombardment was first diagnosed by British Army doctors in 1915. By 1916, similar symptoms were also noted in soldiers not exposed to explosive shocks, leading to questions as to whether the disorder was physical or psychiatric. In the 1920s surrealist opposition to psychiatry was expressed in a number of surrealist publications. In the 1930s several controversial medical practices were introduced including inducing seizures (by electroshock, insulin or other drugs) or cutting parts of the brain apart (leucotomy or lobotomy). Both came into widespread use by psychiatry, but there were grave concerns and much opposition on grounds of basic morality, harmful effects, or misuse.
In the 1950s new psychiatric drugs, notably the antipsychotic chlorpromazine, were designed in laboratories and slowly came into preferred use. Although often accepted as an advance in some ways, there was some opposition, due to serious adverse effects such as tardive dyskinesia. Patients often opposed psychiatry and refused or stopped taking the drugs when not subject to psychiatric control. There was also increasing opposition to the use of psychiatric hospitals, and attempts to move people back into the community on a collaborative user-led group approach (""therapeutic communities"") not controlled by psychiatry. Campaigns against masturbation were done in the Victorian era and elsewhere. Lobotomy was used until the 1970s to treat schizophrenia. This was denounced by the anti-psychiatric movement in the 1960s and later.


=== Women ===
It was very difficult for women to become doctors in any field before the 1970s. Elizabeth Blackwell became the first woman to formally study and practice medicine in the United States. She was a leader in women's medical education. While Blackwell viewed medicine as a means for social and moral reform, her student Mary Putnam Jacobi (1842–1906) focused on curing disease. At a deeper level of disagreement, Blackwell felt that women would succeed in medicine because of their humane female values, but Jacobi believed that women should participate as the equals of men in all medical specialties using identical methods, values and insights. In the Soviet Union although the majority of medical doctors were women, they were paid less than the mostly male factory workers.


=== Asia ===


==== Places ====
China
Finally in the 19th century, Western medicine was introduced at the local level by Christian medical missionaries from the London Missionary Society (Britain), the Methodist Church (Britain) and the Presbyterian Church (US). Benjamin Hobson (1816–1873) in 1839, set up a highly successful Wai Ai Clinic in Guangzhou, China. The Hong Kong College of Medicine for Chinese was founded in 1887 by the London Missionary Society, with its first graduate (in 1892) being Sun Yat-sen, who later led the Chinese Revolution (1911). The Hong Kong College of Medicine for Chinese was the forerunner of the School of Medicine of the University of Hong Kong, which started in 1911.
Because of the social custom that men and women should not be near to one another, the women of China were reluctant to be treated by male doctors. The missionaries sent women doctors such as Dr. Mary Hannah Fulton (1854–1927). Supported by the Foreign Missions Board of the Presbyterian Church (US) she in 1902 founded the first medical college for women in China, the Hackett Medical College for Women, in Guangzhou.

Japan
European ideas of modern medicine were spread widely through the world by medical missionaries, and the dissemination of textbooks. Japanese elites enthusiastically embraced Western medicine after the Meiji Restoration of the 1860s. However they had been prepared by their knowledge of the Dutch and German medicine, for they had some contact with Europe through the Dutch. Highly influential was the 1765 edition of Hendrik van Deventer's pioneer work Nieuw Ligt (""A New Light"") on Japanese obstetrics, especially on Katakura Kakuryo's publication in 1799 of Sanka Hatsumo (""Enlightenment of Obstetrics""). A cadre of Japanese physicians began to interact with Dutch doctors, who introduced smallpox vaccinations. By 1820 Japanese ranpô medical practitioners not only translated Dutch medical texts, they integrated their readings with clinical diagnoses. These men became leaders of the modernization of medicine in their country. They broke from Japanese traditions of closed medical fraternities and adopted the European approach of an open community of collaboration based on expertise in the latest scientific methods.
Kitasato Shibasaburō (1853–1931) studied bacteriology in Germany under Robert Koch. In 1891 he founded the Institute of Infectious Diseases in Tokyo, which introduced the study of bacteriology to Japan. He and French researcher Alexandre Yersin went to Hong Kong in 1894, where; Kitasato confirmed Yersin's discovery that the bacterium Yersinia pestis is the agent of the plague. In 1897 he isolated and described the organism that caused dysentery. He became the first dean of medicine at Keio University, and the first president of the Japan Medical Association.
Japanese physicians immediately recognized the values of X-Rays. They were able to purchase the equipment locally from the Shimadzu Company, which developed, manufactured, marketed, and distributed X-Ray machines after 1900. Japan not only adopted German methods of public health in the home islands, but implemented them in its colonies, especially Korea and Taiwan, and after 1931 in Manchuria. A heavy investment in sanitation resulted in a dramatic increase of life expectancy.


=== Europe ===
The practice of medicine changed in the face of rapid advances in science, as well as new approaches by physicians. Hospital doctors began much more systematic analysis of patients' symptoms in diagnosis. Among the more powerful new techniques were anaesthesia, and the development of both antiseptic and aseptic operating theatres. Effective cures were developed for certain endemic infectious diseases. However, the decline in many of the most lethal diseases was due more to improvements in public health and nutrition than to advances in medicine.
Medicine was revolutionized in the 19th century and beyond by advances in chemistry, laboratory techniques, and equipment. Old ideas of infectious disease epidemiology were gradually replaced by advances in bacteriology and virology.
The Russian Orthodox Church sponsored seven orders of nursing sisters in the late 19th century. They ran hospitals, clinics, almshouses, pharmacies, and shelters as well as training schools for nurses. In the Soviet era (1917–1991), with the aristocratic sponsors gone, nursing became a low-prestige occupation based in poorly maintained hospitals.


=== Places ===


==== France ====
Paris (France) and Vienna were the two leading medical centers on the Continent in the era 1750–1914.
In the 1770s–1850s Paris became a world center of medical research and teaching. The ""Paris School"" emphasized that teaching and research should be based in large hospitals and promoted the professionalization of the medical profession and the emphasis on sanitation and public health. A major reformer was Jean-Antoine Chaptal (1756–1832), a physician who was Minister of Internal Affairs. He created the Paris Hospital, health councils, and other bodies.
Louis Pasteur (1822–1895) was one of the most important founders of medical microbiology. He is remembered for his remarkable breakthroughs in the causes and preventions of diseases. His discoveries reduced mortality from puerperal fever, and he created the first vaccines for rabies and anthrax. His experiments supported the germ theory of disease. He was best known to the general public for inventing a method to treat milk and wine to prevent it from causing sickness, a process that came to be called pasteurization. He is regarded as one of the three main founders of microbiology, together with Ferdinand Cohn and Robert Koch. He worked chiefly in Paris and in 1887 founded the Pasteur Institute there to perpetuate his commitment to basic research and its practical applications. As soon as his institute was created, Pasteur brought together scientists with various specialties. The first five departments were directed by Emile Duclaux (general microbiology research) and Charles Chamberland (microbe research applied to hygiene), as well as a biologist, Ilya Ilyich Mechnikov (morphological microbe research) and two physicians, Jacques-Joseph Grancher (rabies) and Emile Roux (technical microbe research). One year after the inauguration of the Institut Pasteur, Roux set up the first course of microbiology ever taught in the world, then entitled Cours de Microbie Technique (Course of microbe research techniques). It became the model for numerous research centers around the world named ""Pasteur Institutes.


==== Vienna ====
The First Viennese School of Medicine, 1750–1800, was led by the Dutchman Gerard van Swieten (1700–1772), who aimed to put medicine on new scientific foundations—promoting unprejudiced clinical observation, botanical and chemical research, and introducing simple but powerful remedies. When the Vienna General Hospital opened in 1784, it at once became the world's largest hospital and physicians acquired a facility that gradually developed into the most important research centre. Progress ended with the Napoleonic wars and the government shutdown in 1819 of all liberal journals and schools; this caused a general return to traditionalism and eclecticism in medicine.
Vienna was the capital of a diverse empire and attracted not just Germans but Czechs, Hungarians, Jews, Poles and others to its world-class medical facilities. After 1820 the Second Viennese School of Medicine emerged with the contributions of physicians such as Carl Freiherr von Rokitansky, Josef Škoda, Ferdinand Ritter von Hebra, and Ignaz Philipp Semmelweis. Basic medical science expanded and specialization advanced. Furthermore, the first dermatology, eye, as well as ear, nose, and throat clinics in the world were founded in Vienna. The textbook of ophthalmologist Georg Joseph Beer (1763–1821) Lehre von den Augenkrankheiten combined practical research and philosophical speculations, and became the standard reference work for decades.


==== Berlin ====

After 1871 Berlin, the capital of the new German Empire, became a leading center for medical research. The Charité is tracing back its origins to the year 1710. More than half of all German Nobel Prize winners in Physiology or Medicine, including Emil von Behring, Robert Koch and Paul Ehrlich, worked there. Koch, (1843–1910), was a representative leader. He became famous for isolating Bacillus anthracis (1877), the Tuberculosis bacillus (1882) and Vibrio cholerae (1883) and for his development of Koch's postulates. He was awarded the Nobel Prize in Physiology or Medicine in 1905 for his tuberculosis findings. Koch is one of the founders of microbiology and modern medicine. He inspired such major figures as Ehrlich, who discovered the first antibiotic, arsphenamine and Gerhard Domagk, who created the first commercially available antibiotic, Prontosil.


=== North America ===


==== Events ====
American Civil War 
In the American Civil War (1861–65), as was typical of the 19th century, more soldiers died of disease than in battle, and even larger numbers were temporarily incapacitated by wounds, disease and accidents. Conditions were poor in the Confederacy, where doctors and medical supplies were in short supply. The war had a dramatic long-term impact on medicine in the U.S., from surgical technique to hospitals to nursing and to research facilities. Weapon development -particularly the appearance of Springfield Model 1861, mass-produced and much more accurate than muskets led to generals underestimating the risks of long range rifle fire; risks exemplified in the death of John Sedgwick and the disastrous Pickett's Charge. The rifles could shatter bone forcing amputation and longer ranges meant casualties were sometimes not quickly found. Evacuation of the wounded from Second Battle of Bull Run took a week. As in earlier wars, untreated casualties sometimes survived unexpectedly due to maggots debriding the wound -an observation which led to the surgical use of maggots -still a useful method in the absence of effective antibiotics.
The hygiene of the training and field camps was poor, especially at the beginning of the war when men who had seldom been far from home were brought together for training with thousands of strangers. First came epidemics of the childhood diseases of chicken pox, mumps, whooping cough, and, especially, measles. Operations in the South meant a dangerous and new disease environment, bringing diarrhea, dysentery, typhoid fever, and malaria. There were no antibiotics, so the surgeons prescribed coffee, whiskey, and quinine. Harsh weather, bad water, inadequate shelter in winter quarters, poor policing of camps, and dirty camp hospitals took their toll.
This was a common scenario in wars from time immemorial, and conditions faced by the Confederate army were even worse. The Union responded by building army hospitals in every state. What was different in the Union was the emergence of skilled, well-funded medical organizers who took proactive action, especially in the much enlarged United States Army Medical Department, and the United States Sanitary Commission, a new private agency. Numerous other new agencies also targeted the medical and morale needs of soldiers, including the United States Christian Commission as well as smaller private agencies.
The U.S. Army learned many lessons and in August 1886, it established the Hospital Corps.


=== Institutions ===
Johns Hopkins Hospital, founded in 1889, originated several modern medical practices, including residency and rounds.


=== People ===

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		


=== Cardiovascular ===


==== Blood groups ====
The ABO blood group system was discovered in 1901 by Karl Landsteiner at the University of Vienna. Landsteiner experimented on his staff, mixing their various blood components together, and found that some people's blood agglutinated (clumped together) with other blood, while some did not. This then lead him identifying three blood groups, ABC, which would later be renamed to ABO. The less frequently found blood group AB was discovered later in 1902 by Alfred Von Decastello and Adriano Sturli. In 1937 Landsteiner and Alexander S. Wiener further discovered the Rh factor (misnamed from early thinking that this blood group was similar to that found in rhesus monkeys) whose antigens further determine blood reaction between people. This was demonstrated in a 1939 case study by Phillip Levine and Rufus Stetson where a mother who had recently given birth had reacted to their partner's blood, highlighting the Rh factor.


==== Blood transfusion ====
Canadian physician Norman Bethune, M.D. developed a mobile blood-transfusion service for frontline operations in the Spanish Civil War (1936–1939), but ironically, he himself died of sepsis.


==== Pacemaker ====
In 1958, Arne Larsson in Sweden became the first patient to depend on an artificial cardiac pacemaker. He died in 2001 at age 86, having outlived its inventor, the surgeon, and 26 pacemakers.


=== Cancer ===
Cancer treatment has been developed with radiotherapy, chemotherapy and surgical oncology.


=== Diagnosis ===
X-ray imaging was the first kind of medical imaging, and later ultrasonic imaging, CT scanning, MR scanning and other imaging methods became available.


=== Disabilities ===
 Prosthetics have improved with lightweight materials as well as neural prosthetics emerging in the end of the 20th century.


=== Diseases ===
Oral rehydration therapy has been extensively used since the 1970s to treat cholera and other diarrhea-inducing infections.
As infectious diseases have become less lethal, and the most common causes of death in developed countries are now tumors and cardiovascular diseases, these conditions have received increased attention in medical research.


=== Disease eradication ===


==== Malaria eradication ====

Starting in World War II, DDT was used as insecticide to combat insect vectors carrying malaria, which was endemic in most tropical regions of the world. The first goal was to protect soldiers, but it was widely adopted as a public health device. In Liberia, for example, the United States had large military operations during the war and the U.S. Public Health Service began the use of DDT for indoor residual spraying (IRS) and as a larvicide, with the goal of controlling malaria in Monrovia, the Liberian capital. In the early 1950s, the project was expanded to nearby villages. In 1953, the World Health Organization (WHO) launched an antimalaria program in parts of Liberia as a pilot project to determine the feasibility of malaria eradication in tropical Africa. However these projects encountered a spate of difficulties that foreshadowed the general retreat from malaria eradication efforts across tropical Africa by the mid-1960s.


=== Pandemics ===


==== 1918 influenza pandemic (1918-1920) ====
The 1918 influenza pandemic was a global pandemic in the early 20th century that occurred between 1918 and 1920. Sometimes known as Spanish Flu due to popular opinion at the time thinking the flu originated from Spain, this pandemic caused close to 50 million deaths around the world. Spreading at the end of World War I.


=== Public health ===
Public health measures became particularly important during the 1918 flu pandemic, which killed at least 50 million people around the world. It became an important case study in epidemiology. Bristow shows there was a gendered response of health caregivers to the pandemic in the United States. Male doctors were unable to cure the patients, and they felt like failures. Women nurses also saw their patients die, but they took pride in their success in fulfilling their professional role of caring for, ministering, comforting, and easing the last hours of their patients, and helping the families of the patients cope as well.


=== Research ===
Evidence-based medicine is a modern concept, not introduced to literature until the 1990s.


=== Sexual and reproductive health ===
The sexual revolution included taboo-breaking research in human sexuality such as the 1948 and 1953 Kinsey reports, invention of hormonal contraception, and the normalization of abortion and homosexuality in many countries. Family planning has promoted a demographic transition in most of the world. With threatening sexually transmitted infections, not least HIV, use of barrier contraception has become imperative. The struggle against HIV has improved antiretroviral treatments.


=== Smoking ===
Tobacco smoking as a cause of lung cancer was first researched in the 1920s, but was not widely supported by publications until the 1950s.


=== Surgery ===
Cardiac surgery was revolutionized in 1948 as open-heart surgery was introduced for the first time since 1925. In 1954 Joseph Murray, J. Hartwell Harrison and others accomplished the first kidney transplantation. Transplantations of other organs, such as heart, liver and pancreas, were also introduced during the later 20th century. The first partial face transplant was performed in 2005, and the first full one in 2010. By the end of the 20th century, microtechnology had been used to create tiny robotic devices to assist microsurgery using micro-video and fiber-optic cameras to view internal tissues during surgery with minimally invasive practices. Laparoscopic surgery was broadly introduced in the 1990s. Natural orifice surgery has followed.


=== War ===


==== Mexican Revolution (1910-1920) ====
During the 19th century, large-scale wars were attended with medics and mobile hospital units which developed advanced techniques for healing massive injuries and controlling infections rampant in battlefield conditions. During the Mexican Revolution (1910–1920), General Pancho Villa organized hospital trains for wounded soldiers. Boxcars marked Servicio Sanitario (""sanitary service"") were re-purposed as surgical operating theaters and areas for recuperation, and staffed by up to 40 Mexican and U.S. physicians. Severely wounded soldiers were shuttled back to base hospitals.


==== World War I (1914-1918) ====
Thousands of scarred troops provided the need for improved prosthetic limbs and expanded techniques in plastic surgery or reconstructive surgery. Those practices were combined to broaden cosmetic surgery and other forms of elective surgery.


==== Interwar period (1918–1939) ====
From 1917 to 1932, the American Red Cross moved into Europe with a battery of long-term child health projects. It built and operated hospitals and clinics, and organized antituberculosis and antityphus campaigns. A high priority involved child health programs such as clinics, better baby shows, playgrounds, fresh air camps, and courses for women on infant hygiene. Hundreds of U.S. doctors, nurses, and welfare professionals administered these programs, which aimed to reform the health of European youth and to reshape European public health and welfare along American lines.


==== World War II (1939-1945) ====

The advances in medicine made a dramatic difference for Allied troops, while the Germans and especially the Japanese and Chinese suffered from a severe lack of newer medicines, techniques and facilities. Harrison finds that the chances of recovery for a badly wounded British infantryman were as much as 25 times better than in the First World War. The reason was that:

""By 1944 most casualties were receiving treatment within hours of wounding, due to the increased mobility of field hospitals and the extensive use of aeroplanes as ambulances. The care of the sick and wounded had also been revolutionized by new medical technologies, such as active immunization against tetanus, sulphonamide drugs, and penicillin.""
During the second World War, Alexis Carrel and Henry Dakin developed the Carrel-Dakin method of treating wounds with an irrigation, Dakin's solution, a germicide which helped prevent gangrene.
The War spurred the usage of Roentgen's X-ray, and the electrocardiograph, for the monitoring of internal bodily functions. This was followed in the inter-war period by the development of the first anti-bacterial agents such as the sulpha antibiotics.
Nazi and Japanese medical research
Unethical human subject research, and killing of patients with disabilities, peaked during the Nazi era, with Nazi human experimentation and Aktion T4 during the Holocaust as the most significant examples. Many of the details of these and related events were the focus of the Doctors' Trial. Subsequently, principles of medical ethics, such as the Nuremberg Code, were introduced to prevent a recurrence of such atrocities. After 1937, the Japanese Army established programs of biological warfare in China. In Unit 731, Japanese doctors and research scientists conducted large numbers of vivisections and experiments on human beings, mostly Chinese victims.


=== Institutions ===


==== World Health Organization ====
The World Health Organization was founded in 1948 as a United Nations agency to improve global health. In most of the world, life expectancy has improved since then, and was about 67 years as of 2010, and well above 80 years in some countries. Eradication of infectious diseases is an international effort, and several new vaccines have been developed during the post-war years, against infections such as measles, mumps, several strains of influenza and human papilloma virus. The long-known vaccine against Smallpox finally eradicated the disease in the 1970s, and Rinderpest was wiped out in 2011. Eradication of polio is underway. Tissue culture is important for development of vaccines. Though the early success of antiviral vaccines and antibacterial drugs, antiviral drugs were not introduced until the 1970s. Through the WHO, the international community has developed a response protocol against epidemics, displayed during the SARS epidemic in 2003, the Influenza A virus subtype H5N1 from 2004, the Ebola virus epidemic in West Africa and onwards.


=== People ===

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		


== Contemporary medicine ==


=== Antibiotics and antibiotic resistance ===
The discovery of penicillin in the 20th century by Alexander Fleming provided a vital line of defence against bacterical infections that, without them, often cause patients to suffer prelonged recovery periods and highly increased chances of death. Its discovery and application within medicine allowed previously impossible treatments to take place, including cancer treatments, organ transplants, to open heart surgery. Throughout the 20th century, though, their overprescribed use to humans, as well as to animals that need them due to the conditions of intensive animal farming, has led to the development of antibiotic resistant bacteria.


=== Robotics ===


=== HIV First death ===

The early 21st century, facilitated by extensive global connections, international travel, and unprecedented human disruption of ecological systems, has been defined by a number of noval as well as continuing global pandemics from the 20th century.


==== Past ====
The SARS 2002 to 2004 outbreak affected a number of countries around the world and killed hundreds. This outbreak gave rise to a number of lessons learnt from viral infection control, including more effective isolation room protocols to better hand washing techniques for medical staff. A mutated strain of SARS would go on to develop into COVID-19, causing the future COVID-19 pandemic. A significant influenza strain, H1N1, caused a further pandemic between 2009 and 2010. Known as swine flu, due to its indirect source from pigs, it went on to infect over 700 million people.


==== Ongoing ====
The continuing HIV pandemic, starting in 1981, has infected and led to the deaths of millions of people around the world. Emerging and improved pre-exposure prophylaxis (PrEP) and post-exposure prophylaxis (PEP) treatments that aim to reduce the spread of the disease have proven effective in limiting the spread of HIV alongside combined use of safe sex methods, sexual health education, needle exchange programmes, and sexual health screenings. Efforts to find a HIV vaccine are ongoing while health inequities have left certain population groups, like trans women, as well as resource limited regions, like sub-Saharan Africa, at greater risk of contracting HIV compared with, for example, developed countries.
The outbreak of COVID-19, starting in 2019, and subsequent declaration of the COVID-19 pandemic by the WHO is a major pandemic event within the early 21st century. Causing global disruptions, millions of infections and deaths, the pandemic has caused suffering throughout communities. The pandemic has also seen some of the largest logistical organisations of goods, medical equipment, medical professionals, and military personnel since World War II that highlights its far-reaching impact.


=== Personalised medicine ===
The rise of personalised medicine in the 21st century has generated the possibility to develop diagnosis and treatments based on the individual characteristics of a person, rather than through generic practices that defined 20th century medicine. Areas like DNA sequencing, genetic mapping, gene therapy, imaging protocols, proteomics, stem cell therapy, and wireless health monitoring devices are all rising innovations that can help medical professionals fine tune treatment to the individual.


=== Telemedicine ===
Remote surgery is another recent development, with the transatlantic Lindbergh operation in 2001 as a groundbreaking example.


=== Institutions ===


=== People ===

		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		
		
			
			
		


== Themes in medical history ==


=== Racism in medicine ===
Racism has a long history in how medicine has evolved and established itself, both in terms of racism experience upon patients, professionals, and wider systematic violence within medical institutions and systems. See: medical racism in the United States, race and health, and scientific racism.


=== Women in medicine ===

Women have always served as healers and midwives since ancient times. However, the professionalization of medicine forced them increasingly to the sidelines. As hospitals multiplied they relied in Europe on orders of Roman Catholic nun-nurses, and German Protestant and Anglican deaconesses in the early 19th century. They were trained in traditional methods of physical care that involved little knowledge of medicine.


== See also ==


== Explanatory notes ==


== References ==


== Further reading ==


== External links ==

 Texts on Wikisource:
Senfelder L (1911). ""History of Medicine"". Catholic Encyclopedia. Vol. 10.
The history of medicine and surgery as portrayed by various artists
Directory of History of Medicine Collections Archived 2013-09-16 at the Wayback Machine, Index to the major collections in the United States and Canada, selected by the US National Institute of Health
Newsletter / Hannah Institute for the History of Medicine Archived 2024-05-02 at the Wayback Machine. Date: [1988-1997], Wellcome Collection"
Artificial Intelligence,"Artificial intelligence (AI), in its broadest sense, is intelligence exhibited by machines, particularly computer systems. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals. Such machines may be called AIs.
Some high-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); interacting via human speech (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., ChatGPT, and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: ""A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.""
The various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include reasoning, knowledge representation, planning, learning, natural language processing, perception, and support for robotics. General intelligence—the ability to complete any task performable by a human on an at least equal level—is among the field's long-term goals. To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics. AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.
Artificial intelligence was founded as an academic discipline in 1956, and the field went through multiple cycles of optimism, followed by periods of disappointment and loss of funding, known as AI winter. Funding and interest vastly increased after 2012 when deep learning outperformed previous AI techniques. This growth accelerated further after 2017 with the transformer architecture, and by the early 2020s hundreds of billions of dollars were being invested in AI (known as the ""AI boom""). The widespread use of AI in the 21st century exposed several unintended consequences and harms in the present and raised concerns about its risks and long-term effects in the future, prompting discussions about regulatory policies to ensure the safety and benefits of the technology.


== Goals ==
The general problem of simulating (or creating) intelligence has been broken into subproblems. These consist of particular traits or capabilities that researchers expect an intelligent system to display. The traits described below have received the most attention and cover the scope of AI research.


=== Reasoning and problem-solving ===
Early researchers developed algorithms that imitated step-by-step reasoning that humans use when they solve puzzles or make logical deductions. By the late 1980s and 1990s, methods were developed for dealing with uncertain or incomplete information, employing concepts from probability and economics.
Many of these algorithms are insufficient for solving large reasoning problems because they experience a ""combinatorial explosion"": They become exponentially slower as the problems grow. Even humans rarely use the step-by-step deduction that early AI research could model. They solve most of their problems using fast, intuitive judgments. Accurate and efficient reasoning is an unsolved problem.


=== Knowledge representation ===

Knowledge representation and knowledge engineering allow AI programs to answer questions intelligently and make deductions about real-world facts. Formal knowledge representations are used in content-based indexing and retrieval, scene interpretation, clinical decision support, knowledge discovery (mining ""interesting"" and actionable inferences from large databases), and other areas.
A knowledge base is a body of knowledge represented in a form that can be used by a program. An ontology is the set of objects, relations, concepts, and properties used by a particular domain of knowledge. Knowledge bases need to represent things such as objects, properties, categories, and relations between objects; situations, events, states, and time; causes and effects; knowledge about knowledge (what we know about what other people know); default reasoning (things that humans assume are true until they are told differently and will remain true even when other facts are changing); and many other aspects and domains of knowledge.
Among the most difficult problems in knowledge representation are the breadth of commonsense knowledge (the set of atomic facts that the average person knows is enormous); and the sub-symbolic form of most commonsense knowledge (much of what people know is not represented as ""facts"" or ""statements"" that they could express verbally). There is also the difficulty of knowledge acquisition, the problem of obtaining knowledge for AI applications.


=== Planning and decision-making ===
An ""agent"" is anything that perceives and takes actions in the world. A rational agent has goals or preferences and takes actions to make them happen. In automated planning, the agent has a specific goal. In automated decision-making, the agent has preferences—there are some situations it would prefer to be in, and some situations it is trying to avoid. The decision-making agent assigns a number to each situation (called the ""utility"") that measures how much the agent prefers it. For each possible action, it can calculate the ""expected utility"": the utility of all possible outcomes of the action, weighted by the probability that the outcome will occur. It can then choose the action with the maximum expected utility.
In classical planning, the agent knows exactly what the effect of any action will be. In most real-world problems, however, the agent may not be certain about the situation they are in (it is ""unknown"" or ""unobservable"") and it may not know for certain what will happen after each possible action (it is not ""deterministic""). It must choose an action by making a probabilistic guess and then reassess the situation to see if the action worked.
In some problems, the agent's preferences may be uncertain, especially if there are other agents or humans involved. These can be learned (e.g., with inverse reinforcement learning), or the agent can seek information to improve its preferences. Information value theory can be used to weigh the value of exploratory or experimental actions. The space of possible future actions and situations is typically intractably large, so the agents must take actions and evaluate situations while being uncertain of what the outcome will be.
A Markov decision process has a transition model that describes the probability that a particular action will change the state in a particular way and a reward function that supplies the utility of each state and the cost of each action. A policy associates a decision with each possible state. The policy could be calculated (e.g., by iteration), be heuristic, or it can be learned.
Game theory describes the rational behavior of multiple interacting agents and is used in AI programs that make decisions that involve other agents.


=== Learning ===
Machine learning is the study of programs that can improve their performance on a given task automatically. It has been a part of AI from the beginning.
There are several kinds of machine learning. Unsupervised learning analyzes a stream of data and finds patterns and makes predictions without any other guidance. Supervised learning requires a human to label the input data first, and comes in two main varieties: classification (where the program must learn to predict what category the input belongs in) and regression (where the program must deduce a numeric function based on numeric input).
In reinforcement learning, the agent is rewarded for good responses and punished for bad ones. The agent learns to choose responses that are classified as ""good"". Transfer learning is when the knowledge gained from one problem is applied to a new problem. Deep learning is a type of machine learning that runs inputs through biologically inspired artificial neural networks for all of these types of learning.
Computational learning theory can assess learners by computational complexity, by sample complexity (how much data is required), or by other notions of optimization.


=== Natural language processing ===
Natural language processing (NLP) allows programs to read, write and communicate in human languages such as English. Specific problems include speech recognition, speech synthesis, machine translation, information extraction, information retrieval and question answering.
Early work, based on Noam Chomsky's generative grammar and semantic networks, had difficulty with word-sense disambiguation unless restricted to small domains called ""micro-worlds"" (due to the common sense knowledge problem). Margaret Masterman believed that it was meaning and not grammar that was the key to understanding languages, and that thesauri and not dictionaries should be the basis of computational language structure.
Modern deep learning techniques for NLP include word embedding (representing words, typically as vectors encoding their meaning), transformers (a deep learning architecture using an attention mechanism), and others. In 2019, generative pre-trained transformer (or ""GPT"") language models began to generate coherent text, and by 2023, these models were able to get human-level scores on the bar exam, SAT test, GRE test, and many other real-world applications.


=== Perception ===
Machine perception is the ability to use input from sensors (such as cameras, microphones, wireless signals, active lidar, sonar, radar, and tactile sensors) to deduce aspects of the world. Computer vision is the ability to analyze visual input.
The field includes speech recognition, image classification, facial recognition, object recognition,object tracking, and robotic perception.


=== Social intelligence ===

Affective computing is an interdisciplinary umbrella that comprises systems that recognize, interpret, process, or simulate human feeling, emotion, and mood. For example, some virtual assistants are programmed to speak conversationally or even to banter humorously; it makes them appear more sensitive to the emotional dynamics of human interaction, or to otherwise facilitate human–computer interaction.
However, this tends to give naïve users an unrealistic conception of the intelligence of existing computer agents. Moderate successes related to affective computing include textual sentiment analysis and, more recently, multimodal sentiment analysis, wherein AI classifies the affects displayed by a videotaped subject.


=== General intelligence ===
A machine with artificial general intelligence should be able to solve a wide variety of problems with breadth and versatility similar to human intelligence.


== Techniques ==
AI research uses a wide variety of techniques to accomplish the goals above.


=== Search and optimization ===
AI can solve many problems by intelligently searching through many possible solutions. There are two very different kinds of search used in AI: state space search and local search.


==== State space search ====
State space search searches through a tree of possible states to try to find a goal state. For example, planning algorithms search through trees of goals and subgoals, attempting to find a path to a target goal, a process called means-ends analysis.
Simple exhaustive searches are rarely sufficient for most real-world problems: the search space (the number of places to search) quickly grows to astronomical numbers. The result is a search that is too slow or never completes. ""Heuristics"" or ""rules of thumb"" can help prioritize choices that are more likely to reach a goal.
Adversarial search is used for game-playing programs, such as chess or Go. It searches through a tree of possible moves and counter-moves, looking for a winning position.


==== Local search ====
 Local search uses mathematical optimization to find a solution to a problem. It begins with some form of guess and refines it incrementally.
Gradient descent is a type of local search that optimizes a set of numerical parameters by incrementally adjusting them to minimize a loss function. Variants of gradient descent are commonly used to train neural networks.
Another type of local search is evolutionary computation, which aims to iteratively improve a set of candidate solutions by ""mutating"" and ""recombining"" them, selecting only the fittest to survive each generation.
Distributed search processes can coordinate via swarm intelligence algorithms. Two popular swarm algorithms used in search are particle swarm optimization (inspired by bird flocking) and ant colony optimization (inspired by ant trails).


=== Logic ===
Formal logic is used for reasoning and knowledge representation.
Formal logic comes in two main forms: propositional logic (which operates on statements that are true or false and uses logical connectives such as ""and"", ""or"", ""not"" and ""implies"") and predicate logic (which also operates on objects, predicates and relations and uses quantifiers such as ""Every X is a Y"" and ""There are some Xs that are Ys"").
Deductive reasoning in logic is the process of proving a new statement (conclusion) from other statements that are given and assumed to be true (the premises). Proofs can be structured as proof trees, in which nodes are labelled by sentences, and children nodes are connected to parent nodes by inference rules.
Given a problem and a set of premises, problem-solving reduces to searching for a proof tree whose root node is labelled by a solution of the problem and whose leaf nodes are labelled by premises or axioms. In the case of Horn clauses, problem-solving search can be performed by reasoning forwards from the premises or backwards from the problem. In the more general case of the clausal form of first-order logic, resolution is a single, axiom-free rule of inference, in which a problem is solved by proving a contradiction from premises that include the negation of the problem to be solved.
Inference in both Horn clause logic and first-order logic is undecidable, and therefore intractable. However, backward reasoning with Horn clauses, which underpins computation in the logic programming language Prolog, is Turing complete. Moreover, its efficiency is competitive with computation in other symbolic programming languages.
Fuzzy logic assigns a ""degree of truth"" between 0 and 1. It can therefore handle propositions that are vague and partially true.
Non-monotonic logics, including logic programming with negation as failure, are designed to handle default reasoning. Other specialized versions of logic have been developed to describe many complex domains.


=== Probabilistic methods for uncertain reasoning ===

Many problems in AI (including in reasoning, planning, learning, perception, and robotics) require the agent to operate with incomplete or uncertain information. AI researchers have devised a number of tools to solve these problems using methods from probability theory and economics. Precise mathematical tools have been developed that analyze how an agent can make choices and plan, using decision theory, decision analysis, and information value theory. These tools include models such as Markov decision processes, dynamic decision networks, game theory and mechanism design.
Bayesian networks are a tool that can be used for reasoning (using the Bayesian inference algorithm), learning (using the expectation–maximization algorithm), planning (using decision networks) and perception (using dynamic Bayesian networks).
Probabilistic algorithms can also be used for filtering, prediction, smoothing, and finding explanations for streams of data, thus helping perception systems analyze processes that occur over time (e.g., hidden Markov models or Kalman filters).


=== Classifiers and statistical learning methods ===
The simplest AI applications can be divided into two types: classifiers (e.g., ""if shiny then diamond""), on one hand, and controllers (e.g., ""if diamond then pick up""), on the other hand. Classifiers are functions that use pattern matching to determine the closest match. They can be fine-tuned based on chosen examples using supervised learning. Each pattern (also called an ""observation"") is labeled with a certain predefined class. All the observations combined with their class labels are known as a data set. When a new observation is received, that observation is classified based on previous experience.
There are many kinds of classifiers in use. The decision tree is the simplest and most widely used symbolic machine learning algorithm. K-nearest neighbor algorithm was the most widely used analogical AI until the mid-1990s, and Kernel methods such as the support vector machine (SVM) displaced k-nearest neighbor in the 1990s.
The naive Bayes classifier is reportedly the ""most widely used learner"" at Google, due in part to its scalability.
Neural networks are also used as classifiers.


=== Artificial neural networks ===

An artificial neural network is based on a collection of nodes also known as artificial neurons, which loosely model the neurons in a biological brain. It is trained to recognise patterns; once trained, it can recognise those patterns in fresh data. There is an input, at least one hidden layer of nodes and an output. Each node applies a function and once the weight crosses its specified threshold, the data is transmitted to the next layer. A network is typically called a deep neural network if it has at least 2 hidden layers.
Learning algorithms for neural networks use local search to choose the weights that will get the right output for each input during training. The most common training technique is the backpropagation algorithm. Neural networks learn to model complex relationships between inputs and outputs and find patterns in data. In theory, a neural network can learn any function.
In feedforward neural networks the signal passes in only one direction. Recurrent neural networks feed the output signal back into the input, which allows short-term memories of previous input events. Long short term memory is the most successful network architecture for recurrent networks. Perceptrons use only a single layer of neurons; deep learning uses multiple layers. Convolutional neural networks strengthen the connection between neurons that are ""close"" to each other—this is especially important in image processing, where a local set of neurons must identify an ""edge"" before the network can identify an object.


=== Deep learning ===

Deep learning uses several layers of neurons between the network's inputs and outputs. The multiple layers can progressively extract higher-level features from the raw input. For example, in image processing, lower layers may identify edges, while higher layers may identify the concepts relevant to a human such as digits, letters, or faces.
Deep learning has profoundly improved the performance of programs in many important subfields of artificial intelligence, including computer vision, speech recognition, natural language processing, image classification, and others. The reason that deep learning performs so well in so many applications is not known as of 2023. The sudden success of deep learning in 2012–2015 did not occur because of some new discovery or theoretical breakthrough (deep neural networks and backpropagation had been described by many people, as far back as the 1950s) but because of two factors: the incredible increase in computer power (including the hundred-fold increase in speed by switching to GPUs) and the availability of vast amounts of training data, especially the giant curated datasets used for benchmark testing, such as ImageNet.


=== GPT ===
Generative pre-trained transformers (GPT) are large language models (LLMs) that generate text based on the semantic relationships between words in sentences. Text-based GPT models are pretrained on a large corpus of text that can be from the Internet. The pretraining consists of predicting the next token (a token being usually a word, subword, or punctuation). Throughout this pretraining, GPT models accumulate knowledge about the world and can then generate human-like text by repeatedly predicting the next token. Typically, a subsequent training phase makes the model more truthful, useful, and harmless, usually with a technique called reinforcement learning from human feedback (RLHF). Current GPT models are prone to generating falsehoods called ""hallucinations"", although this can be reduced with RLHF and quality data. They are used in chatbots, which allow people to ask a question or request a task in simple text.
Current models and services include Gemini (formerly Bard), ChatGPT, Grok, Claude, Copilot, and LLaMA. Multimodal GPT models can process different types of data (modalities) such as images, videos, sound, and text.


=== Hardware and software ===

In the late 2010s, graphics processing units (GPUs) that were increasingly designed with AI-specific enhancements and used with specialized TensorFlow software had replaced previously used central processing unit (CPUs) as the dominant means for large-scale (commercial and academic) machine learning models' training. Specialized programming languages such as Prolog were used in early AI research, but general-purpose programming languages like Python have become predominant.
The transistor density in integrated circuits has been observed to roughly double every 18 months—a trend known as Moore's law, named after the Intel co-founder Gordon Moore, who first identified it. Improvements in GPUs have been even faster.


== Applications ==
AI and machine learning technology is used in most of the essential applications of the 2020s, including: search engines (such as Google Search), targeting online advertisements, recommendation systems (offered by Netflix, YouTube or Amazon), driving internet traffic, targeted advertising (AdSense, Facebook), virtual assistants (such as Siri or Alexa), autonomous vehicles (including drones, ADAS and self-driving cars), automatic language translation (Microsoft Translator, Google Translate), facial recognition (Apple's Face ID or Microsoft's DeepFace and Google's FaceNet) and image labeling (used by Facebook, Apple's iPhoto and TikTok). The deployment of AI may be overseen by a Chief automation officer (CAO).


=== Health and medicine ===

The application of AI in medicine and medical research has the potential to increase patient care and quality of life. Through the lens of the Hippocratic Oath, medical professionals are ethically compelled to use AI, if applications can more accurately diagnose and treat patients.
For medical research, AI is an important tool for processing and integrating big data. This is particularly important for organoid and tissue engineering development which use microscopy imaging as a key technique in fabrication. It has been suggested that AI can overcome discrepancies in funding allocated to different fields of research. New AI tools can deepen the understanding of biomedically relevant pathways. For example, AlphaFold 2 (2021) demonstrated the ability to approximate, in hours rather than months, the 3D structure of a protein. In 2023, it was reported that AI-guided drug discovery helped find a class of antibiotics capable of killing two different types of drug-resistant bacteria. In 2024, researchers used machine learning to accelerate the search for Parkinson's disease drug treatments. Their aim was to identify compounds that block the clumping, or aggregation, of alpha-synuclein (the protein that characterises Parkinson's disease). They were able to speed up the initial screening process ten-fold and reduce the cost by a thousand-fold.


=== Games ===

Game playing programs have been used since the 1950s to demonstrate and test AI's most advanced techniques. Deep Blue became the first computer chess-playing system to beat a reigning world chess champion, Garry Kasparov, on 11 May 1997. In 2011, in a Jeopardy! quiz show exhibition match, IBM's question answering system, Watson, defeated the two greatest Jeopardy! champions, Brad Rutter and Ken Jennings, by a significant margin. In March 2016, AlphaGo won 4 out of 5 games of Go in a match with Go champion Lee Sedol, becoming the first computer Go-playing system to beat a professional Go player without handicaps. Then, in 2017, it defeated Ke Jie, who was the best Go player in the world. Other programs handle imperfect-information games, such as the poker-playing program Pluribus. DeepMind developed increasingly generalistic reinforcement learning models, such as with MuZero, which could be trained to play chess, Go, or Atari games. In 2019, DeepMind's AlphaStar achieved grandmaster level in StarCraft II, a particularly challenging real-time strategy game that involves incomplete knowledge of what happens on the map. In 2021, an AI agent competed in a PlayStation Gran Turismo competition, winning against four of the world's best Gran Turismo drivers using deep reinforcement learning. In 2024, Google DeepMind introduced SIMA, a type of AI capable of autonomously playing nine previously unseen open-world video games by observing screen output, as well as executing short, specific tasks in response to natural language instructions.


=== Mathematics ===
In mathematics, special forms of formal step-by-step reasoning are used. In contrast, LLMs such as GPT-4 Turbo, Gemini Ultra, Claude Opus, LLaMa-2 or Mistral Large are working with probabilistic models, which can produce wrong answers in the form of hallucinations. Therefore, they need not only a large database of mathematical problems to learn from but also methods such as supervised fine-tuning or trained classifiers with human-annotated data to improve answers for new problems and learn from corrections. A 2024 study showed that the performance of some language models for reasoning capabilities in solving math problems not included in their training data was low, even for problems with only minor deviations from trained data.
Alternatively, dedicated models for mathematic problem solving with higher precision for the outcome including proof of theorems have been developed such as Alpha Tensor, Alpha Geometry and Alpha Proof all from Google DeepMind, Llemma from eleuther or Julius.
When natural language is used to describe mathematical problems, converters transform such prompts into a formal language such as Lean to define mathematic tasks.
Some models have been developed to solve challenging problems and reach good results in benchmark tests, others to serve as educational tools in mathematics.


=== Finance ===
Finance is one of the fastest growing sectors where applied AI tools are being deployed: from retail online banking to investment advice and insurance, where automated ""robot advisers"" have been in use for some years.
World Pensions experts like Nicolas Firzli insist it may be too early to see the emergence of highly innovative AI-informed financial products and services: ""the deployment of AI tools will simply further automatise things: destroying tens of thousands of jobs in banking, financial planning, and pension advice in the process, but I'm not sure it will unleash a new wave of [e.g., sophisticated] pension innovation.""


=== Military ===

Various countries are deploying AI military applications. The main applications enhance command and control, communications, sensors, integration and interoperability. Research is targeting intelligence collection and analysis, logistics, cyber operations, information operations, and semiautonomous and autonomous vehicles. AI technologies enable coordination of sensors and effectors, threat detection and identification, marking of enemy positions, target acquisition, coordination and deconfliction of distributed Joint Fires between networked combat vehicles involving manned and unmanned teams. AI was incorporated into military operations in Iraq and Syria.
In November 2023, US Vice President Kamala Harris disclosed a declaration signed by 31 nations to set guardrails for the military use of AI. The commitments include using legal reviews to ensure the compliance of military AI with international laws, and being cautious and transparent in the development of this technology.


=== Generative AI ===

In the early 2020s, generative AI gained widespread prominence. GenAI is AI capable of generating text, images, videos, or other data using generative models, often in response to prompts.
In March 2023, 58% of U.S. adults had heard about ChatGPT and 14% had tried it. The increasing realism and ease-of-use of AI-based text-to-image generators such as Midjourney, DALL-E, and Stable Diffusion sparked a trend of viral AI-generated photos. Widespread attention was gained by a fake photo of Pope Francis wearing a white puffer coat, the fictional arrest of Donald Trump, and a hoax of an attack on the Pentagon, as well as the usage in professional creative arts.


=== Agents ===
Artificial intelligent (AI) agents are software entities designed to perceive their environment, make decisions, and take actions autonomously to achieve specific goals. These agents can interact with users, their environment, or other agents. AI agents are used in various applications, including virtual assistants, chatbots, autonomous vehicles, game-playing systems, and industrial robotics. AI agents operate within the constraints of their programming, available computational resources, and hardware limitations. This means they are restricted to performing tasks within their defined scope and have finite memory and processing capabilities. In real-world applications, AI agents often face time constraints for decision-making and action execution. Many AI agents incorporate learning algorithms, enabling them to improve their performance over time through experience or training. Using machine learning, AI agents can adapt to new situations and optimise their behaviour for their designated tasks.


=== Other industry-specific tasks ===
There are also thousands of successful AI applications used to solve specific problems for specific industries or institutions. In a 2017 survey, one in five companies reported having incorporated ""AI"" in some offerings or processes. A few examples are energy storage, medical diagnosis, military logistics, applications that predict the result of judicial decisions, foreign policy, or supply chain management.
AI applications for evacuation and disaster management are growing. AI has been used to investigate if and how people evacuated in large scale and small scale evacuations using historical data from GPS, videos or social media. Further, AI can provide real time information on the real time evacuation conditions.
In agriculture, AI has helped farmers identify areas that need irrigation, fertilization, pesticide treatments or increasing yield. Agronomists use AI to conduct research and development. AI has been used to predict the ripening time for crops such as tomatoes, monitor soil moisture, operate agricultural robots, conduct predictive analytics, classify livestock pig call emotions, automate greenhouses, detect diseases and pests, and save water.
Artificial intelligence is used in astronomy to analyze increasing amounts of available data and applications, mainly for ""classification, regression, clustering, forecasting, generation, discovery, and the development of new scientific insights."" For example, it is used for discovering exoplanets, forecasting solar activity, and distinguishing between signals and instrumental effects in gravitational wave astronomy. Additionally, it could be used for activities in space, such as space exploration, including the analysis of data from space missions, real-time science decisions of spacecraft, space debris avoidance, and more autonomous operation.


== Ethics ==

AI has potential benefits and potential risks. AI may be able to advance science and find solutions for serious problems: Demis Hassabis of Deep Mind hopes to ""solve intelligence, and then use that to solve everything else"". However, as the use of AI has become widespread, several unintended consequences and risks have been identified. In-production systems can sometimes not factor ethics and bias into their AI training processes, especially when the AI algorithms are inherently unexplainable in deep learning.


=== Risks and harm ===


==== Privacy and copyright ====

Machine learning algorithms require large amounts of data. The techniques used to acquire this data have raised concerns about privacy, surveillance and copyright.
AI-powered devices and services, such as virtual assistants and IoT products, continuously collect personal information, raising concerns about intrusive data gathering and unauthorized access by third parties. The loss of privacy is further exacerbated by AI's ability to process and combine vast amounts of data, potentially leading to a surveillance society where individual activities are constantly monitored and analyzed without adequate safeguards or transparency.
Sensitive user data collected may include online activity records, geolocation data, video or audio. For example, in order to build speech recognition algorithms, Amazon has recorded millions of private conversations and allowed temporary workers to listen to and transcribe some of them. Opinions about this widespread surveillance range from those who see it as a necessary evil to those for whom it is clearly unethical and a violation of the right to privacy.
AI developers argue that this is the only way to deliver valuable applications. and have developed several techniques that attempt to preserve privacy while still obtaining the data, such as data aggregation, de-identification and differential privacy. Since 2016, some privacy experts, such as Cynthia Dwork, have begun to view privacy in terms of fairness. Brian Christian wrote that experts have pivoted ""from the question of 'what they know' to the question of 'what they're doing with it'.""
Generative AI is often trained on unlicensed copyrighted works, including in domains such as images or computer code; the output is then used under the rationale of ""fair use"". Experts disagree about how well and under what circumstances this rationale will hold up in courts of law; relevant factors may include ""the purpose and character of the use of the copyrighted work"" and ""the effect upon the potential market for the copyrighted work"". Website owners who do not wish to have their content scraped can indicate it in a ""robots.txt"" file. In 2023, leading authors (including John Grisham and Jonathan Franzen) sued AI companies for using their work to train generative AI. Another discussed approach is to envision a separate sui generis system of protection for creations generated by AI to ensure fair attribution and compensation for human authors.


==== Dominance by tech giants ====
The commercial AI scene is dominated by Big Tech companies such as Alphabet Inc., Amazon, Apple Inc., Meta Platforms, and Microsoft. Some of these players already own the vast majority of existing cloud infrastructure and computing power from data centers, allowing them to entrench further in the marketplace.


==== Substantial power needs and other environmental impacts ====

In January 2024, the International Energy Agency (IEA) released Electricity 2024, Analysis and Forecast to 2026, forecasting electric power use. This is the first IEA report to make projections for data centers and power consumption for artificial intelligence and cryptocurrency. The report states that power demand for these uses might double by 2026, with additional electric power usage equal to electricity used by the whole Japanese nation.
Prodigious power consumption by AI is responsible for the growth of fossil fuels use, and might delay closings of obsolete, carbon-emitting coal energy facilities. There is a feverish rise in the construction of data centers throughout the US, making large technology firms (e.g., Microsoft, Meta, Google, Amazon) into voracious consumers of electric power. Projected electric consumption is so immense that there is concern that it will be fulfilled no matter the source. A ChatGPT search involves the use of 10 times the electrical energy as a Google search. The large firms are in haste to find power sources – from nuclear energy to geothermal to fusion. The tech firms argue that – in the long view – AI will be eventually kinder to the environment, but they need the energy now. AI makes the power grid more efficient and ""intelligent"", will assist in the growth of nuclear power, and track overall carbon emissions, according to technology firms.
A 2024 Goldman Sachs Research Paper, AI Data Centers and the Coming US Power Demand Surge, found ""US power demand (is) likely to experience growth not seen in a generation...."" and forecasts that, by 2030, US data centers will consume 8% of US power, as opposed to 3% in 2022, presaging growth for the electrical power generation industry by a variety of means.Data centers' need for more and more electrical power is such that they might max out the electrical grid. The Big Tech companies counter that AI can be used to maximize the utilization of the grid by all.
In 2024, the Wall Street Journal reported that big AI companies have begun negotiations with the US nuclear power providers to provide electricity to the data centers. In March 2024 Amazon purchased a Pennsylvania nuclear-powered data center for $650 Million (US).
In September 2024, Microsoft announced an agreement with Constellation Energy to re-open the Three Mile Island nuclear power plant to provide Microsoft with 100% of all electric power produced by the plant for 20 years. Reopening the plant, which suffered a partial nuclear meltdown of its Unit 2 reactor in 1979, will require Constellation to get through strict regulatory processes which will include extensive safety scrutiny from the US Nuclear Regulatory Commission. If approved (this will be the first ever US re-commissioning of a nuclear plant), over 835 megahertz of power – enough for 800,000 homes – of energy will be produced. The cost for re-opening and upgrading is estimated at $1.6 billion (US) and is dependent on tax breaks for nuclear power contained in the 2022 US Inflation Reduction Act. The US government and the state of Michigan are investing almost $2 billion (US) to reopen the Palisades Nuclear reactor on Lake Michigan. Closed since 2022, the plant is planned to be reopened in October 2025. The Three Mile Island facility will be renamed the Crane Clean Energy Center after Chris Crane, a nuclear proponent and former CEO of Exelon who was responsible for Exelon spinoff of Constellation.


==== Misinformation ====

YouTube, Facebook and others use recommender systems to guide users to more content. These AI programs were given the goal of maximizing user engagement (that is, the only goal was to keep people watching). The AI learned that users tended to choose misinformation, conspiracy theories, and extreme partisan content, and, to keep them watching, the AI recommended more of it. Users also tended to watch more content on the same subject, so the AI led people into filter bubbles where they received multiple versions of the same misinformation. This convinced many users that the misinformation was true, and ultimately undermined trust in institutions, the media and the government. The AI program had correctly learned to maximize its goal, but the result was harmful to society. After the U.S. election in 2016, major technology companies took steps to mitigate the problem .
In 2022, generative AI began to create images, audio, video and text that are indistinguishable from real photographs, recordings, films, or human writing. It is possible for bad actors to use this technology to create massive amounts of misinformation or propaganda. AI pioneer Geoffrey Hinton expressed concern about AI enabling ""authoritarian leaders to manipulate their electorates"" on a large scale, among other risks.


==== Algorithmic bias and fairness ====

Machine learning applications will be biased if they learn from biased data. The developers may not be aware that the bias exists. Bias can be introduced by the way training data is selected and by the way a model is deployed. If a biased algorithm is used to make decisions that can seriously harm people (as it can in medicine, finance, recruitment, housing or policing) then the algorithm may cause discrimination. The field of fairness studies how to prevent harms from algorithmic biases.
On June 28, 2015, Google Photos's new image labeling feature mistakenly identified Jacky Alcine and a friend as ""gorillas"" because they were black. The system was trained on a dataset that contained very few images of black people, a problem called ""sample size disparity"". Google ""fixed"" this problem by preventing the system from labelling anything as a ""gorilla"". Eight years later, in 2023, Google Photos still could not identify a gorilla, and neither could similar products from Apple, Facebook, Microsoft and Amazon.
COMPAS is a commercial program widely used by U.S. courts to assess the likelihood of a defendant becoming a recidivist. In 2016, Julia Angwin at ProPublica discovered that COMPAS exhibited racial bias, despite the fact that the program was not told the races of the defendants. Although the error rate for both whites and blacks was calibrated equal at exactly 61%, the errors for each race were different—the system consistently overestimated the chance that a black person would re-offend and would underestimate the chance that a white person would not re-offend. In 2017, several researchers showed that it was mathematically impossible for COMPAS to accommodate all possible measures of fairness when the base rates of re-offense were different for whites and blacks in the data.
A program can make biased decisions even if the data does not explicitly mention a problematic feature (such as ""race"" or ""gender""). The feature will correlate with other features (like ""address"", ""shopping history"" or ""first name""), and the program will make the same decisions based on these features as it would on ""race"" or ""gender"". Moritz Hardt said ""the most robust fact in this research area is that fairness through blindness doesn't work.""
Criticism of COMPAS highlighted that machine learning models are designed to make ""predictions"" that are only valid if we assume that the future will resemble the past. If they are trained on data that includes the results of racist decisions in the past, machine learning models must predict that racist decisions will be made in the future. If an application then uses these predictions as recommendations, some of these ""recommendations"" will likely be racist. Thus, machine learning is not well suited to help make decisions in areas where there is hope that the future will be better than the past. It is descriptive rather than prescriptive.
Bias and unfairness may go undetected because the developers are overwhelmingly white and male: among AI engineers, about 4% are black and 20% are women.
There are various conflicting definitions and mathematical models of fairness. These notions depend on ethical assumptions, and are influenced by beliefs about society. One broad category is distributive fairness, which focuses on the outcomes, often identifying groups and seeking to compensate for statistical disparities. Representational fairness tries to ensure that AI systems do not reinforce negative stereotypes or render certain groups invisible. Procedural fairness focuses on the decision process rather than the outcome. The most relevant notions of fairness may depend on the context, notably the type of AI application and the stakeholders. The subjectivity in the notions of bias and fairness makes it difficult for companies to operationalize them. Having access to sensitive attributes such as race or gender is also considered by many AI ethicists to be necessary in order to compensate for biases, but it may conflict with anti-discrimination laws.
At its 2022 Conference on Fairness, Accountability, and Transparency (ACM FAccT 2022), the Association for Computing Machinery, in Seoul, South Korea, presented and published findings that recommend that until AI and robotics systems are demonstrated to be free of bias mistakes, they are unsafe, and the use of self-learning neural networks trained on vast, unregulated sources of flawed internet data should be curtailed.


==== Lack of transparency ====

Many AI systems are so complex that their designers cannot explain how they reach their decisions. Particularly with deep neural networks, in which there are a large amount of non-linear relationships between inputs and outputs. But some popular explainability techniques exist.
It is impossible to be certain that a program is operating correctly if no one knows how exactly it works. There have been many cases where a machine learning program passed rigorous tests, but nevertheless learned something different than what the programmers intended. For example, a system that could identify skin diseases better than medical professionals was found to actually have a strong tendency to classify images with a ruler as ""cancerous"", because pictures of malignancies typically include a ruler to show the scale. Another machine learning system designed to help effectively allocate medical resources was found to classify patients with asthma as being at ""low risk"" of dying from pneumonia. Having asthma is actually a severe risk factor, but since the patients having asthma would usually get much more medical care, they were relatively unlikely to die according to the training data. The correlation between asthma and low risk of dying from pneumonia was real, but misleading.
People who have been harmed by an algorithm's decision have a right to an explanation. Doctors, for example, are expected to clearly and completely explain to their colleagues the reasoning behind any decision they make. Early drafts of the European Union's General Data Protection Regulation in 2016 included an explicit statement that this right exists. Industry experts noted that this is an unsolved problem with no solution in sight. Regulators argued that nevertheless the harm is real: if the problem has no solution, the tools should not be used.
DARPA established the XAI (""Explainable Artificial Intelligence"") program in 2014 to try to solve these problems.
Several approaches aim to address the transparency problem. SHAP enables to visualise the contribution of each feature to the output. LIME can locally approximate a model's outputs with a simpler, interpretable model. Multitask learning provides a large number of outputs in addition to the target classification. These other outputs can help developers deduce what the network has learned. Deconvolution, DeepDream and other generative methods can allow developers to see what different layers of a deep network for computer vision have learned, and produce output that can suggest what the network is learning. For generative pre-trained transformers, Anthropic developed a technique based on dictionary learning that associates patterns of neuron activations with human-understandable concepts.


==== Bad actors and weaponized AI ====

Artificial intelligence provides a number of tools that are useful to bad actors, such as authoritarian governments, terrorists, criminals or rogue states.
A lethal autonomous weapon is a machine that locates, selects and engages human targets without human supervision. Widely available AI tools can be used by bad actors to develop inexpensive autonomous weapons and, if produced at scale, they are potentially weapons of mass destruction. Even when used in conventional warfare, it is unlikely that they will be unable to reliably choose targets and could potentially kill an innocent person. In 2014, 30 nations (including China) supported a ban on autonomous weapons under the United Nations' Convention on Certain Conventional Weapons, however the United States and others disagreed. By 2015, over fifty countries were reported to be researching battlefield robots.
AI tools make it easier for authoritarian governments to efficiently control their citizens in several ways. Face and voice recognition allow widespread surveillance. Machine learning, operating this data, can classify potential enemies of the state and prevent them from hiding. Recommendation systems can precisely target propaganda and misinformation for maximum effect. Deepfakes and generative AI aid in producing misinformation. Advanced AI can make authoritarian centralized decision making more competitive than liberal and decentralized systems such as markets. It lowers the cost and difficulty of digital warfare and advanced spyware. All these technologies have been available since 2020 or earlier—AI facial recognition systems are already being used for mass surveillance in China.
There many other ways that AI is expected to help bad actors, some of which can not be foreseen. For example, machine-learning AI is able to design tens of thousands of toxic molecules in a matter of hours.


==== Technological unemployment ====

Economists have frequently highlighted the risks of redundancies from AI, and speculated about unemployment if there is no adequate social policy for full employment.
In the past, technology has tended to increase rather than reduce total employment, but economists acknowledge that ""we're in uncharted territory"" with AI. A survey of economists showed disagreement about whether the increasing use of robots and AI will cause a substantial increase in long-term unemployment, but they generally agree that it could be a net benefit if productivity gains are redistributed. Risk estimates vary; for example, in the 2010s, Michael Osborne and Carl Benedikt Frey estimated 47% of U.S. jobs are at ""high risk"" of potential automation, while an OECD report classified only 9% of U.S. jobs as ""high risk"". The methodology of speculating about future employment levels has been criticised as lacking evidential foundation, and for implying that technology, rather than social policy, creates unemployment, as opposed to redundancies. In April 2023, it was reported that 70% of the jobs for Chinese video game illustrators had been eliminated by generative artificial intelligence.
Unlike previous waves of automation, many middle-class jobs may be eliminated by artificial intelligence; The Economist stated in 2015 that ""the worry that AI could do to white-collar jobs what steam power did to blue-collar ones during the Industrial Revolution"" is ""worth taking seriously"". Jobs at extreme risk range from paralegals to fast food cooks, while job demand is likely to increase for care-related professions ranging from personal healthcare to the clergy.
From the early days of the development of artificial intelligence, there have been arguments, for example, those put forward by Joseph Weizenbaum, about whether tasks that can be done by computers actually should be done by them, given the difference between computers and humans, and between quantitative calculation and qualitative, value-based judgement.


==== Existential risk ====

It has been argued AI will become so powerful that humanity may irreversibly lose control of it. This could, as physicist Stephen Hawking stated, ""spell the end of the human race"". This scenario has been common in science fiction, when a computer or robot suddenly develops a human-like ""self-awareness"" (or ""sentience"" or ""consciousness"") and becomes a malevolent character. These sci-fi scenarios are misleading in several ways.
First, AI does not require human-like ""sentience"" to be an existential risk. Modern AI programs are given specific goals and use learning and intelligence to achieve them. Philosopher Nick Bostrom argued that if one gives almost any goal to a sufficiently powerful AI, it may choose to destroy humanity to achieve it (he used the example of a paperclip factory manager). Stuart Russell gives the example of household robot that tries to find a way to kill its owner to prevent it from being unplugged, reasoning that ""you can't fetch the coffee if you're dead."" In order to be safe for humanity, a superintelligence would have to be genuinely aligned with humanity's morality and values so that it is ""fundamentally on our side"".
Second, Yuval Noah Harari argues that AI does not require a robot body or physical control to pose an existential risk. The essential parts of civilization are not physical. Things like ideologies, law, government, money and the economy are made of language; they exist because there are stories that billions of people believe. The current prevalence of misinformation suggests that an AI could use language to convince people to believe anything, even to take actions that are destructive.
The opinions amongst experts and industry insiders are mixed, with sizable fractions both concerned and unconcerned by risk from eventual superintelligent AI. Personalities such as Stephen Hawking, Bill Gates, and Elon Musk, as well as AI pioneers such as Yoshua Bengio, Stuart Russell, Demis Hassabis, and Sam Altman, have expressed concerns about existential risk from AI.
In May 2023, Geoffrey Hinton announced his resignation from Google in order to be able to ""freely speak out about the risks of AI"" without ""considering how this impacts Google."" He notably mentioned risks of an AI takeover, and stressed that in order to avoid the worst outcomes, establishing safety guidelines will require cooperation among those competing in use of AI.
In 2023, many leading AI experts issued the joint statement that ""Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war"".
Other researchers, however, spoke in favor of a less dystopian view. AI pioneer Juergen Schmidhuber did not sign the joint statement, emphasising that in 95% of all cases, AI research is about making ""human lives longer and healthier and easier."" While the tools that are now being used to improve lives can also be used by bad actors, ""they can also be used against the bad actors."" Andrew Ng also argued that ""it's a mistake to fall for the doomsday hype on AI—and that regulators who do will only benefit vested interests."" Yann LeCun ""scoffs at his peers' dystopian scenarios of supercharged misinformation and even, eventually, human extinction."" In the early 2010s, experts argued that the risks are too distant in the future to warrant research or that humans will be valuable from the perspective of a superintelligent machine. However, after 2016, the study of current and future risks and possible solutions became a serious area of research.


=== Ethical machines and alignment ===

Friendly AI are machines that have been designed from the beginning to minimize risks and to make choices that benefit humans. Eliezer Yudkowsky, who coined the term, argues that developing friendly AI should be a higher research priority: it may require a large investment and it must be completed before AI becomes an existential risk.
Machines with intelligence have the potential to use their intelligence to make ethical decisions. The field of machine ethics provides machines with ethical principles and procedures for resolving ethical dilemmas.
The field of machine ethics is also called computational morality,
and was founded at an AAAI symposium in 2005.
Other approaches include Wendell Wallach's ""artificial moral agents"" and Stuart J. Russell's three principles for developing provably beneficial machines.


=== Open source ===
Active organizations in the AI open-source community include Hugging Face, Google, EleutherAI and Meta. Various AI models, such as Llama 2, Mistral or Stable Diffusion, have been made open-weight, meaning that their architecture and trained parameters (the ""weights"") are publicly available. Open-weight models can be freely fine-tuned, which allows companies to specialize them with their own data and for their own use-case. Open-weight models are useful for research and innovation but can also be misused. Since they can be fine-tuned, any built-in security measure, such as objecting to harmful requests, can be trained away until it becomes ineffective. Some researchers warn that future AI models may develop dangerous capabilities (such as the potential to drastically facilitate bioterrorism) and that once released on the Internet, they cannot be deleted everywhere if needed. They recommend pre-release audits and cost-benefit analyses.


=== Frameworks ===
Artificial Intelligence projects can have their ethical permissibility tested while designing, developing, and implementing an AI system. An AI framework such as the Care and Act Framework containing the SUM values—developed by the Alan Turing Institute tests projects in four main areas:

Respect the dignity of individual people
Connect with other people sincerely, openly, and inclusively
Care for the wellbeing of everyone
Protect social values, justice, and the public interest
Other developments in ethical frameworks include those decided upon during the Asilomar Conference, the Montreal Declaration for Responsible AI, and the IEEE's Ethics of Autonomous Systems initiative, among others; however, these principles do not go without their criticisms, especially regards to the people chosen contributes to these frameworks.
Promotion of the wellbeing of the people and communities that these technologies affect requires consideration of the social and ethical implications at all stages of AI system design, development and implementation, and collaboration between job roles such as data scientists, product managers, data engineers, domain experts, and delivery managers.
The UK AI Safety Institute released in 2024 a testing toolset called 'Inspect' for AI safety evaluations available under a MIT open-source licence which is freely available on GitHub and can be improved with third-party packages. It can be used to evaluate AI models in a range of areas including core knowledge, ability to reason, and autonomous capabilities.


=== Regulation ===

The regulation of artificial intelligence is the development of public sector policies and laws for promoting and regulating AI; it is therefore related to the broader regulation of algorithms. The regulatory and policy landscape for AI is an emerging issue in jurisdictions globally. According to AI Index at Stanford, the annual number of AI-related laws passed in the 127 survey countries jumped from one passed in 2016 to 37 passed in 2022 alone. Between 2016 and 2020, more than 30 countries adopted dedicated strategies for AI. Most EU member states had released national AI strategies, as had Canada, China, India, Japan, Mauritius, the Russian Federation, Saudi Arabia, United Arab Emirates, U.S., and Vietnam. Others were in the process of elaborating their own AI strategy, including Bangladesh, Malaysia and Tunisia. The Global Partnership on Artificial Intelligence was launched in June 2020, stating a need for AI to be developed in accordance with human rights and democratic values, to ensure public confidence and trust in the technology. Henry Kissinger, Eric Schmidt, and Daniel Huttenlocher published a joint statement in November 2021 calling for a government commission to regulate AI. In 2023, OpenAI leaders published recommendations for the governance of superintelligence, which they believe may happen in less than 10 years. In 2023, the United Nations also launched an advisory body to provide recommendations on AI governance; the body comprises technology company executives, governments officials and academics. In 2024, the Council of Europe created the first international legally binding treaty on AI, called the ""Framework Convention on Artificial Intelligence and Human Rights, Democracy and the Rule of Law"". It was adopted by the European Union, the United States, the United Kingdom, and other signatories.
In a 2022 Ipsos survey, attitudes towards AI varied greatly by country; 78% of Chinese citizens, but only 35% of Americans, agreed that ""products and services using AI have more benefits than drawbacks"". A 2023 Reuters/Ipsos poll found that 61% of Americans agree, and 22% disagree, that AI poses risks to humanity. In a 2023 Fox News poll, 35% of Americans thought it ""very important"", and an additional 41% thought it ""somewhat important"", for the federal government to regulate AI, versus 13% responding ""not very important"" and 8% responding ""not at all important"".
In November 2023, the first global AI Safety Summit was held in Bletchley Park in the UK to discuss the near and far term risks of AI and the possibility of mandatory and voluntary regulatory frameworks. 28 countries including the United States, China, and the European Union issued a declaration at the start of the summit, calling for international co-operation to manage the challenges and risks of artificial intelligence. In May 2024 at the AI Seoul Summit, 16 global AI tech companies agreed to safety commitments on the development of AI.


== History ==

The study of mechanical or ""formal"" reasoning began with philosophers and mathematicians in antiquity. The study of logic led directly to Alan Turing's theory of computation, which suggested that a machine, by shuffling symbols as simple as ""0"" and ""1"", could simulate any conceivable form of mathematical reasoning. This, along with concurrent discoveries in cybernetics, information theory and neurobiology, led researchers to consider the possibility of building an ""electronic brain"". They developed several areas of research that would become part of AI, such as McCullouch and Pitts design for ""artificial neurons"" in 1943, and Turing's influential 1950 paper 'Computing Machinery and Intelligence', which introduced the Turing test and showed that ""machine intelligence"" was plausible.
The field of AI research was founded at a workshop at Dartmouth College in 1956. The attendees became the leaders of AI research in the 1960s. They and their students produced programs that the press described as ""astonishing"": computers were learning checkers strategies, solving word problems in algebra, proving logical theorems and speaking English. Artificial intelligence laboratories were set up at a number of British and U.S. universities in the latter 1950s and early 1960s.
Researchers in the 1960s and the 1970s were convinced that their methods would eventually succeed in creating a machine with general intelligence and considered this the goal of their field. In 1965 Herbert Simon predicted, ""machines will be capable, within twenty years, of doing any work a man can do"". In 1967 Marvin Minsky agreed, writing that ""within a generation ... the problem of creating 'artificial intelligence' will substantially be solved"". They had, however, underestimated the difficulty of the problem. In 1974, both the U.S. and British governments cut off exploratory research in response to the criticism of Sir James Lighthill and ongoing pressure from the U.S. Congress to fund more productive projects. Minsky's and Papert's book Perceptrons was understood as proving that artificial neural networks would never be useful for solving real-world tasks, thus discrediting the approach altogether. The ""AI winter"", a period when obtaining funding for AI projects was difficult, followed.
In the early 1980s, AI research was revived by the commercial success of expert systems, a form of AI program that simulated the knowledge and analytical skills of human experts. By 1985, the market for AI had reached over a billion dollars. At the same time, Japan's fifth generation computer project inspired the U.S. and British governments to restore funding for academic research. However, beginning with the collapse of the Lisp Machine market in 1987, AI once again fell into disrepute, and a second, longer-lasting winter began.
Up to this point, most of AI's funding had gone to projects that used high-level symbols to represent mental objects like plans, goals, beliefs, and known facts. In the 1980s, some researchers began to doubt that this approach would be able to imitate all the processes of human cognition, especially perception, robotics, learning and pattern recognition, and began to look into ""sub-symbolic"" approaches. Rodney Brooks rejected ""representation"" in general and focussed directly on engineering machines that move and survive. Judea Pearl, Lofti Zadeh and others developed methods that handled incomplete and uncertain information by making reasonable guesses rather than precise logic. But the most important development was the revival of ""connectionism"", including neural network research, by Geoffrey Hinton and others. In 1990, Yann LeCun successfully showed that convolutional neural networks can recognize handwritten digits, the first of many successful applications of neural networks.
AI gradually restored its reputation in the late 1990s and early 21st century by exploiting formal mathematical methods and by finding specific solutions to specific problems. This ""narrow"" and ""formal"" focus allowed researchers to produce verifiable results and collaborate with other fields (such as statistics, economics and mathematics). By 2000, solutions developed by AI researchers were being widely used, although in the 1990s they were rarely described as ""artificial intelligence"".
However, several academic researchers became concerned that AI was no longer pursuing its original goal of creating versatile, fully intelligent machines. Beginning around 2002, they founded the subfield of artificial general intelligence (or ""AGI""), which had several well-funded institutions by the 2010s.
Deep learning began to dominate industry benchmarks in 2012 and was adopted throughout the field.
For many specific tasks, other methods were abandoned.
Deep learning's success was based on both hardware improvements (faster computers, graphics processing units, cloud computing) and access to large amounts of data (including curated datasets, such as ImageNet). Deep learning's success led to an enormous increase in interest and funding in AI. The amount of machine learning research (measured by total publications) increased by 50% in the years 2015–2019.
In 2016, issues of fairness and the misuse of technology were catapulted into center stage at machine learning conferences, publications vastly increased, funding became available, and many researchers re-focussed their careers on these issues. The alignment problem became a serious field of academic study.
In the late teens and early 2020s, AGI companies began to deliver programs that created enormous interest. In 2015, AlphaGo, developed by DeepMind, beat the world champion Go player. The program was taught only the rules of the game and developed strategy by itself. GPT-3 is a large language model that was released in 2020 by OpenAI and is capable of generating high-quality human-like text. These programs, and others, inspired an aggressive AI boom, where large companies began investing billions in AI research. According to AI Impacts, about $50 billion annually was invested in ""AI"" around 2022 in the U.S. alone and about 20% of the new U.S. Computer Science PhD graduates have specialized in ""AI"". About 800,000 ""AI""-related U.S. job openings existed in 2022.


== Philosophy ==
Philosophical debates have historically sought to determine the nature of intelligence and how to make intelligent machines. Another major focus has been whether machines can be conscious, and the associated ethical implications. Many other topics in philosophy can relevant to AI, such as epistemology and free will. Rapid advancements have intensified public discussions on the philosophy and ethics of AI.


=== Defining artificial intelligence ===

Alan Turing wrote in 1950 ""I propose to consider the question 'can machines think'?"" He advised changing the question from whether a machine ""thinks"", to ""whether or not it is possible for machinery to show intelligent behaviour"". He devised the Turing test, which measures the ability of a machine to simulate human conversation. Since we can only observe the behavior of the machine, it does not matter if it is ""actually"" thinking or literally has a ""mind"". Turing notes that we can not determine these things about other people but ""it is usual to have a polite convention that everyone thinks.""

Russell and Norvig agree with Turing that intelligence must be defined in terms of external behavior, not internal structure. However, they are critical that the test requires the machine to imitate humans. ""Aeronautical engineering texts,"" they wrote, ""do not define the goal of their field as making 'machines that fly so exactly like pigeons that they can fool other pigeons.'"" AI founder John McCarthy agreed, writing that ""Artificial intelligence is not, by definition, simulation of human intelligence"".
McCarthy defines intelligence as ""the computational part of the ability to achieve goals in the world"". Another AI founder, Marvin Minsky similarly describes it as ""the ability to solve hard problems"". The leading AI textbook defines it as the study of agents that perceive their environment and take actions that maximize their chances of achieving defined goals. These definitions view intelligence in terms of well-defined problems with well-defined solutions, where both the difficulty of the problem and the performance of the program are direct measures of the ""intelligence"" of the machine—and no other philosophical discussion is required, or may not even be possible.
Another definition has been adopted by Google, a major practitioner in the field of AI. This definition stipulates the ability of systems to synthesize information as the manifestation of intelligence, similar to the way it is defined in biological intelligence.
Some authors have suggested in practice, that the definition of AI is vague and difficult to define, with contention as to whether classical algorithms should be categorised as AI, with many companies during the early 2020s AI boom using the term as a marketing buzzword, often even if they did ""not actually use AI in a material way"".


=== Evaluating approaches to AI ===
No established unifying theory or paradigm has guided AI research for most of its history. The unprecedented success of statistical machine learning in the 2010s eclipsed all other approaches (so much so that some sources, especially in the business world, use the term ""artificial intelligence"" to mean ""machine learning with neural networks""). This approach is mostly sub-symbolic, soft and narrow. Critics argue that these questions may have to be revisited by future generations of AI researchers.


==== Symbolic AI and its limits ====
Symbolic AI (or ""GOFAI"") simulated the high-level conscious reasoning that people use when they solve puzzles, express legal reasoning and do mathematics. They were highly successful at ""intelligent"" tasks such as algebra or IQ tests. In the 1960s, Newell and Simon proposed the physical symbol systems hypothesis: ""A physical symbol system has the necessary and sufficient means of general intelligent action.""
However, the symbolic approach failed on many tasks that humans solve easily, such as learning, recognizing an object or commonsense reasoning. Moravec's paradox is the discovery that high-level ""intelligent"" tasks were easy for AI, but low level ""instinctive"" tasks were extremely difficult. Philosopher Hubert Dreyfus had argued since the 1960s that human expertise depends on unconscious instinct rather than conscious symbol manipulation, and on having a ""feel"" for the situation, rather than explicit symbolic knowledge. Although his arguments had been ridiculed and ignored when they were first presented, eventually, AI research came to agree with him.
The issue is not resolved: sub-symbolic reasoning can make many of the same inscrutable mistakes that human intuition does, such as algorithmic bias. Critics such as Noam Chomsky argue continuing research into symbolic AI will still be necessary to attain general intelligence, in part because sub-symbolic AI is a move away from explainable AI: it can be difficult or impossible to understand why a modern statistical AI program made a particular decision. The emerging field of neuro-symbolic artificial intelligence attempts to bridge the two approaches.


==== Neat vs. scruffy ====

""Neats"" hope that intelligent behavior is described using simple, elegant principles (such as logic, optimization, or neural networks). ""Scruffies"" expect that it necessarily requires solving a large number of unrelated problems. Neats defend their programs with theoretical rigor, scruffies rely mainly on incremental testing to see if they work. This issue was actively discussed in the 1970s and 1980s, but eventually was seen as irrelevant. Modern AI has elements of both.


==== Soft vs. hard computing ====

Finding a provably correct or optimal solution is intractable for many important problems. Soft computing is a set of techniques, including genetic algorithms, fuzzy logic and neural networks, that are tolerant of imprecision, uncertainty, partial truth and approximation. Soft computing was introduced in the late 1980s and most successful AI programs in the 21st century are examples of soft computing with neural networks.


==== Narrow vs. general AI ====

AI researchers are divided as to whether to pursue the goals of artificial general intelligence and superintelligence directly or to solve as many specific problems as possible (narrow AI) in hopes these solutions will lead indirectly to the field's long-term goals. General intelligence is difficult to define and difficult to measure, and modern AI has had more verifiable successes by focusing on specific problems with specific solutions. The sub-field of artificial general intelligence studies this area exclusively.


=== Machine consciousness, sentience, and mind ===

The philosophy of mind does not know whether a machine can have a mind, consciousness and mental states, in the same sense that human beings do. This issue considers the internal experiences of the machine, rather than its external behavior. Mainstream AI research considers this issue irrelevant because it does not affect the goals of the field: to build machines that can solve problems using intelligence. Russell and Norvig add that ""[t]he additional project of making a machine conscious in exactly the way humans are is not one that we are equipped to take on."" However, the question has become central to the philosophy of mind. It is also typically the central question at issue in artificial intelligence in fiction.


==== Consciousness ====

David Chalmers identified two problems in understanding the mind, which he named the ""hard"" and ""easy"" problems of consciousness. The easy problem is understanding how the brain processes signals, makes plans and controls behavior. The hard problem is explaining how this feels or why it should feel like anything at all, assuming we are right in thinking that it truly does feel like something (Dennett's consciousness illusionism says this is an illusion). While human information processing is easy to explain, human subjective experience is difficult to explain. For example, it is easy to imagine a color-blind person who has learned to identify which objects in their field of view are red, but it is not clear what would be required for the person to know what red looks like.


==== Computationalism and functionalism ====

Computationalism is the position in the philosophy of mind that the human mind is an information processing system and that thinking is a form of computing. Computationalism argues that the relationship between mind and body is similar or identical to the relationship between software and hardware and thus may be a solution to the mind–body problem. This philosophical position was inspired by the work of AI researchers and cognitive scientists in the 1960s and was originally proposed by philosophers Jerry Fodor and Hilary Putnam.
Philosopher John Searle characterized this position as ""strong AI"": ""The appropriately programmed computer with the right inputs and outputs would thereby have a mind in exactly the same sense human beings have minds."" Searle counters this assertion with his Chinese room argument, which attempts to show that, even if a machine perfectly simulates human behavior, there is still no reason to suppose it also has a mind.


==== AI welfare and rights ====
It is difficult or impossible to reliably evaluate whether an advanced AI is sentient (has the ability to feel), and if so, to what degree. But if there is a significant chance that a given machine can feel and suffer, then it may be entitled to certain rights or welfare protection measures, similarly to animals. Sapience (a set of capacities related to high intelligence, such as discernment or self-awareness) may provide another moral basis for AI rights. Robot rights are also sometimes proposed as a practical way to integrate autonomous agents into society.
In 2017, the European Union considered granting ""electronic personhood"" to some of the most capable AI systems. Similarly to the legal status of companies, it would have conferred rights but also responsibilities. Critics argued in 2018 that granting rights to AI systems would downplay the importance of human rights, and that legislation should focus on user needs rather than speculative futuristic scenarios. They also noted that robots lacked the autonomy to take part to society on their own.
Progress in AI increased interest in the topic. Proponents of AI welfare and rights often argue that AI sentience, if it emerges, would be particularly easy to deny. They warn that this may be a moral blind spot analogous to slavery or factory farming, which could lead to large-scale suffering if sentient AI is created and carelessly exploited.


== Future ==


=== Superintelligence and the singularity ===
A superintelligence is a hypothetical agent that would possess intelligence far surpassing that of the brightest and most gifted human mind.If research into artificial general intelligence produced sufficiently intelligent software, it might be able to reprogram and improve itself. The improved software would be even better at improving itself, leading to what I. J. Good called an ""intelligence explosion"" and Vernor Vinge called a ""singularity"".
However, technologies cannot improve exponentially indefinitely, and typically follow an S-shaped curve, slowing when they reach the physical limits of what the technology can do.


=== Transhumanism ===

Robot designer Hans Moravec, cyberneticist Kevin Warwick and inventor Ray Kurzweil have predicted that humans and machines may merge in the future into cyborgs that are more capable and powerful than either. This idea, called transhumanism, has roots in the writings of Aldous Huxley and Robert Ettinger.
Edward Fredkin argues that ""artificial intelligence is the next step in evolution"", an idea first proposed by Samuel Butler's ""Darwin among the Machines"" as far back as 1863, and expanded upon by George Dyson in his 1998 book Darwin Among the Machines: The Evolution of Global Intelligence.


== In fiction ==

Thought-capable artificial beings have appeared as storytelling devices since antiquity, and have been a persistent theme in science fiction.
A common trope in these works began with Mary Shelley's Frankenstein, where a human creation becomes a threat to its masters. This includes such works as Arthur C. Clarke's and Stanley Kubrick's 2001: A Space Odyssey (both 1968), with HAL 9000, the murderous computer in charge of the Discovery One spaceship, as well as The Terminator (1984) and The Matrix (1999). In contrast, the rare loyal robots such as Gort from The Day the Earth Stood Still (1951) and Bishop from Aliens (1986) are less prominent in popular culture.
Isaac Asimov introduced the Three Laws of Robotics in many stories, most notably with the ""Multivac"" super-intelligent computer. Asimov's laws are often brought up during lay discussions of machine ethics; while almost all artificial intelligence researchers are familiar with Asimov's laws through popular culture, they generally consider the laws useless for many reasons, one of which is their ambiguity.
Several works use AI to force us to confront the fundamental question of what makes us human, showing us artificial beings that have the ability to feel, and thus to suffer. This appears in Karel Čapek's R.U.R., the films A.I. Artificial Intelligence and Ex Machina, as well as the novel Do Androids Dream of Electric Sheep?, by Philip K. Dick. Dick considers the idea that our understanding of human subjectivity is altered by technology created with artificial intelligence.


== See also ==
AI Convention – International treatyPages displaying short descriptions of redirect targets
Artificial intelligence content detection – Software to detect AI-generated content
Behavior selection algorithm – Algorithm that selects actions for intelligent agents
Business process automation – Automation of business processes
Case-based reasoning – Process of solving new problems based on the solutions of similar past problems
Computational intelligence – Ability of a computer to learn a specific task from data or experimental observation
Digital immortality – Hypothetical concept of storing a personality in digital form
Emergent algorithm – Algorithm exhibiting emergent behavior
Female gendering of AI technologies – Gender biases in digital technologyPages displaying short descriptions of redirect targets
Glossary of artificial intelligence – List of definitions of terms and concepts commonly used in the study of artificial intelligence
Intelligence amplification – Use of information technology to augment human intelligence
Mind uploading – Hypothetical process of digitally emulating a brain
Moravec's paradox
Organoid intelligence – Use of brain cells and brain organoids for intelligent computing
Robotic process automation – Form of business process automation technology
Weak artificial intelligence – Form of artificial intelligence
Wetware computer – Computer composed of organic material
Hallucination (artificial intelligence) – Erroneous material generated by AI


== Explanatory notes ==


== References ==


=== AI textbooks ===
The two most widely used textbooks in 2023 (see the Open Syllabus):

Russell, Stuart J.; Norvig, Peter (2021). Artificial Intelligence: A Modern Approach (4th ed.). Hoboken: Pearson. ISBN 978-0-1346-1099-3. LCCN 20190474.
Rich, Elaine; Knight, Kevin; Nair, Shivashankar B (2010). Artificial Intelligence (3rd ed.). New Delhi: Tata McGraw Hill India. ISBN 978-0-0700-8770-5.
These were the four of the most widely used AI textbooks in 2008:

Other textbooks:

Ertel, Wolfgang (2017). Introduction to Artificial Intelligence (2nd ed.). Springer. ISBN 978-3-3195-8486-7.
Ciaramella, Alberto; Ciaramella, Marco (2024). Introduction to Artificial Intelligence: from data analysis to generative AI (1st ed.). Intellisemantic Editions. ISBN 978-8-8947-8760-3.


=== History of AI ===


=== Other sources ===


== Further reading ==


== External links ==

""Artificial Intelligence"". Internet Encyclopedia of Philosophy.
Thomason, Richmond. ""Logic and Artificial Intelligence"". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy.
Artificial Intelligence. BBC Radio 4 discussion with John Agar, Alison Adam & Igor Aleksander (In Our Time, 8 December 2005)."
